{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdbbd0282f43527",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        pooled = torch.mean(gru_out, dim=1)\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(self.relu(dropped))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4317 - Avg Loss: 1.0846 - Accuracy: 31.25%\n",
      "Epoch: 0. Batch 10/4317 - Avg Loss: 1.1517 - Accuracy: 32.39%\n",
      "Epoch: 0. Batch 20/4317 - Avg Loss: 1.1508 - Accuracy: 31.25%\n",
      "Epoch: 0. Batch 30/4317 - Avg Loss: 1.1555 - Accuracy: 32.86%\n",
      "Epoch: 0. Batch 40/4317 - Avg Loss: 1.1552 - Accuracy: 31.86%\n",
      "Epoch: 0. Batch 50/4317 - Avg Loss: 1.1510 - Accuracy: 32.35%\n",
      "Epoch: 0. Batch 60/4317 - Avg Loss: 1.1545 - Accuracy: 33.30%\n",
      "Epoch: 0. Batch 70/4317 - Avg Loss: 1.1466 - Accuracy: 33.89%\n",
      "Epoch: 0. Batch 80/4317 - Avg Loss: 1.1430 - Accuracy: 34.34%\n",
      "Epoch: 0. Batch 90/4317 - Avg Loss: 1.1398 - Accuracy: 34.75%\n",
      "Epoch: 0. Batch 100/4317 - Avg Loss: 1.1390 - Accuracy: 34.34%\n",
      "Epoch: 0. Batch 110/4317 - Avg Loss: 1.1376 - Accuracy: 34.23%\n",
      "Epoch: 0. Batch 120/4317 - Avg Loss: 1.1405 - Accuracy: 34.14%\n",
      "Epoch: 0. Batch 130/4317 - Avg Loss: 1.1414 - Accuracy: 33.87%\n",
      "Epoch: 0. Batch 140/4317 - Avg Loss: 1.1406 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 150/4317 - Avg Loss: 1.1380 - Accuracy: 34.64%\n",
      "Epoch: 0. Batch 160/4317 - Avg Loss: 1.1380 - Accuracy: 34.28%\n",
      "Epoch: 0. Batch 170/4317 - Avg Loss: 1.1375 - Accuracy: 33.92%\n",
      "Epoch: 0. Batch 180/4317 - Avg Loss: 1.1371 - Accuracy: 33.84%\n",
      "Epoch: 0. Batch 190/4317 - Avg Loss: 1.1382 - Accuracy: 33.57%\n",
      "Epoch: 0. Batch 200/4317 - Avg Loss: 1.1382 - Accuracy: 33.43%\n",
      "Epoch: 0. Batch 210/4317 - Avg Loss: 1.1377 - Accuracy: 33.53%\n",
      "Epoch: 0. Batch 220/4317 - Avg Loss: 1.1380 - Accuracy: 33.57%\n",
      "Epoch: 0. Batch 230/4317 - Avg Loss: 1.1374 - Accuracy: 33.20%\n",
      "Epoch: 0. Batch 240/4317 - Avg Loss: 1.1369 - Accuracy: 33.35%\n",
      "Epoch: 0. Batch 250/4317 - Avg Loss: 1.1356 - Accuracy: 33.62%\n",
      "Epoch: 0. Batch 260/4317 - Avg Loss: 1.1351 - Accuracy: 33.76%\n",
      "Epoch: 0. Batch 270/4317 - Avg Loss: 1.1344 - Accuracy: 33.51%\n",
      "Epoch: 0. Batch 280/4317 - Avg Loss: 1.1340 - Accuracy: 33.54%\n",
      "Epoch: 0. Batch 290/4317 - Avg Loss: 1.1327 - Accuracy: 33.57%\n",
      "Epoch: 0. Batch 300/4317 - Avg Loss: 1.1306 - Accuracy: 33.97%\n",
      "Epoch: 0. Batch 310/4317 - Avg Loss: 1.1296 - Accuracy: 34.16%\n",
      "Epoch: 0. Batch 320/4317 - Avg Loss: 1.1290 - Accuracy: 34.00%\n",
      "Epoch: 0. Batch 330/4317 - Avg Loss: 1.1277 - Accuracy: 33.99%\n",
      "Epoch: 0. Batch 340/4317 - Avg Loss: 1.1264 - Accuracy: 34.13%\n",
      "Epoch: 0. Batch 350/4317 - Avg Loss: 1.1263 - Accuracy: 34.37%\n",
      "Epoch: 0. Batch 360/4317 - Avg Loss: 1.1261 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 370/4317 - Avg Loss: 1.1251 - Accuracy: 34.62%\n",
      "Epoch: 0. Batch 380/4317 - Avg Loss: 1.1241 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 390/4317 - Avg Loss: 1.1245 - Accuracy: 34.88%\n",
      "Epoch: 0. Batch 400/4317 - Avg Loss: 1.1234 - Accuracy: 34.93%\n",
      "Epoch: 0. Batch 410/4317 - Avg Loss: 1.1225 - Accuracy: 35.01%\n",
      "Epoch: 0. Batch 420/4317 - Avg Loss: 1.1220 - Accuracy: 35.18%\n",
      "Epoch: 0. Batch 430/4317 - Avg Loss: 1.1219 - Accuracy: 35.18%\n",
      "Epoch: 0. Batch 440/4317 - Avg Loss: 1.1218 - Accuracy: 35.47%\n",
      "Epoch: 0. Batch 450/4317 - Avg Loss: 1.1217 - Accuracy: 35.44%\n",
      "Epoch: 0. Batch 460/4317 - Avg Loss: 1.1209 - Accuracy: 35.55%\n",
      "Epoch: 0. Batch 470/4317 - Avg Loss: 1.1202 - Accuracy: 35.66%\n",
      "Epoch: 0. Batch 480/4317 - Avg Loss: 1.1205 - Accuracy: 35.67%\n",
      "Epoch: 0. Batch 490/4317 - Avg Loss: 1.1201 - Accuracy: 35.82%\n",
      "Epoch: 0. Batch 500/4317 - Avg Loss: 1.1199 - Accuracy: 35.98%\n",
      "Epoch: 0. Batch 510/4317 - Avg Loss: 1.1189 - Accuracy: 36.03%\n",
      "Epoch: 0. Batch 520/4317 - Avg Loss: 1.1184 - Accuracy: 36.11%\n",
      "Epoch: 0. Batch 530/4317 - Avg Loss: 1.1163 - Accuracy: 36.38%\n",
      "Epoch: 0. Batch 540/4317 - Avg Loss: 1.1143 - Accuracy: 36.58%\n",
      "Epoch: 0. Batch 550/4317 - Avg Loss: 1.1122 - Accuracy: 36.68%\n",
      "Epoch: 0. Batch 560/4317 - Avg Loss: 1.1099 - Accuracy: 36.84%\n",
      "Epoch: 0. Batch 570/4317 - Avg Loss: 1.1086 - Accuracy: 37.06%\n",
      "Epoch: 0. Batch 580/4317 - Avg Loss: 1.1079 - Accuracy: 37.20%\n",
      "Epoch: 0. Batch 590/4317 - Avg Loss: 1.1074 - Accuracy: 37.19%\n",
      "Epoch: 0. Batch 600/4317 - Avg Loss: 1.1059 - Accuracy: 37.38%\n",
      "Epoch: 0. Batch 610/4317 - Avg Loss: 1.1037 - Accuracy: 37.61%\n",
      "Epoch: 0. Batch 620/4317 - Avg Loss: 1.1019 - Accuracy: 37.68%\n",
      "Epoch: 0. Batch 630/4317 - Avg Loss: 1.1016 - Accuracy: 37.64%\n",
      "Epoch: 0. Batch 640/4317 - Avg Loss: 1.1010 - Accuracy: 37.75%\n",
      "Epoch: 0. Batch 650/4317 - Avg Loss: 1.1005 - Accuracy: 37.79%\n",
      "Epoch: 0. Batch 660/4317 - Avg Loss: 1.0992 - Accuracy: 37.88%\n",
      "Epoch: 0. Batch 670/4317 - Avg Loss: 1.0971 - Accuracy: 38.04%\n",
      "Epoch: 0. Batch 680/4317 - Avg Loss: 1.0960 - Accuracy: 38.11%\n",
      "Epoch: 0. Batch 690/4317 - Avg Loss: 1.0950 - Accuracy: 38.22%\n",
      "Epoch: 0. Batch 700/4317 - Avg Loss: 1.0940 - Accuracy: 38.41%\n",
      "Epoch: 0. Batch 710/4317 - Avg Loss: 1.0932 - Accuracy: 38.52%\n",
      "Epoch: 0. Batch 720/4317 - Avg Loss: 1.0919 - Accuracy: 38.76%\n",
      "Epoch: 0. Batch 730/4317 - Avg Loss: 1.0905 - Accuracy: 38.93%\n",
      "Epoch: 0. Batch 740/4317 - Avg Loss: 1.0895 - Accuracy: 38.98%\n",
      "Epoch: 0. Batch 750/4317 - Avg Loss: 1.0883 - Accuracy: 39.07%\n",
      "Epoch: 0. Batch 760/4317 - Avg Loss: 1.0869 - Accuracy: 39.18%\n",
      "Epoch: 0. Batch 770/4317 - Avg Loss: 1.0860 - Accuracy: 39.29%\n",
      "Epoch: 0. Batch 780/4317 - Avg Loss: 1.0849 - Accuracy: 39.35%\n",
      "Epoch: 0. Batch 790/4317 - Avg Loss: 1.0838 - Accuracy: 39.50%\n",
      "Epoch: 0. Batch 800/4317 - Avg Loss: 1.0822 - Accuracy: 39.62%\n",
      "Epoch: 0. Batch 810/4317 - Avg Loss: 1.0803 - Accuracy: 39.70%\n",
      "Epoch: 0. Batch 820/4317 - Avg Loss: 1.0790 - Accuracy: 39.83%\n",
      "Epoch: 0. Batch 830/4317 - Avg Loss: 1.0781 - Accuracy: 39.91%\n",
      "Epoch: 0. Batch 840/4317 - Avg Loss: 1.0775 - Accuracy: 40.02%\n",
      "Epoch: 0. Batch 850/4317 - Avg Loss: 1.0764 - Accuracy: 40.11%\n",
      "Epoch: 0. Batch 860/4317 - Avg Loss: 1.0757 - Accuracy: 40.20%\n",
      "Epoch: 0. Batch 870/4317 - Avg Loss: 1.0744 - Accuracy: 40.34%\n",
      "Epoch: 0. Batch 880/4317 - Avg Loss: 1.0725 - Accuracy: 40.47%\n",
      "Epoch: 0. Batch 890/4317 - Avg Loss: 1.0712 - Accuracy: 40.62%\n",
      "Epoch: 0. Batch 900/4317 - Avg Loss: 1.0710 - Accuracy: 40.69%\n",
      "Epoch: 0. Batch 910/4317 - Avg Loss: 1.0691 - Accuracy: 40.82%\n",
      "Epoch: 0. Batch 920/4317 - Avg Loss: 1.0676 - Accuracy: 40.96%\n",
      "Epoch: 0. Batch 930/4317 - Avg Loss: 1.0669 - Accuracy: 41.03%\n",
      "Epoch: 0. Batch 940/4317 - Avg Loss: 1.0663 - Accuracy: 41.11%\n",
      "Epoch: 0. Batch 950/4317 - Avg Loss: 1.0656 - Accuracy: 41.19%\n",
      "Epoch: 0. Batch 960/4317 - Avg Loss: 1.0646 - Accuracy: 41.23%\n",
      "Epoch: 0. Batch 970/4317 - Avg Loss: 1.0636 - Accuracy: 41.34%\n",
      "Epoch: 0. Batch 980/4317 - Avg Loss: 1.0627 - Accuracy: 41.39%\n",
      "Epoch: 0. Batch 990/4317 - Avg Loss: 1.0619 - Accuracy: 41.48%\n",
      "Epoch: 0. Batch 1000/4317 - Avg Loss: 1.0605 - Accuracy: 41.60%\n",
      "Epoch: 0. Batch 1010/4317 - Avg Loss: 1.0595 - Accuracy: 41.70%\n",
      "Epoch: 0. Batch 1020/4317 - Avg Loss: 1.0584 - Accuracy: 41.82%\n",
      "Epoch: 0. Batch 1030/4317 - Avg Loss: 1.0573 - Accuracy: 41.96%\n",
      "Epoch: 0. Batch 1040/4317 - Avg Loss: 1.0572 - Accuracy: 42.06%\n",
      "Epoch: 0. Batch 1050/4317 - Avg Loss: 1.0567 - Accuracy: 42.14%\n",
      "Epoch: 0. Batch 1060/4317 - Avg Loss: 1.0553 - Accuracy: 42.24%\n",
      "Epoch: 0. Batch 1070/4317 - Avg Loss: 1.0542 - Accuracy: 42.32%\n",
      "Epoch: 0. Batch 1080/4317 - Avg Loss: 1.0534 - Accuracy: 42.41%\n",
      "Epoch: 0. Batch 1090/4317 - Avg Loss: 1.0524 - Accuracy: 42.50%\n",
      "Epoch: 0. Batch 1100/4317 - Avg Loss: 1.0516 - Accuracy: 42.59%\n",
      "Epoch: 0. Batch 1110/4317 - Avg Loss: 1.0505 - Accuracy: 42.66%\n",
      "Epoch: 0. Batch 1120/4317 - Avg Loss: 1.0503 - Accuracy: 42.65%\n",
      "Epoch: 0. Batch 1130/4317 - Avg Loss: 1.0502 - Accuracy: 42.69%\n",
      "Epoch: 0. Batch 1140/4317 - Avg Loss: 1.0492 - Accuracy: 42.79%\n",
      "Epoch: 0. Batch 1150/4317 - Avg Loss: 1.0483 - Accuracy: 42.83%\n",
      "Epoch: 0. Batch 1160/4317 - Avg Loss: 1.0474 - Accuracy: 42.93%\n",
      "Epoch: 0. Batch 1170/4317 - Avg Loss: 1.0463 - Accuracy: 43.02%\n",
      "Epoch: 0. Batch 1180/4317 - Avg Loss: 1.0452 - Accuracy: 43.11%\n",
      "Epoch: 0. Batch 1190/4317 - Avg Loss: 1.0440 - Accuracy: 43.20%\n",
      "Epoch: 0. Batch 1200/4317 - Avg Loss: 1.0427 - Accuracy: 43.29%\n",
      "Epoch: 0. Batch 1210/4317 - Avg Loss: 1.0416 - Accuracy: 43.38%\n",
      "Epoch: 0. Batch 1220/4317 - Avg Loss: 1.0405 - Accuracy: 43.46%\n",
      "Epoch: 0. Batch 1230/4317 - Avg Loss: 1.0395 - Accuracy: 43.55%\n",
      "Epoch: 0. Batch 1240/4317 - Avg Loss: 1.0377 - Accuracy: 43.67%\n",
      "Epoch: 0. Batch 1250/4317 - Avg Loss: 1.0364 - Accuracy: 43.77%\n",
      "Epoch: 0. Batch 1260/4317 - Avg Loss: 1.0356 - Accuracy: 43.84%\n",
      "Epoch: 0. Batch 1270/4317 - Avg Loss: 1.0353 - Accuracy: 43.91%\n",
      "Epoch: 0. Batch 1280/4317 - Avg Loss: 1.0352 - Accuracy: 43.91%\n",
      "Epoch: 0. Batch 1290/4317 - Avg Loss: 1.0342 - Accuracy: 44.03%\n",
      "Epoch: 0. Batch 1300/4317 - Avg Loss: 1.0329 - Accuracy: 44.12%\n",
      "Epoch: 0. Batch 1310/4317 - Avg Loss: 1.0321 - Accuracy: 44.17%\n",
      "Epoch: 0. Batch 1320/4317 - Avg Loss: 1.0310 - Accuracy: 44.25%\n",
      "Epoch: 0. Batch 1330/4317 - Avg Loss: 1.0306 - Accuracy: 44.29%\n",
      "Epoch: 0. Batch 1340/4317 - Avg Loss: 1.0298 - Accuracy: 44.35%\n",
      "Epoch: 0. Batch 1350/4317 - Avg Loss: 1.0285 - Accuracy: 44.45%\n",
      "Epoch: 0. Batch 1360/4317 - Avg Loss: 1.0277 - Accuracy: 44.52%\n",
      "Epoch: 0. Batch 1370/4317 - Avg Loss: 1.0268 - Accuracy: 44.62%\n",
      "Epoch: 0. Batch 1380/4317 - Avg Loss: 1.0252 - Accuracy: 44.73%\n",
      "Epoch: 0. Batch 1390/4317 - Avg Loss: 1.0245 - Accuracy: 44.79%\n",
      "Epoch: 0. Batch 1400/4317 - Avg Loss: 1.0239 - Accuracy: 44.88%\n",
      "Epoch: 0. Batch 1410/4317 - Avg Loss: 1.0234 - Accuracy: 44.94%\n",
      "Epoch: 0. Batch 1420/4317 - Avg Loss: 1.0232 - Accuracy: 44.96%\n",
      "Epoch: 0. Batch 1430/4317 - Avg Loss: 1.0223 - Accuracy: 45.05%\n",
      "Epoch: 0. Batch 1440/4317 - Avg Loss: 1.0212 - Accuracy: 45.13%\n",
      "Epoch: 0. Batch 1450/4317 - Avg Loss: 1.0199 - Accuracy: 45.24%\n",
      "Epoch: 0. Batch 1460/4317 - Avg Loss: 1.0190 - Accuracy: 45.31%\n",
      "Epoch: 0. Batch 1470/4317 - Avg Loss: 1.0184 - Accuracy: 45.39%\n",
      "Epoch: 0. Batch 1480/4317 - Avg Loss: 1.0178 - Accuracy: 45.44%\n",
      "Epoch: 0. Batch 1490/4317 - Avg Loss: 1.0170 - Accuracy: 45.48%\n",
      "Epoch: 0. Batch 1500/4317 - Avg Loss: 1.0161 - Accuracy: 45.56%\n",
      "Epoch: 0. Batch 1510/4317 - Avg Loss: 1.0154 - Accuracy: 45.63%\n",
      "Epoch: 0. Batch 1520/4317 - Avg Loss: 1.0147 - Accuracy: 45.72%\n",
      "Epoch: 0. Batch 1530/4317 - Avg Loss: 1.0140 - Accuracy: 45.77%\n",
      "Epoch: 0. Batch 1540/4317 - Avg Loss: 1.0137 - Accuracy: 45.84%\n",
      "Epoch: 0. Batch 1550/4317 - Avg Loss: 1.0129 - Accuracy: 45.92%\n",
      "Epoch: 0. Batch 1560/4317 - Avg Loss: 1.0118 - Accuracy: 46.00%\n",
      "Epoch: 0. Batch 1570/4317 - Avg Loss: 1.0109 - Accuracy: 46.11%\n",
      "Epoch: 0. Batch 1580/4317 - Avg Loss: 1.0101 - Accuracy: 46.17%\n",
      "Epoch: 0. Batch 1590/4317 - Avg Loss: 1.0096 - Accuracy: 46.21%\n",
      "Epoch: 0. Batch 1600/4317 - Avg Loss: 1.0086 - Accuracy: 46.28%\n",
      "Epoch: 0. Batch 1610/4317 - Avg Loss: 1.0085 - Accuracy: 46.33%\n",
      "Epoch: 0. Batch 1620/4317 - Avg Loss: 1.0074 - Accuracy: 46.41%\n",
      "Epoch: 0. Batch 1630/4317 - Avg Loss: 1.0058 - Accuracy: 46.52%\n",
      "Epoch: 0. Batch 1640/4317 - Avg Loss: 1.0053 - Accuracy: 46.56%\n",
      "Epoch: 0. Batch 1650/4317 - Avg Loss: 1.0049 - Accuracy: 46.60%\n",
      "Epoch: 0. Batch 1660/4317 - Avg Loss: 1.0042 - Accuracy: 46.63%\n",
      "Epoch: 0. Batch 1670/4317 - Avg Loss: 1.0033 - Accuracy: 46.72%\n",
      "Epoch: 0. Batch 1680/4317 - Avg Loss: 1.0031 - Accuracy: 46.77%\n",
      "Epoch: 0. Batch 1690/4317 - Avg Loss: 1.0020 - Accuracy: 46.86%\n",
      "Epoch: 0. Batch 1700/4317 - Avg Loss: 1.0010 - Accuracy: 46.92%\n",
      "Epoch: 0. Batch 1710/4317 - Avg Loss: 1.0009 - Accuracy: 46.94%\n",
      "Epoch: 0. Batch 1720/4317 - Avg Loss: 1.0003 - Accuracy: 46.97%\n",
      "Epoch: 0. Batch 1730/4317 - Avg Loss: 1.0000 - Accuracy: 47.00%\n",
      "Epoch: 0. Batch 1740/4317 - Avg Loss: 0.9999 - Accuracy: 47.02%\n",
      "Epoch: 0. Batch 1750/4317 - Avg Loss: 0.9989 - Accuracy: 47.08%\n",
      "Epoch: 0. Batch 1760/4317 - Avg Loss: 0.9985 - Accuracy: 47.11%\n",
      "Epoch: 0. Batch 1770/4317 - Avg Loss: 0.9979 - Accuracy: 47.16%\n",
      "Epoch: 0. Batch 1780/4317 - Avg Loss: 0.9981 - Accuracy: 47.19%\n",
      "Epoch: 0. Batch 1790/4317 - Avg Loss: 0.9975 - Accuracy: 47.23%\n",
      "Epoch: 0. Batch 1800/4317 - Avg Loss: 0.9972 - Accuracy: 47.23%\n",
      "Epoch: 0. Batch 1810/4317 - Avg Loss: 0.9965 - Accuracy: 47.25%\n",
      "Epoch: 0. Batch 1820/4317 - Avg Loss: 0.9960 - Accuracy: 47.30%\n",
      "Epoch: 0. Batch 1830/4317 - Avg Loss: 0.9955 - Accuracy: 47.35%\n",
      "Epoch: 0. Batch 1840/4317 - Avg Loss: 0.9948 - Accuracy: 47.41%\n",
      "Epoch: 0. Batch 1850/4317 - Avg Loss: 0.9938 - Accuracy: 47.47%\n",
      "Epoch: 0. Batch 1860/4317 - Avg Loss: 0.9927 - Accuracy: 47.56%\n",
      "Epoch: 0. Batch 1870/4317 - Avg Loss: 0.9919 - Accuracy: 47.62%\n",
      "Epoch: 0. Batch 1880/4317 - Avg Loss: 0.9911 - Accuracy: 47.70%\n",
      "Epoch: 0. Batch 1890/4317 - Avg Loss: 0.9903 - Accuracy: 47.75%\n",
      "Epoch: 0. Batch 1900/4317 - Avg Loss: 0.9896 - Accuracy: 47.80%\n",
      "Epoch: 0. Batch 1910/4317 - Avg Loss: 0.9893 - Accuracy: 47.83%\n",
      "Epoch: 0. Batch 1920/4317 - Avg Loss: 0.9884 - Accuracy: 47.90%\n",
      "Epoch: 0. Batch 1930/4317 - Avg Loss: 0.9879 - Accuracy: 47.94%\n",
      "Epoch: 0. Batch 1940/4317 - Avg Loss: 0.9875 - Accuracy: 47.94%\n",
      "Epoch: 0. Batch 1950/4317 - Avg Loss: 0.9869 - Accuracy: 47.99%\n",
      "Epoch: 0. Batch 1960/4317 - Avg Loss: 0.9864 - Accuracy: 48.05%\n",
      "Epoch: 0. Batch 1970/4317 - Avg Loss: 0.9855 - Accuracy: 48.12%\n",
      "Epoch: 0. Batch 1980/4317 - Avg Loss: 0.9851 - Accuracy: 48.16%\n",
      "Epoch: 0. Batch 1990/4317 - Avg Loss: 0.9842 - Accuracy: 48.23%\n",
      "Epoch: 0. Batch 2000/4317 - Avg Loss: 0.9833 - Accuracy: 48.29%\n",
      "Epoch: 0. Batch 2010/4317 - Avg Loss: 0.9828 - Accuracy: 48.33%\n",
      "Epoch: 0. Batch 2020/4317 - Avg Loss: 0.9821 - Accuracy: 48.39%\n",
      "Epoch: 0. Batch 2030/4317 - Avg Loss: 0.9817 - Accuracy: 48.43%\n",
      "Epoch: 0. Batch 2040/4317 - Avg Loss: 0.9809 - Accuracy: 48.50%\n",
      "Epoch: 0. Batch 2050/4317 - Avg Loss: 0.9801 - Accuracy: 48.55%\n",
      "Epoch: 0. Batch 2060/4317 - Avg Loss: 0.9796 - Accuracy: 48.58%\n",
      "Epoch: 0. Batch 2070/4317 - Avg Loss: 0.9792 - Accuracy: 48.62%\n",
      "Epoch: 0. Batch 2080/4317 - Avg Loss: 0.9787 - Accuracy: 48.66%\n",
      "Epoch: 0. Batch 2090/4317 - Avg Loss: 0.9781 - Accuracy: 48.70%\n",
      "Epoch: 0. Batch 2100/4317 - Avg Loss: 0.9774 - Accuracy: 48.76%\n",
      "Epoch: 0. Batch 2110/4317 - Avg Loss: 0.9765 - Accuracy: 48.82%\n",
      "Epoch: 0. Batch 2120/4317 - Avg Loss: 0.9757 - Accuracy: 48.87%\n",
      "Epoch: 0. Batch 2130/4317 - Avg Loss: 0.9757 - Accuracy: 48.89%\n",
      "Epoch: 0. Batch 2140/4317 - Avg Loss: 0.9750 - Accuracy: 48.92%\n",
      "Epoch: 0. Batch 2150/4317 - Avg Loss: 0.9744 - Accuracy: 48.95%\n",
      "Epoch: 0. Batch 2160/4317 - Avg Loss: 0.9741 - Accuracy: 48.99%\n",
      "Epoch: 0. Batch 2170/4317 - Avg Loss: 0.9738 - Accuracy: 49.04%\n",
      "Epoch: 0. Batch 2180/4317 - Avg Loss: 0.9734 - Accuracy: 49.11%\n",
      "Epoch: 0. Batch 2190/4317 - Avg Loss: 0.9725 - Accuracy: 49.15%\n",
      "Epoch: 0. Batch 2200/4317 - Avg Loss: 0.9716 - Accuracy: 49.19%\n",
      "Epoch: 0. Batch 2210/4317 - Avg Loss: 0.9705 - Accuracy: 49.25%\n",
      "Epoch: 0. Batch 2220/4317 - Avg Loss: 0.9697 - Accuracy: 49.29%\n",
      "Epoch: 0. Batch 2230/4317 - Avg Loss: 0.9691 - Accuracy: 49.32%\n",
      "Epoch: 0. Batch 2240/4317 - Avg Loss: 0.9684 - Accuracy: 49.38%\n",
      "Epoch: 0. Batch 2250/4317 - Avg Loss: 0.9681 - Accuracy: 49.41%\n",
      "Epoch: 0. Batch 2260/4317 - Avg Loss: 0.9672 - Accuracy: 49.48%\n",
      "Epoch: 0. Batch 2270/4317 - Avg Loss: 0.9669 - Accuracy: 49.51%\n",
      "Epoch: 0. Batch 2280/4317 - Avg Loss: 0.9663 - Accuracy: 49.54%\n",
      "Epoch: 0. Batch 2290/4317 - Avg Loss: 0.9660 - Accuracy: 49.59%\n",
      "Epoch: 0. Batch 2300/4317 - Avg Loss: 0.9648 - Accuracy: 49.68%\n",
      "Epoch: 0. Batch 2310/4317 - Avg Loss: 0.9643 - Accuracy: 49.74%\n",
      "Epoch: 0. Batch 2320/4317 - Avg Loss: 0.9642 - Accuracy: 49.75%\n",
      "Epoch: 0. Batch 2330/4317 - Avg Loss: 0.9636 - Accuracy: 49.81%\n",
      "Epoch: 0. Batch 2340/4317 - Avg Loss: 0.9628 - Accuracy: 49.87%\n",
      "Epoch: 0. Batch 2350/4317 - Avg Loss: 0.9624 - Accuracy: 49.90%\n",
      "Epoch: 0. Batch 2360/4317 - Avg Loss: 0.9614 - Accuracy: 49.98%\n",
      "Epoch: 0. Batch 2370/4317 - Avg Loss: 0.9611 - Accuracy: 50.03%\n",
      "Epoch: 0. Batch 2380/4317 - Avg Loss: 0.9606 - Accuracy: 50.08%\n",
      "Epoch: 0. Batch 2390/4317 - Avg Loss: 0.9600 - Accuracy: 50.10%\n",
      "Epoch: 0. Batch 2400/4317 - Avg Loss: 0.9590 - Accuracy: 50.15%\n",
      "Epoch: 0. Batch 2410/4317 - Avg Loss: 0.9583 - Accuracy: 50.21%\n",
      "Epoch: 0. Batch 2420/4317 - Avg Loss: 0.9578 - Accuracy: 50.25%\n",
      "Epoch: 0. Batch 2430/4317 - Avg Loss: 0.9574 - Accuracy: 50.29%\n",
      "Epoch: 0. Batch 2440/4317 - Avg Loss: 0.9576 - Accuracy: 50.28%\n",
      "Epoch: 0. Batch 2450/4317 - Avg Loss: 0.9571 - Accuracy: 50.31%\n",
      "Epoch: 0. Batch 2460/4317 - Avg Loss: 0.9568 - Accuracy: 50.33%\n",
      "Epoch: 0. Batch 2470/4317 - Avg Loss: 0.9565 - Accuracy: 50.37%\n",
      "Epoch: 0. Batch 2480/4317 - Avg Loss: 0.9561 - Accuracy: 50.40%\n",
      "Epoch: 0. Batch 2490/4317 - Avg Loss: 0.9558 - Accuracy: 50.44%\n",
      "Epoch: 0. Batch 2500/4317 - Avg Loss: 0.9554 - Accuracy: 50.47%\n",
      "Epoch: 0. Batch 2510/4317 - Avg Loss: 0.9549 - Accuracy: 50.51%\n",
      "Epoch: 0. Batch 2520/4317 - Avg Loss: 0.9544 - Accuracy: 50.54%\n",
      "Epoch: 0. Batch 2530/4317 - Avg Loss: 0.9537 - Accuracy: 50.56%\n",
      "Epoch: 0. Batch 2540/4317 - Avg Loss: 0.9535 - Accuracy: 50.58%\n",
      "Epoch: 0. Batch 2550/4317 - Avg Loss: 0.9529 - Accuracy: 50.62%\n",
      "Epoch: 0. Batch 2560/4317 - Avg Loss: 0.9526 - Accuracy: 50.65%\n",
      "Epoch: 0. Batch 2570/4317 - Avg Loss: 0.9525 - Accuracy: 50.66%\n",
      "Epoch: 0. Batch 2580/4317 - Avg Loss: 0.9520 - Accuracy: 50.70%\n",
      "Epoch: 0. Batch 2590/4317 - Avg Loss: 0.9516 - Accuracy: 50.72%\n",
      "Epoch: 0. Batch 2600/4317 - Avg Loss: 0.9512 - Accuracy: 50.76%\n",
      "Epoch: 0. Batch 2610/4317 - Avg Loss: 0.9509 - Accuracy: 50.80%\n",
      "Epoch: 0. Batch 2620/4317 - Avg Loss: 0.9507 - Accuracy: 50.83%\n",
      "Epoch: 0. Batch 2630/4317 - Avg Loss: 0.9499 - Accuracy: 50.88%\n",
      "Epoch: 0. Batch 2640/4317 - Avg Loss: 0.9493 - Accuracy: 50.93%\n",
      "Epoch: 0. Batch 2650/4317 - Avg Loss: 0.9488 - Accuracy: 50.96%\n",
      "Epoch: 0. Batch 2660/4317 - Avg Loss: 0.9483 - Accuracy: 50.98%\n",
      "Epoch: 0. Batch 2670/4317 - Avg Loss: 0.9478 - Accuracy: 51.03%\n",
      "Epoch: 0. Batch 2680/4317 - Avg Loss: 0.9471 - Accuracy: 51.09%\n",
      "Epoch: 0. Batch 2690/4317 - Avg Loss: 0.9466 - Accuracy: 51.13%\n",
      "Epoch: 0. Batch 2700/4317 - Avg Loss: 0.9459 - Accuracy: 51.16%\n",
      "Epoch: 0. Batch 2710/4317 - Avg Loss: 0.9456 - Accuracy: 51.18%\n",
      "Epoch: 0. Batch 2720/4317 - Avg Loss: 0.9456 - Accuracy: 51.19%\n",
      "Epoch: 0. Batch 2730/4317 - Avg Loss: 0.9450 - Accuracy: 51.24%\n",
      "Epoch: 0. Batch 2740/4317 - Avg Loss: 0.9446 - Accuracy: 51.26%\n",
      "Epoch: 0. Batch 2750/4317 - Avg Loss: 0.9442 - Accuracy: 51.28%\n",
      "Epoch: 0. Batch 2760/4317 - Avg Loss: 0.9442 - Accuracy: 51.29%\n",
      "Epoch: 0. Batch 2770/4317 - Avg Loss: 0.9438 - Accuracy: 51.31%\n",
      "Epoch: 0. Batch 2780/4317 - Avg Loss: 0.9433 - Accuracy: 51.35%\n",
      "Epoch: 0. Batch 2790/4317 - Avg Loss: 0.9430 - Accuracy: 51.36%\n",
      "Epoch: 0. Batch 2800/4317 - Avg Loss: 0.9423 - Accuracy: 51.42%\n",
      "Epoch: 0. Batch 2810/4317 - Avg Loss: 0.9419 - Accuracy: 51.45%\n",
      "Epoch: 0. Batch 2820/4317 - Avg Loss: 0.9414 - Accuracy: 51.48%\n",
      "Epoch: 0. Batch 2830/4317 - Avg Loss: 0.9408 - Accuracy: 51.54%\n",
      "Epoch: 0. Batch 2840/4317 - Avg Loss: 0.9405 - Accuracy: 51.57%\n",
      "Epoch: 0. Batch 2850/4317 - Avg Loss: 0.9403 - Accuracy: 51.61%\n",
      "Epoch: 0. Batch 2860/4317 - Avg Loss: 0.9395 - Accuracy: 51.66%\n",
      "Epoch: 0. Batch 2870/4317 - Avg Loss: 0.9393 - Accuracy: 51.67%\n",
      "Epoch: 0. Batch 2880/4317 - Avg Loss: 0.9391 - Accuracy: 51.69%\n",
      "Epoch: 0. Batch 2890/4317 - Avg Loss: 0.9389 - Accuracy: 51.73%\n",
      "Epoch: 0. Batch 2900/4317 - Avg Loss: 0.9388 - Accuracy: 51.74%\n",
      "Epoch: 0. Batch 2910/4317 - Avg Loss: 0.9385 - Accuracy: 51.77%\n",
      "Epoch: 0. Batch 2920/4317 - Avg Loss: 0.9382 - Accuracy: 51.80%\n",
      "Epoch: 0. Batch 2930/4317 - Avg Loss: 0.9379 - Accuracy: 51.83%\n",
      "Epoch: 0. Batch 2940/4317 - Avg Loss: 0.9373 - Accuracy: 51.87%\n",
      "Epoch: 0. Batch 2950/4317 - Avg Loss: 0.9370 - Accuracy: 51.90%\n",
      "Epoch: 0. Batch 2960/4317 - Avg Loss: 0.9366 - Accuracy: 51.92%\n",
      "Epoch: 0. Batch 2970/4317 - Avg Loss: 0.9362 - Accuracy: 51.94%\n",
      "Epoch: 0. Batch 2980/4317 - Avg Loss: 0.9357 - Accuracy: 51.96%\n",
      "Epoch: 0. Batch 2990/4317 - Avg Loss: 0.9354 - Accuracy: 51.98%\n",
      "Epoch: 0. Batch 3000/4317 - Avg Loss: 0.9350 - Accuracy: 52.01%\n",
      "Epoch: 0. Batch 3010/4317 - Avg Loss: 0.9345 - Accuracy: 52.03%\n",
      "Epoch: 0. Batch 3020/4317 - Avg Loss: 0.9343 - Accuracy: 52.05%\n",
      "Epoch: 0. Batch 3030/4317 - Avg Loss: 0.9340 - Accuracy: 52.07%\n",
      "Epoch: 0. Batch 3040/4317 - Avg Loss: 0.9338 - Accuracy: 52.10%\n",
      "Epoch: 0. Batch 3050/4317 - Avg Loss: 0.9336 - Accuracy: 52.11%\n",
      "Epoch: 0. Batch 3060/4317 - Avg Loss: 0.9332 - Accuracy: 52.14%\n",
      "Epoch: 0. Batch 3070/4317 - Avg Loss: 0.9328 - Accuracy: 52.16%\n",
      "Epoch: 0. Batch 3080/4317 - Avg Loss: 0.9323 - Accuracy: 52.17%\n",
      "Epoch: 0. Batch 3090/4317 - Avg Loss: 0.9320 - Accuracy: 52.20%\n",
      "Epoch: 0. Batch 3100/4317 - Avg Loss: 0.9316 - Accuracy: 52.24%\n",
      "Epoch: 0. Batch 3110/4317 - Avg Loss: 0.9315 - Accuracy: 52.25%\n",
      "Epoch: 0. Batch 3120/4317 - Avg Loss: 0.9311 - Accuracy: 52.26%\n",
      "Epoch: 0. Batch 3130/4317 - Avg Loss: 0.9310 - Accuracy: 52.29%\n",
      "Epoch: 0. Batch 3140/4317 - Avg Loss: 0.9304 - Accuracy: 52.33%\n",
      "Epoch: 0. Batch 3150/4317 - Avg Loss: 0.9301 - Accuracy: 52.34%\n",
      "Epoch: 0. Batch 3160/4317 - Avg Loss: 0.9298 - Accuracy: 52.38%\n",
      "Epoch: 0. Batch 3170/4317 - Avg Loss: 0.9295 - Accuracy: 52.39%\n",
      "Epoch: 0. Batch 3180/4317 - Avg Loss: 0.9295 - Accuracy: 52.40%\n",
      "Epoch: 0. Batch 3190/4317 - Avg Loss: 0.9291 - Accuracy: 52.43%\n",
      "Epoch: 0. Batch 3200/4317 - Avg Loss: 0.9288 - Accuracy: 52.44%\n",
      "Epoch: 0. Batch 3210/4317 - Avg Loss: 0.9287 - Accuracy: 52.46%\n",
      "Epoch: 0. Batch 3220/4317 - Avg Loss: 0.9285 - Accuracy: 52.48%\n",
      "Epoch: 0. Batch 3230/4317 - Avg Loss: 0.9282 - Accuracy: 52.50%\n",
      "Epoch: 0. Batch 3240/4317 - Avg Loss: 0.9279 - Accuracy: 52.53%\n",
      "Epoch: 0. Batch 3250/4317 - Avg Loss: 0.9277 - Accuracy: 52.55%\n",
      "Epoch: 0. Batch 3260/4317 - Avg Loss: 0.9275 - Accuracy: 52.55%\n",
      "Epoch: 0. Batch 3270/4317 - Avg Loss: 0.9271 - Accuracy: 52.58%\n",
      "Epoch: 0. Batch 3280/4317 - Avg Loss: 0.9271 - Accuracy: 52.60%\n",
      "Epoch: 0. Batch 3290/4317 - Avg Loss: 0.9268 - Accuracy: 52.62%\n",
      "Epoch: 0. Batch 3300/4317 - Avg Loss: 0.9262 - Accuracy: 52.67%\n",
      "Epoch: 0. Batch 3310/4317 - Avg Loss: 0.9261 - Accuracy: 52.68%\n",
      "Epoch: 0. Batch 3320/4317 - Avg Loss: 0.9257 - Accuracy: 52.70%\n",
      "Epoch: 0. Batch 3330/4317 - Avg Loss: 0.9255 - Accuracy: 52.72%\n",
      "Epoch: 0. Batch 3340/4317 - Avg Loss: 0.9251 - Accuracy: 52.75%\n",
      "Epoch: 0. Batch 3350/4317 - Avg Loss: 0.9249 - Accuracy: 52.78%\n",
      "Epoch: 0. Batch 3360/4317 - Avg Loss: 0.9247 - Accuracy: 52.80%\n",
      "Epoch: 0. Batch 3370/4317 - Avg Loss: 0.9244 - Accuracy: 52.84%\n",
      "Epoch: 0. Batch 3380/4317 - Avg Loss: 0.9241 - Accuracy: 52.86%\n",
      "Epoch: 0. Batch 3390/4317 - Avg Loss: 0.9235 - Accuracy: 52.89%\n",
      "Epoch: 0. Batch 3400/4317 - Avg Loss: 0.9231 - Accuracy: 52.92%\n",
      "Epoch: 0. Batch 3410/4317 - Avg Loss: 0.9229 - Accuracy: 52.93%\n",
      "Epoch: 0. Batch 3420/4317 - Avg Loss: 0.9225 - Accuracy: 52.95%\n",
      "Epoch: 0. Batch 3430/4317 - Avg Loss: 0.9218 - Accuracy: 53.00%\n",
      "Epoch: 0. Batch 3440/4317 - Avg Loss: 0.9214 - Accuracy: 53.03%\n",
      "Epoch: 0. Batch 3450/4317 - Avg Loss: 0.9212 - Accuracy: 53.06%\n",
      "Epoch: 0. Batch 3460/4317 - Avg Loss: 0.9210 - Accuracy: 53.09%\n",
      "Epoch: 0. Batch 3470/4317 - Avg Loss: 0.9204 - Accuracy: 53.12%\n",
      "Epoch: 0. Batch 3480/4317 - Avg Loss: 0.9201 - Accuracy: 53.14%\n",
      "Epoch: 0. Batch 3490/4317 - Avg Loss: 0.9197 - Accuracy: 53.16%\n",
      "Epoch: 0. Batch 3500/4317 - Avg Loss: 0.9194 - Accuracy: 53.18%\n",
      "Epoch: 0. Batch 3510/4317 - Avg Loss: 0.9194 - Accuracy: 53.22%\n",
      "Epoch: 0. Batch 3520/4317 - Avg Loss: 0.9191 - Accuracy: 53.25%\n",
      "Epoch: 0. Batch 3530/4317 - Avg Loss: 0.9188 - Accuracy: 53.26%\n",
      "Epoch: 0. Batch 3540/4317 - Avg Loss: 0.9184 - Accuracy: 53.29%\n",
      "Epoch: 0. Batch 3550/4317 - Avg Loss: 0.9178 - Accuracy: 53.33%\n",
      "Epoch: 0. Batch 3560/4317 - Avg Loss: 0.9174 - Accuracy: 53.36%\n",
      "Epoch: 0. Batch 3570/4317 - Avg Loss: 0.9168 - Accuracy: 53.40%\n",
      "Epoch: 0. Batch 3580/4317 - Avg Loss: 0.9165 - Accuracy: 53.42%\n",
      "Epoch: 0. Batch 3590/4317 - Avg Loss: 0.9162 - Accuracy: 53.46%\n",
      "Epoch: 0. Batch 3600/4317 - Avg Loss: 0.9157 - Accuracy: 53.49%\n",
      "Epoch: 0. Batch 3610/4317 - Avg Loss: 0.9156 - Accuracy: 53.51%\n",
      "Epoch: 0. Batch 3620/4317 - Avg Loss: 0.9155 - Accuracy: 53.52%\n",
      "Epoch: 0. Batch 3630/4317 - Avg Loss: 0.9153 - Accuracy: 53.54%\n",
      "Epoch: 0. Batch 3640/4317 - Avg Loss: 0.9150 - Accuracy: 53.55%\n",
      "Epoch: 0. Batch 3650/4317 - Avg Loss: 0.9148 - Accuracy: 53.57%\n",
      "Epoch: 0. Batch 3660/4317 - Avg Loss: 0.9147 - Accuracy: 53.59%\n",
      "Epoch: 0. Batch 3670/4317 - Avg Loss: 0.9144 - Accuracy: 53.61%\n",
      "Epoch: 0. Batch 3680/4317 - Avg Loss: 0.9141 - Accuracy: 53.64%\n",
      "Epoch: 0. Batch 3690/4317 - Avg Loss: 0.9138 - Accuracy: 53.66%\n",
      "Epoch: 0. Batch 3700/4317 - Avg Loss: 0.9134 - Accuracy: 53.70%\n",
      "Epoch: 0. Batch 3710/4317 - Avg Loss: 0.9130 - Accuracy: 53.73%\n",
      "Epoch: 0. Batch 3720/4317 - Avg Loss: 0.9127 - Accuracy: 53.76%\n",
      "Epoch: 0. Batch 3730/4317 - Avg Loss: 0.9124 - Accuracy: 53.78%\n",
      "Epoch: 0. Batch 3740/4317 - Avg Loss: 0.9123 - Accuracy: 53.79%\n",
      "Epoch: 0. Batch 3750/4317 - Avg Loss: 0.9118 - Accuracy: 53.80%\n",
      "Epoch: 0. Batch 3760/4317 - Avg Loss: 0.9116 - Accuracy: 53.82%\n",
      "Epoch: 0. Batch 3770/4317 - Avg Loss: 0.9112 - Accuracy: 53.85%\n",
      "Epoch: 0. Batch 3780/4317 - Avg Loss: 0.9109 - Accuracy: 53.86%\n",
      "Epoch: 0. Batch 3790/4317 - Avg Loss: 0.9107 - Accuracy: 53.88%\n",
      "Epoch: 0. Batch 3800/4317 - Avg Loss: 0.9105 - Accuracy: 53.89%\n",
      "Epoch: 0. Batch 3810/4317 - Avg Loss: 0.9100 - Accuracy: 53.94%\n",
      "Epoch: 0. Batch 3820/4317 - Avg Loss: 0.9096 - Accuracy: 53.96%\n",
      "Epoch: 0. Batch 3830/4317 - Avg Loss: 0.9094 - Accuracy: 53.97%\n",
      "Epoch: 0. Batch 3840/4317 - Avg Loss: 0.9094 - Accuracy: 53.99%\n",
      "Epoch: 0. Batch 3850/4317 - Avg Loss: 0.9088 - Accuracy: 54.04%\n",
      "Epoch: 0. Batch 3860/4317 - Avg Loss: 0.9082 - Accuracy: 54.08%\n",
      "Epoch: 0. Batch 3870/4317 - Avg Loss: 0.9079 - Accuracy: 54.10%\n",
      "Epoch: 0. Batch 3880/4317 - Avg Loss: 0.9077 - Accuracy: 54.11%\n",
      "Epoch: 0. Batch 3890/4317 - Avg Loss: 0.9072 - Accuracy: 54.15%\n",
      "Epoch: 0. Batch 3900/4317 - Avg Loss: 0.9067 - Accuracy: 54.18%\n",
      "Epoch: 0. Batch 3910/4317 - Avg Loss: 0.9066 - Accuracy: 54.18%\n",
      "Epoch: 0. Batch 3920/4317 - Avg Loss: 0.9061 - Accuracy: 54.21%\n",
      "Epoch: 0. Batch 3930/4317 - Avg Loss: 0.9055 - Accuracy: 54.24%\n",
      "Epoch: 0. Batch 3940/4317 - Avg Loss: 0.9051 - Accuracy: 54.28%\n",
      "Epoch: 0. Batch 3950/4317 - Avg Loss: 0.9048 - Accuracy: 54.31%\n",
      "Epoch: 0. Batch 3960/4317 - Avg Loss: 0.9045 - Accuracy: 54.33%\n",
      "Epoch: 0. Batch 3970/4317 - Avg Loss: 0.9044 - Accuracy: 54.34%\n",
      "Epoch: 0. Batch 3980/4317 - Avg Loss: 0.9040 - Accuracy: 54.38%\n",
      "Epoch: 0. Batch 3990/4317 - Avg Loss: 0.9037 - Accuracy: 54.40%\n",
      "Epoch: 0. Batch 4000/4317 - Avg Loss: 0.9035 - Accuracy: 54.43%\n",
      "Epoch: 0. Batch 4010/4317 - Avg Loss: 0.9032 - Accuracy: 54.44%\n",
      "Epoch: 0. Batch 4020/4317 - Avg Loss: 0.9028 - Accuracy: 54.48%\n",
      "Epoch: 0. Batch 4030/4317 - Avg Loss: 0.9023 - Accuracy: 54.51%\n",
      "Epoch: 0. Batch 4040/4317 - Avg Loss: 0.9018 - Accuracy: 54.53%\n",
      "Epoch: 0. Batch 4050/4317 - Avg Loss: 0.9015 - Accuracy: 54.56%\n",
      "Epoch: 0. Batch 4060/4317 - Avg Loss: 0.9010 - Accuracy: 54.59%\n",
      "Epoch: 0. Batch 4070/4317 - Avg Loss: 0.9005 - Accuracy: 54.62%\n",
      "Epoch: 0. Batch 4080/4317 - Avg Loss: 0.9003 - Accuracy: 54.63%\n",
      "Epoch: 0. Batch 4090/4317 - Avg Loss: 0.9000 - Accuracy: 54.67%\n",
      "Epoch: 0. Batch 4100/4317 - Avg Loss: 0.8996 - Accuracy: 54.68%\n",
      "Epoch: 0. Batch 4110/4317 - Avg Loss: 0.8991 - Accuracy: 54.71%\n",
      "Epoch: 0. Batch 4120/4317 - Avg Loss: 0.8987 - Accuracy: 54.73%\n",
      "Epoch: 0. Batch 4130/4317 - Avg Loss: 0.8986 - Accuracy: 54.75%\n",
      "Epoch: 0. Batch 4140/4317 - Avg Loss: 0.8983 - Accuracy: 54.78%\n",
      "Epoch: 0. Batch 4150/4317 - Avg Loss: 0.8978 - Accuracy: 54.81%\n",
      "Epoch: 0. Batch 4160/4317 - Avg Loss: 0.8974 - Accuracy: 54.84%\n",
      "Epoch: 0. Batch 4170/4317 - Avg Loss: 0.8972 - Accuracy: 54.87%\n",
      "Epoch: 0. Batch 4180/4317 - Avg Loss: 0.8970 - Accuracy: 54.89%\n",
      "Epoch: 0. Batch 4190/4317 - Avg Loss: 0.8968 - Accuracy: 54.92%\n",
      "Epoch: 0. Batch 4200/4317 - Avg Loss: 0.8964 - Accuracy: 54.95%\n",
      "Epoch: 0. Batch 4210/4317 - Avg Loss: 0.8959 - Accuracy: 54.99%\n",
      "Epoch: 0. Batch 4220/4317 - Avg Loss: 0.8957 - Accuracy: 55.01%\n",
      "Epoch: 0. Batch 4230/4317 - Avg Loss: 0.8954 - Accuracy: 55.03%\n",
      "Epoch: 0. Batch 4240/4317 - Avg Loss: 0.8949 - Accuracy: 55.06%\n",
      "Epoch: 0. Batch 4250/4317 - Avg Loss: 0.8947 - Accuracy: 55.08%\n",
      "Epoch: 0. Batch 4260/4317 - Avg Loss: 0.8944 - Accuracy: 55.10%\n",
      "Epoch: 0. Batch 4270/4317 - Avg Loss: 0.8944 - Accuracy: 55.09%\n",
      "Epoch: 0. Batch 4280/4317 - Avg Loss: 0.8940 - Accuracy: 55.11%\n",
      "Epoch: 0. Batch 4290/4317 - Avg Loss: 0.8938 - Accuracy: 55.14%\n",
      "Epoch: 0. Batch 4300/4317 - Avg Loss: 0.8935 - Accuracy: 55.17%\n",
      "Epoch: 0. Batch 4310/4317 - Avg Loss: 0.8933 - Accuracy: 55.18%\n",
      "Train loss: 0.8930 - Train accuracy: 55.20%\n",
      "Validation accuracy: 64.2814\n",
      "Epoch: 1. Batch 0/4317 - Avg Loss: 0.8976 - Accuracy: 50.00%\n",
      "Epoch: 1. Batch 10/4317 - Avg Loss: 0.6610 - Accuracy: 69.32%\n",
      "Epoch: 1. Batch 20/4317 - Avg Loss: 0.6568 - Accuracy: 69.35%\n",
      "Epoch: 1. Batch 30/4317 - Avg Loss: 0.6891 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 40/4317 - Avg Loss: 0.6649 - Accuracy: 69.82%\n",
      "Epoch: 1. Batch 50/4317 - Avg Loss: 0.6687 - Accuracy: 70.22%\n",
      "Epoch: 1. Batch 60/4317 - Avg Loss: 0.6833 - Accuracy: 69.16%\n",
      "Epoch: 1. Batch 70/4317 - Avg Loss: 0.6832 - Accuracy: 69.19%\n",
      "Epoch: 1. Batch 80/4317 - Avg Loss: 0.6938 - Accuracy: 68.90%\n",
      "Epoch: 1. Batch 90/4317 - Avg Loss: 0.7050 - Accuracy: 68.27%\n",
      "Epoch: 1. Batch 100/4317 - Avg Loss: 0.7096 - Accuracy: 68.56%\n",
      "Epoch: 1. Batch 110/4317 - Avg Loss: 0.6987 - Accuracy: 69.03%\n",
      "Epoch: 1. Batch 120/4317 - Avg Loss: 0.6928 - Accuracy: 69.52%\n",
      "Epoch: 1. Batch 130/4317 - Avg Loss: 0.6879 - Accuracy: 69.75%\n",
      "Epoch: 1. Batch 140/4317 - Avg Loss: 0.6901 - Accuracy: 69.37%\n",
      "Epoch: 1. Batch 150/4317 - Avg Loss: 0.6882 - Accuracy: 69.54%\n",
      "Epoch: 1. Batch 160/4317 - Avg Loss: 0.6867 - Accuracy: 69.64%\n",
      "Epoch: 1. Batch 170/4317 - Avg Loss: 0.6812 - Accuracy: 69.74%\n",
      "Epoch: 1. Batch 180/4317 - Avg Loss: 0.6778 - Accuracy: 69.75%\n",
      "Epoch: 1. Batch 190/4317 - Avg Loss: 0.6796 - Accuracy: 69.34%\n",
      "Epoch: 1. Batch 200/4317 - Avg Loss: 0.6808 - Accuracy: 69.31%\n",
      "Epoch: 1. Batch 210/4317 - Avg Loss: 0.6805 - Accuracy: 69.28%\n",
      "Epoch: 1. Batch 220/4317 - Avg Loss: 0.6827 - Accuracy: 69.17%\n",
      "Epoch: 1. Batch 230/4317 - Avg Loss: 0.6857 - Accuracy: 68.97%\n",
      "Epoch: 1. Batch 240/4317 - Avg Loss: 0.6842 - Accuracy: 68.96%\n",
      "Epoch: 1. Batch 250/4317 - Avg Loss: 0.6835 - Accuracy: 68.92%\n",
      "Epoch: 1. Batch 260/4317 - Avg Loss: 0.6822 - Accuracy: 68.97%\n",
      "Epoch: 1. Batch 270/4317 - Avg Loss: 0.6836 - Accuracy: 68.93%\n",
      "Epoch: 1. Batch 280/4317 - Avg Loss: 0.6831 - Accuracy: 68.93%\n",
      "Epoch: 1. Batch 290/4317 - Avg Loss: 0.6838 - Accuracy: 68.88%\n",
      "Epoch: 1. Batch 300/4317 - Avg Loss: 0.6857 - Accuracy: 68.71%\n",
      "Epoch: 1. Batch 310/4317 - Avg Loss: 0.6851 - Accuracy: 68.73%\n",
      "Epoch: 1. Batch 320/4317 - Avg Loss: 0.6826 - Accuracy: 68.89%\n",
      "Epoch: 1. Batch 330/4317 - Avg Loss: 0.6855 - Accuracy: 68.73%\n",
      "Epoch: 1. Batch 340/4317 - Avg Loss: 0.6845 - Accuracy: 68.82%\n",
      "Epoch: 1. Batch 350/4317 - Avg Loss: 0.6840 - Accuracy: 68.91%\n",
      "Epoch: 1. Batch 360/4317 - Avg Loss: 0.6837 - Accuracy: 68.99%\n",
      "Epoch: 1. Batch 370/4317 - Avg Loss: 0.6825 - Accuracy: 69.05%\n",
      "Epoch: 1. Batch 380/4317 - Avg Loss: 0.6831 - Accuracy: 68.95%\n",
      "Epoch: 1. Batch 390/4317 - Avg Loss: 0.6813 - Accuracy: 68.94%\n",
      "Epoch: 1. Batch 400/4317 - Avg Loss: 0.6804 - Accuracy: 69.06%\n",
      "Epoch: 1. Batch 410/4317 - Avg Loss: 0.6789 - Accuracy: 69.11%\n",
      "Epoch: 1. Batch 420/4317 - Avg Loss: 0.6771 - Accuracy: 69.21%\n",
      "Epoch: 1. Batch 430/4317 - Avg Loss: 0.6767 - Accuracy: 69.20%\n",
      "Epoch: 1. Batch 440/4317 - Avg Loss: 0.6758 - Accuracy: 69.27%\n",
      "Epoch: 1. Batch 450/4317 - Avg Loss: 0.6743 - Accuracy: 69.47%\n",
      "Epoch: 1. Batch 460/4317 - Avg Loss: 0.6763 - Accuracy: 69.33%\n",
      "Epoch: 1. Batch 470/4317 - Avg Loss: 0.6759 - Accuracy: 69.32%\n",
      "Epoch: 1. Batch 480/4317 - Avg Loss: 0.6754 - Accuracy: 69.27%\n",
      "Epoch: 1. Batch 490/4317 - Avg Loss: 0.6740 - Accuracy: 69.30%\n",
      "Epoch: 1. Batch 500/4317 - Avg Loss: 0.6727 - Accuracy: 69.36%\n",
      "Epoch: 1. Batch 510/4317 - Avg Loss: 0.6753 - Accuracy: 69.23%\n",
      "Epoch: 1. Batch 520/4317 - Avg Loss: 0.6770 - Accuracy: 69.19%\n",
      "Epoch: 1. Batch 530/4317 - Avg Loss: 0.6768 - Accuracy: 69.22%\n",
      "Epoch: 1. Batch 540/4317 - Avg Loss: 0.6766 - Accuracy: 69.27%\n",
      "Epoch: 1. Batch 550/4317 - Avg Loss: 0.6769 - Accuracy: 69.29%\n",
      "Epoch: 1. Batch 560/4317 - Avg Loss: 0.6777 - Accuracy: 69.33%\n",
      "Epoch: 1. Batch 570/4317 - Avg Loss: 0.6788 - Accuracy: 69.30%\n",
      "Epoch: 1. Batch 580/4317 - Avg Loss: 0.6796 - Accuracy: 69.21%\n",
      "Epoch: 1. Batch 590/4317 - Avg Loss: 0.6806 - Accuracy: 69.16%\n",
      "Epoch: 1. Batch 600/4317 - Avg Loss: 0.6806 - Accuracy: 69.16%\n",
      "Epoch: 1. Batch 610/4317 - Avg Loss: 0.6809 - Accuracy: 69.13%\n",
      "Epoch: 1. Batch 620/4317 - Avg Loss: 0.6804 - Accuracy: 69.17%\n",
      "Epoch: 1. Batch 630/4317 - Avg Loss: 0.6805 - Accuracy: 69.12%\n",
      "Epoch: 1. Batch 640/4317 - Avg Loss: 0.6803 - Accuracy: 69.18%\n",
      "Epoch: 1. Batch 650/4317 - Avg Loss: 0.6804 - Accuracy: 69.11%\n",
      "Epoch: 1. Batch 660/4317 - Avg Loss: 0.6815 - Accuracy: 69.10%\n",
      "Epoch: 1. Batch 670/4317 - Avg Loss: 0.6821 - Accuracy: 69.12%\n",
      "Epoch: 1. Batch 680/4317 - Avg Loss: 0.6809 - Accuracy: 69.22%\n",
      "Epoch: 1. Batch 690/4317 - Avg Loss: 0.6822 - Accuracy: 69.15%\n",
      "Epoch: 1. Batch 700/4317 - Avg Loss: 0.6823 - Accuracy: 69.14%\n",
      "Epoch: 1. Batch 710/4317 - Avg Loss: 0.6812 - Accuracy: 69.17%\n",
      "Epoch: 1. Batch 720/4317 - Avg Loss: 0.6814 - Accuracy: 69.20%\n",
      "Epoch: 1. Batch 730/4317 - Avg Loss: 0.6806 - Accuracy: 69.26%\n",
      "Epoch: 1. Batch 740/4317 - Avg Loss: 0.6796 - Accuracy: 69.32%\n",
      "Epoch: 1. Batch 750/4317 - Avg Loss: 0.6792 - Accuracy: 69.38%\n",
      "Epoch: 1. Batch 760/4317 - Avg Loss: 0.6784 - Accuracy: 69.33%\n",
      "Epoch: 1. Batch 770/4317 - Avg Loss: 0.6791 - Accuracy: 69.31%\n",
      "Epoch: 1. Batch 780/4317 - Avg Loss: 0.6799 - Accuracy: 69.26%\n",
      "Epoch: 1. Batch 790/4317 - Avg Loss: 0.6803 - Accuracy: 69.25%\n",
      "Epoch: 1. Batch 800/4317 - Avg Loss: 0.6803 - Accuracy: 69.21%\n",
      "Epoch: 1. Batch 810/4317 - Avg Loss: 0.6812 - Accuracy: 69.09%\n",
      "Epoch: 1. Batch 820/4317 - Avg Loss: 0.6816 - Accuracy: 69.05%\n",
      "Epoch: 1. Batch 830/4317 - Avg Loss: 0.6819 - Accuracy: 69.01%\n",
      "Epoch: 1. Batch 840/4317 - Avg Loss: 0.6821 - Accuracy: 69.03%\n",
      "Epoch: 1. Batch 850/4317 - Avg Loss: 0.6827 - Accuracy: 68.94%\n",
      "Epoch: 1. Batch 860/4317 - Avg Loss: 0.6822 - Accuracy: 68.98%\n",
      "Epoch: 1. Batch 870/4317 - Avg Loss: 0.6826 - Accuracy: 68.95%\n",
      "Epoch: 1. Batch 880/4317 - Avg Loss: 0.6823 - Accuracy: 68.94%\n",
      "Epoch: 1. Batch 890/4317 - Avg Loss: 0.6829 - Accuracy: 68.92%\n",
      "Epoch: 1. Batch 900/4317 - Avg Loss: 0.6830 - Accuracy: 68.87%\n",
      "Epoch: 1. Batch 910/4317 - Avg Loss: 0.6830 - Accuracy: 68.89%\n",
      "Epoch: 1. Batch 920/4317 - Avg Loss: 0.6834 - Accuracy: 68.91%\n",
      "Epoch: 1. Batch 930/4317 - Avg Loss: 0.6827 - Accuracy: 68.97%\n",
      "Epoch: 1. Batch 940/4317 - Avg Loss: 0.6823 - Accuracy: 68.99%\n",
      "Epoch: 1. Batch 950/4317 - Avg Loss: 0.6825 - Accuracy: 68.93%\n",
      "Epoch: 1. Batch 960/4317 - Avg Loss: 0.6822 - Accuracy: 68.95%\n",
      "Epoch: 1. Batch 970/4317 - Avg Loss: 0.6827 - Accuracy: 68.93%\n",
      "Epoch: 1. Batch 980/4317 - Avg Loss: 0.6842 - Accuracy: 68.86%\n",
      "Epoch: 1. Batch 990/4317 - Avg Loss: 0.6846 - Accuracy: 68.79%\n",
      "Epoch: 1. Batch 1000/4317 - Avg Loss: 0.6841 - Accuracy: 68.82%\n",
      "Epoch: 1. Batch 1010/4317 - Avg Loss: 0.6838 - Accuracy: 68.78%\n",
      "Epoch: 1. Batch 1020/4317 - Avg Loss: 0.6838 - Accuracy: 68.79%\n",
      "Epoch: 1. Batch 1030/4317 - Avg Loss: 0.6840 - Accuracy: 68.76%\n",
      "Epoch: 1. Batch 1040/4317 - Avg Loss: 0.6853 - Accuracy: 68.70%\n",
      "Epoch: 1. Batch 1050/4317 - Avg Loss: 0.6864 - Accuracy: 68.64%\n",
      "Epoch: 1. Batch 1060/4317 - Avg Loss: 0.6862 - Accuracy: 68.67%\n",
      "Epoch: 1. Batch 1070/4317 - Avg Loss: 0.6861 - Accuracy: 68.67%\n",
      "Epoch: 1. Batch 1080/4317 - Avg Loss: 0.6864 - Accuracy: 68.64%\n",
      "Epoch: 1. Batch 1090/4317 - Avg Loss: 0.6860 - Accuracy: 68.67%\n",
      "Epoch: 1. Batch 1100/4317 - Avg Loss: 0.6859 - Accuracy: 68.65%\n",
      "Epoch: 1. Batch 1110/4317 - Avg Loss: 0.6860 - Accuracy: 68.67%\n",
      "Epoch: 1. Batch 1120/4317 - Avg Loss: 0.6860 - Accuracy: 68.65%\n",
      "Epoch: 1. Batch 1130/4317 - Avg Loss: 0.6862 - Accuracy: 68.63%\n",
      "Epoch: 1. Batch 1140/4317 - Avg Loss: 0.6880 - Accuracy: 68.57%\n",
      "Epoch: 1. Batch 1150/4317 - Avg Loss: 0.6886 - Accuracy: 68.51%\n",
      "Epoch: 1. Batch 1160/4317 - Avg Loss: 0.6893 - Accuracy: 68.47%\n",
      "Epoch: 1. Batch 1170/4317 - Avg Loss: 0.6889 - Accuracy: 68.51%\n",
      "Epoch: 1. Batch 1180/4317 - Avg Loss: 0.6882 - Accuracy: 68.58%\n",
      "Epoch: 1. Batch 1190/4317 - Avg Loss: 0.6885 - Accuracy: 68.52%\n",
      "Epoch: 1. Batch 1200/4317 - Avg Loss: 0.6883 - Accuracy: 68.53%\n",
      "Epoch: 1. Batch 1210/4317 - Avg Loss: 0.6879 - Accuracy: 68.57%\n",
      "Epoch: 1. Batch 1220/4317 - Avg Loss: 0.6887 - Accuracy: 68.53%\n",
      "Epoch: 1. Batch 1230/4317 - Avg Loss: 0.6885 - Accuracy: 68.52%\n",
      "Epoch: 1. Batch 1240/4317 - Avg Loss: 0.6890 - Accuracy: 68.50%\n",
      "Epoch: 1. Batch 1250/4317 - Avg Loss: 0.6893 - Accuracy: 68.48%\n",
      "Epoch: 1. Batch 1260/4317 - Avg Loss: 0.6895 - Accuracy: 68.46%\n",
      "Epoch: 1. Batch 1270/4317 - Avg Loss: 0.6898 - Accuracy: 68.46%\n",
      "Epoch: 1. Batch 1280/4317 - Avg Loss: 0.6894 - Accuracy: 68.49%\n",
      "Epoch: 1. Batch 1290/4317 - Avg Loss: 0.6898 - Accuracy: 68.50%\n",
      "Epoch: 1. Batch 1300/4317 - Avg Loss: 0.6902 - Accuracy: 68.49%\n",
      "Epoch: 1. Batch 1310/4317 - Avg Loss: 0.6906 - Accuracy: 68.45%\n",
      "Epoch: 1. Batch 1320/4317 - Avg Loss: 0.6911 - Accuracy: 68.43%\n",
      "Epoch: 1. Batch 1330/4317 - Avg Loss: 0.6915 - Accuracy: 68.41%\n",
      "Epoch: 1. Batch 1340/4317 - Avg Loss: 0.6915 - Accuracy: 68.41%\n",
      "Epoch: 1. Batch 1350/4317 - Avg Loss: 0.6921 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1360/4317 - Avg Loss: 0.6924 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1370/4317 - Avg Loss: 0.6922 - Accuracy: 68.38%\n",
      "Epoch: 1. Batch 1380/4317 - Avg Loss: 0.6920 - Accuracy: 68.39%\n",
      "Epoch: 1. Batch 1390/4317 - Avg Loss: 0.6923 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1400/4317 - Avg Loss: 0.6926 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1410/4317 - Avg Loss: 0.6931 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1420/4317 - Avg Loss: 0.6931 - Accuracy: 68.29%\n",
      "Epoch: 1. Batch 1430/4317 - Avg Loss: 0.6928 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1440/4317 - Avg Loss: 0.6925 - Accuracy: 68.31%\n",
      "Epoch: 1. Batch 1450/4317 - Avg Loss: 0.6923 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1460/4317 - Avg Loss: 0.6925 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1470/4317 - Avg Loss: 0.6923 - Accuracy: 68.31%\n",
      "Epoch: 1. Batch 1480/4317 - Avg Loss: 0.6923 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1490/4317 - Avg Loss: 0.6918 - Accuracy: 68.39%\n",
      "Epoch: 1. Batch 1500/4317 - Avg Loss: 0.6916 - Accuracy: 68.43%\n",
      "Epoch: 1. Batch 1510/4317 - Avg Loss: 0.6918 - Accuracy: 68.42%\n",
      "Epoch: 1. Batch 1520/4317 - Avg Loss: 0.6918 - Accuracy: 68.43%\n",
      "Epoch: 1. Batch 1530/4317 - Avg Loss: 0.6914 - Accuracy: 68.45%\n",
      "Epoch: 1. Batch 1540/4317 - Avg Loss: 0.6914 - Accuracy: 68.43%\n",
      "Epoch: 1. Batch 1550/4317 - Avg Loss: 0.6917 - Accuracy: 68.40%\n",
      "Epoch: 1. Batch 1560/4317 - Avg Loss: 0.6920 - Accuracy: 68.38%\n",
      "Epoch: 1. Batch 1570/4317 - Avg Loss: 0.6921 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1580/4317 - Avg Loss: 0.6923 - Accuracy: 68.38%\n",
      "Epoch: 1. Batch 1590/4317 - Avg Loss: 0.6930 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1600/4317 - Avg Loss: 0.6931 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1610/4317 - Avg Loss: 0.6924 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1620/4317 - Avg Loss: 0.6924 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1630/4317 - Avg Loss: 0.6927 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1640/4317 - Avg Loss: 0.6927 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1650/4317 - Avg Loss: 0.6933 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1660/4317 - Avg Loss: 0.6928 - Accuracy: 68.40%\n",
      "Epoch: 1. Batch 1670/4317 - Avg Loss: 0.6930 - Accuracy: 68.41%\n",
      "Epoch: 1. Batch 1680/4317 - Avg Loss: 0.6930 - Accuracy: 68.40%\n",
      "Epoch: 1. Batch 1690/4317 - Avg Loss: 0.6933 - Accuracy: 68.38%\n",
      "Epoch: 1. Batch 1700/4317 - Avg Loss: 0.6936 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1710/4317 - Avg Loss: 0.6935 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1720/4317 - Avg Loss: 0.6939 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1730/4317 - Avg Loss: 0.6937 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1740/4317 - Avg Loss: 0.6935 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1750/4317 - Avg Loss: 0.6934 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1760/4317 - Avg Loss: 0.6937 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1770/4317 - Avg Loss: 0.6934 - Accuracy: 68.39%\n",
      "Epoch: 1. Batch 1780/4317 - Avg Loss: 0.6937 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1790/4317 - Avg Loss: 0.6940 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1800/4317 - Avg Loss: 0.6940 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1810/4317 - Avg Loss: 0.6936 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1820/4317 - Avg Loss: 0.6933 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1830/4317 - Avg Loss: 0.6930 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1840/4317 - Avg Loss: 0.6930 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1850/4317 - Avg Loss: 0.6932 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1860/4317 - Avg Loss: 0.6944 - Accuracy: 68.31%\n",
      "Epoch: 1. Batch 1870/4317 - Avg Loss: 0.6946 - Accuracy: 68.29%\n",
      "Epoch: 1. Batch 1880/4317 - Avg Loss: 0.6942 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1890/4317 - Avg Loss: 0.6941 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1900/4317 - Avg Loss: 0.6942 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1910/4317 - Avg Loss: 0.6944 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1920/4317 - Avg Loss: 0.6944 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1930/4317 - Avg Loss: 0.6939 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1940/4317 - Avg Loss: 0.6940 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1950/4317 - Avg Loss: 0.6941 - Accuracy: 68.27%\n",
      "Epoch: 1. Batch 1960/4317 - Avg Loss: 0.6941 - Accuracy: 68.27%\n",
      "Epoch: 1. Batch 1970/4317 - Avg Loss: 0.6941 - Accuracy: 68.27%\n",
      "Epoch: 1. Batch 1980/4317 - Avg Loss: 0.6941 - Accuracy: 68.26%\n",
      "Epoch: 1. Batch 1990/4317 - Avg Loss: 0.6940 - Accuracy: 68.25%\n",
      "Epoch: 1. Batch 2000/4317 - Avg Loss: 0.6945 - Accuracy: 68.22%\n",
      "Epoch: 1. Batch 2010/4317 - Avg Loss: 0.6946 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 2020/4317 - Avg Loss: 0.6943 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 2030/4317 - Avg Loss: 0.6945 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 2040/4317 - Avg Loss: 0.6953 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2050/4317 - Avg Loss: 0.6954 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2060/4317 - Avg Loss: 0.6950 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2070/4317 - Avg Loss: 0.6951 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2080/4317 - Avg Loss: 0.6953 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2090/4317 - Avg Loss: 0.6954 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2100/4317 - Avg Loss: 0.6954 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2110/4317 - Avg Loss: 0.6959 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2120/4317 - Avg Loss: 0.6959 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2130/4317 - Avg Loss: 0.6961 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2140/4317 - Avg Loss: 0.6966 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2150/4317 - Avg Loss: 0.6965 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2160/4317 - Avg Loss: 0.6967 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2170/4317 - Avg Loss: 0.6966 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 2180/4317 - Avg Loss: 0.6971 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2190/4317 - Avg Loss: 0.6964 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2200/4317 - Avg Loss: 0.6969 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2210/4317 - Avg Loss: 0.6969 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2220/4317 - Avg Loss: 0.6969 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2230/4317 - Avg Loss: 0.6964 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2240/4317 - Avg Loss: 0.6963 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2250/4317 - Avg Loss: 0.6965 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2260/4317 - Avg Loss: 0.6962 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2270/4317 - Avg Loss: 0.6963 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2280/4317 - Avg Loss: 0.6959 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2290/4317 - Avg Loss: 0.6960 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2300/4317 - Avg Loss: 0.6960 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2310/4317 - Avg Loss: 0.6958 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2320/4317 - Avg Loss: 0.6956 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2330/4317 - Avg Loss: 0.6962 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2340/4317 - Avg Loss: 0.6964 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 2350/4317 - Avg Loss: 0.6965 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2360/4317 - Avg Loss: 0.6967 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2370/4317 - Avg Loss: 0.6973 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2380/4317 - Avg Loss: 0.6979 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2390/4317 - Avg Loss: 0.6975 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2400/4317 - Avg Loss: 0.6978 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2410/4317 - Avg Loss: 0.6979 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2420/4317 - Avg Loss: 0.6979 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2430/4317 - Avg Loss: 0.6975 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2440/4317 - Avg Loss: 0.6978 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2450/4317 - Avg Loss: 0.6979 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2460/4317 - Avg Loss: 0.6981 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2470/4317 - Avg Loss: 0.6978 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2480/4317 - Avg Loss: 0.6976 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2490/4317 - Avg Loss: 0.6980 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2500/4317 - Avg Loss: 0.6978 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2510/4317 - Avg Loss: 0.6978 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2520/4317 - Avg Loss: 0.6977 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2530/4317 - Avg Loss: 0.6974 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2540/4317 - Avg Loss: 0.6976 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2550/4317 - Avg Loss: 0.6976 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2560/4317 - Avg Loss: 0.6975 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2570/4317 - Avg Loss: 0.6977 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2580/4317 - Avg Loss: 0.6977 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2590/4317 - Avg Loss: 0.6977 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2600/4317 - Avg Loss: 0.6979 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2610/4317 - Avg Loss: 0.6978 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2620/4317 - Avg Loss: 0.6980 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2630/4317 - Avg Loss: 0.6976 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2640/4317 - Avg Loss: 0.6978 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2650/4317 - Avg Loss: 0.6979 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2660/4317 - Avg Loss: 0.6978 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2670/4317 - Avg Loss: 0.6983 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2680/4317 - Avg Loss: 0.6983 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2690/4317 - Avg Loss: 0.6984 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2700/4317 - Avg Loss: 0.6982 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2710/4317 - Avg Loss: 0.6979 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2720/4317 - Avg Loss: 0.6981 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2730/4317 - Avg Loss: 0.6980 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2740/4317 - Avg Loss: 0.6979 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2750/4317 - Avg Loss: 0.6984 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 2760/4317 - Avg Loss: 0.6982 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2770/4317 - Avg Loss: 0.6979 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2780/4317 - Avg Loss: 0.6980 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2790/4317 - Avg Loss: 0.6982 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2800/4317 - Avg Loss: 0.6983 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2810/4317 - Avg Loss: 0.6983 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2820/4317 - Avg Loss: 0.6984 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2830/4317 - Avg Loss: 0.6986 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2840/4317 - Avg Loss: 0.6984 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2850/4317 - Avg Loss: 0.6983 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2860/4317 - Avg Loss: 0.6979 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2870/4317 - Avg Loss: 0.6980 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2880/4317 - Avg Loss: 0.6983 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2890/4317 - Avg Loss: 0.6982 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2900/4317 - Avg Loss: 0.6982 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 2910/4317 - Avg Loss: 0.6986 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 2920/4317 - Avg Loss: 0.6988 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 2930/4317 - Avg Loss: 0.6988 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 2940/4317 - Avg Loss: 0.6987 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 2950/4317 - Avg Loss: 0.6989 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 2960/4317 - Avg Loss: 0.6988 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 2970/4317 - Avg Loss: 0.6989 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 2980/4317 - Avg Loss: 0.6987 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 2990/4317 - Avg Loss: 0.6984 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3000/4317 - Avg Loss: 0.6983 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3010/4317 - Avg Loss: 0.6986 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3020/4317 - Avg Loss: 0.6985 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3030/4317 - Avg Loss: 0.6985 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3040/4317 - Avg Loss: 0.6985 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3050/4317 - Avg Loss: 0.6982 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3060/4317 - Avg Loss: 0.6979 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3070/4317 - Avg Loss: 0.6980 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3080/4317 - Avg Loss: 0.6980 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3090/4317 - Avg Loss: 0.6984 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3100/4317 - Avg Loss: 0.6984 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3110/4317 - Avg Loss: 0.6982 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3120/4317 - Avg Loss: 0.6980 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3130/4317 - Avg Loss: 0.6979 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3140/4317 - Avg Loss: 0.6981 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3150/4317 - Avg Loss: 0.6979 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3160/4317 - Avg Loss: 0.6980 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3170/4317 - Avg Loss: 0.6979 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3180/4317 - Avg Loss: 0.6982 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3190/4317 - Avg Loss: 0.6983 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3200/4317 - Avg Loss: 0.6986 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3210/4317 - Avg Loss: 0.6985 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3220/4317 - Avg Loss: 0.6984 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3230/4317 - Avg Loss: 0.6983 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3240/4317 - Avg Loss: 0.6982 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3250/4317 - Avg Loss: 0.6983 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3260/4317 - Avg Loss: 0.6985 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3270/4317 - Avg Loss: 0.6988 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3280/4317 - Avg Loss: 0.6989 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3290/4317 - Avg Loss: 0.6987 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3300/4317 - Avg Loss: 0.6989 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3310/4317 - Avg Loss: 0.6989 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3320/4317 - Avg Loss: 0.6988 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3330/4317 - Avg Loss: 0.6987 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3340/4317 - Avg Loss: 0.6988 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3350/4317 - Avg Loss: 0.6989 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3360/4317 - Avg Loss: 0.6986 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3370/4317 - Avg Loss: 0.6986 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3380/4317 - Avg Loss: 0.6988 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3390/4317 - Avg Loss: 0.6987 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3400/4317 - Avg Loss: 0.6986 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3410/4317 - Avg Loss: 0.6986 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3420/4317 - Avg Loss: 0.6989 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3430/4317 - Avg Loss: 0.6989 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3440/4317 - Avg Loss: 0.6987 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3450/4317 - Avg Loss: 0.6985 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3460/4317 - Avg Loss: 0.6987 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3470/4317 - Avg Loss: 0.6989 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3480/4317 - Avg Loss: 0.6989 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3490/4317 - Avg Loss: 0.6989 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3500/4317 - Avg Loss: 0.6987 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3510/4317 - Avg Loss: 0.6988 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3520/4317 - Avg Loss: 0.6989 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3530/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3540/4317 - Avg Loss: 0.6988 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3550/4317 - Avg Loss: 0.6989 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3560/4317 - Avg Loss: 0.6988 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3570/4317 - Avg Loss: 0.6990 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3580/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3590/4317 - Avg Loss: 0.6989 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3600/4317 - Avg Loss: 0.6988 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3610/4317 - Avg Loss: 0.6989 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3620/4317 - Avg Loss: 0.6988 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3630/4317 - Avg Loss: 0.6988 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3640/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3650/4317 - Avg Loss: 0.6988 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3660/4317 - Avg Loss: 0.6987 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3670/4317 - Avg Loss: 0.6985 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3680/4317 - Avg Loss: 0.6984 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3690/4317 - Avg Loss: 0.6988 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3700/4317 - Avg Loss: 0.6987 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3710/4317 - Avg Loss: 0.6985 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3720/4317 - Avg Loss: 0.6986 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3730/4317 - Avg Loss: 0.6987 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3740/4317 - Avg Loss: 0.6986 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3750/4317 - Avg Loss: 0.6984 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3760/4317 - Avg Loss: 0.6984 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3770/4317 - Avg Loss: 0.6986 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3780/4317 - Avg Loss: 0.6983 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3790/4317 - Avg Loss: 0.6982 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3800/4317 - Avg Loss: 0.6980 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3810/4317 - Avg Loss: 0.6982 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 3820/4317 - Avg Loss: 0.6978 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3830/4317 - Avg Loss: 0.6979 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3840/4317 - Avg Loss: 0.6979 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3850/4317 - Avg Loss: 0.6981 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3860/4317 - Avg Loss: 0.6980 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3870/4317 - Avg Loss: 0.6980 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3880/4317 - Avg Loss: 0.6982 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3890/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3900/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3910/4317 - Avg Loss: 0.6987 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3920/4317 - Avg Loss: 0.6990 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3930/4317 - Avg Loss: 0.6992 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3940/4317 - Avg Loss: 0.6991 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3950/4317 - Avg Loss: 0.6991 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3960/4317 - Avg Loss: 0.6993 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3970/4317 - Avg Loss: 0.6994 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3980/4317 - Avg Loss: 0.6995 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3990/4317 - Avg Loss: 0.6995 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4000/4317 - Avg Loss: 0.6993 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4010/4317 - Avg Loss: 0.6993 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4020/4317 - Avg Loss: 0.6997 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4030/4317 - Avg Loss: 0.6998 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4040/4317 - Avg Loss: 0.6995 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4050/4317 - Avg Loss: 0.6998 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4060/4317 - Avg Loss: 0.6997 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4070/4317 - Avg Loss: 0.6997 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4080/4317 - Avg Loss: 0.6998 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4090/4317 - Avg Loss: 0.7002 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 4100/4317 - Avg Loss: 0.7001 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4110/4317 - Avg Loss: 0.7000 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4120/4317 - Avg Loss: 0.7001 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 4130/4317 - Avg Loss: 0.7001 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 4140/4317 - Avg Loss: 0.7002 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 4150/4317 - Avg Loss: 0.7001 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4160/4317 - Avg Loss: 0.7001 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 4170/4317 - Avg Loss: 0.6998 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4180/4317 - Avg Loss: 0.6999 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4190/4317 - Avg Loss: 0.6999 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 4200/4317 - Avg Loss: 0.6999 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4210/4317 - Avg Loss: 0.6998 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4220/4317 - Avg Loss: 0.6997 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4230/4317 - Avg Loss: 0.6992 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 4240/4317 - Avg Loss: 0.6990 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 4250/4317 - Avg Loss: 0.6990 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 4260/4317 - Avg Loss: 0.6990 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 4270/4317 - Avg Loss: 0.6990 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 4280/4317 - Avg Loss: 0.6991 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4290/4317 - Avg Loss: 0.6991 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 4300/4317 - Avg Loss: 0.6994 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 4310/4317 - Avg Loss: 0.6994 - Accuracy: 67.96%\n",
      "Train loss: 0.6993 - Train accuracy: 67.95%\n",
      "Validation accuracy: 64.3827\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.3649%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 55.20%        | 64.28%              |\n",
    "| **Epoch 2** | 67.95%        | 64.38%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** and **validation accuracy** are stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_sentiment_model/gru_sentiment_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
