{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdbbd0282f43527",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4317 - Avg Loss: 1.0957 - Accuracy: 37.50%\n",
      "Epoch: 0. Batch 10/4317 - Avg Loss: 1.4136 - Accuracy: 35.23%\n",
      "Epoch: 0. Batch 20/4317 - Avg Loss: 1.3101 - Accuracy: 31.25%\n",
      "Epoch: 0. Batch 30/4317 - Avg Loss: 1.2568 - Accuracy: 32.26%\n",
      "Epoch: 0. Batch 40/4317 - Avg Loss: 1.2238 - Accuracy: 30.34%\n",
      "Epoch: 0. Batch 50/4317 - Avg Loss: 1.2012 - Accuracy: 31.50%\n",
      "Epoch: 0. Batch 60/4317 - Avg Loss: 1.1889 - Accuracy: 31.97%\n",
      "Epoch: 0. Batch 70/4317 - Avg Loss: 1.1801 - Accuracy: 30.81%\n",
      "Epoch: 0. Batch 80/4317 - Avg Loss: 1.1720 - Accuracy: 31.71%\n",
      "Epoch: 0. Batch 90/4317 - Avg Loss: 1.1705 - Accuracy: 31.46%\n",
      "Epoch: 0. Batch 100/4317 - Avg Loss: 1.1693 - Accuracy: 31.56%\n",
      "Epoch: 0. Batch 110/4317 - Avg Loss: 1.1681 - Accuracy: 32.38%\n",
      "Epoch: 0. Batch 120/4317 - Avg Loss: 1.1644 - Accuracy: 31.56%\n",
      "Epoch: 0. Batch 130/4317 - Avg Loss: 1.1603 - Accuracy: 32.35%\n",
      "Epoch: 0. Batch 140/4317 - Avg Loss: 1.1584 - Accuracy: 32.54%\n",
      "Epoch: 0. Batch 150/4317 - Avg Loss: 1.1566 - Accuracy: 32.12%\n",
      "Epoch: 0. Batch 160/4317 - Avg Loss: 1.1537 - Accuracy: 32.45%\n",
      "Epoch: 0. Batch 170/4317 - Avg Loss: 1.1525 - Accuracy: 32.64%\n",
      "Epoch: 0. Batch 180/4317 - Avg Loss: 1.1514 - Accuracy: 32.80%\n",
      "Epoch: 0. Batch 190/4317 - Avg Loss: 1.1500 - Accuracy: 33.02%\n",
      "Epoch: 0. Batch 200/4317 - Avg Loss: 1.1482 - Accuracy: 32.68%\n",
      "Epoch: 0. Batch 210/4317 - Avg Loss: 1.1460 - Accuracy: 32.88%\n",
      "Epoch: 0. Batch 220/4317 - Avg Loss: 1.1437 - Accuracy: 33.60%\n",
      "Epoch: 0. Batch 230/4317 - Avg Loss: 1.1424 - Accuracy: 33.93%\n",
      "Epoch: 0. Batch 240/4317 - Avg Loss: 1.1405 - Accuracy: 33.92%\n",
      "Epoch: 0. Batch 250/4317 - Avg Loss: 1.1396 - Accuracy: 33.49%\n",
      "Epoch: 0. Batch 260/4317 - Avg Loss: 1.1386 - Accuracy: 33.84%\n",
      "Epoch: 0. Batch 270/4317 - Avg Loss: 1.1374 - Accuracy: 33.97%\n",
      "Epoch: 0. Batch 280/4317 - Avg Loss: 1.1363 - Accuracy: 33.70%\n",
      "Epoch: 0. Batch 290/4317 - Avg Loss: 1.1352 - Accuracy: 33.55%\n",
      "Epoch: 0. Batch 300/4317 - Avg Loss: 1.1344 - Accuracy: 33.55%\n",
      "Epoch: 0. Batch 310/4317 - Avg Loss: 1.1337 - Accuracy: 33.44%\n",
      "Epoch: 0. Batch 320/4317 - Avg Loss: 1.1329 - Accuracy: 33.31%\n",
      "Epoch: 0. Batch 330/4317 - Avg Loss: 1.1325 - Accuracy: 33.46%\n",
      "Epoch: 0. Batch 340/4317 - Avg Loss: 1.1319 - Accuracy: 33.41%\n",
      "Epoch: 0. Batch 350/4317 - Avg Loss: 1.1312 - Accuracy: 33.65%\n",
      "Epoch: 0. Batch 360/4317 - Avg Loss: 1.1300 - Accuracy: 33.71%\n",
      "Epoch: 0. Batch 370/4317 - Avg Loss: 1.1299 - Accuracy: 33.61%\n",
      "Epoch: 0. Batch 380/4317 - Avg Loss: 1.1295 - Accuracy: 33.61%\n",
      "Epoch: 0. Batch 390/4317 - Avg Loss: 1.1288 - Accuracy: 33.73%\n",
      "Epoch: 0. Batch 400/4317 - Avg Loss: 1.1282 - Accuracy: 33.87%\n",
      "Epoch: 0. Batch 410/4317 - Avg Loss: 1.1275 - Accuracy: 33.93%\n",
      "Epoch: 0. Batch 420/4317 - Avg Loss: 1.1273 - Accuracy: 33.85%\n",
      "Epoch: 0. Batch 430/4317 - Avg Loss: 1.1268 - Accuracy: 33.67%\n",
      "Epoch: 0. Batch 440/4317 - Avg Loss: 1.1261 - Accuracy: 33.86%\n",
      "Epoch: 0. Batch 450/4317 - Avg Loss: 1.1257 - Accuracy: 34.10%\n",
      "Epoch: 0. Batch 460/4317 - Avg Loss: 1.1266 - Accuracy: 33.91%\n",
      "Epoch: 0. Batch 470/4317 - Avg Loss: 1.1268 - Accuracy: 33.78%\n",
      "Epoch: 0. Batch 480/4317 - Avg Loss: 1.1265 - Accuracy: 33.84%\n",
      "Epoch: 0. Batch 490/4317 - Avg Loss: 1.1264 - Accuracy: 33.66%\n",
      "Epoch: 0. Batch 500/4317 - Avg Loss: 1.1263 - Accuracy: 33.68%\n",
      "Epoch: 0. Batch 510/4317 - Avg Loss: 1.1256 - Accuracy: 33.75%\n",
      "Epoch: 0. Batch 520/4317 - Avg Loss: 1.1256 - Accuracy: 33.55%\n",
      "Epoch: 0. Batch 530/4317 - Avg Loss: 1.1251 - Accuracy: 33.71%\n",
      "Epoch: 0. Batch 540/4317 - Avg Loss: 1.1250 - Accuracy: 33.75%\n",
      "Epoch: 0. Batch 550/4317 - Avg Loss: 1.1249 - Accuracy: 33.73%\n",
      "Epoch: 0. Batch 560/4317 - Avg Loss: 1.1251 - Accuracy: 33.71%\n",
      "Epoch: 0. Batch 570/4317 - Avg Loss: 1.1250 - Accuracy: 33.86%\n",
      "Epoch: 0. Batch 580/4317 - Avg Loss: 1.1248 - Accuracy: 33.91%\n",
      "Epoch: 0. Batch 590/4317 - Avg Loss: 1.1246 - Accuracy: 33.74%\n",
      "Epoch: 0. Batch 600/4317 - Avg Loss: 1.1248 - Accuracy: 33.84%\n",
      "Epoch: 0. Batch 610/4317 - Avg Loss: 1.1253 - Accuracy: 33.64%\n",
      "Epoch: 0. Batch 620/4317 - Avg Loss: 1.1247 - Accuracy: 33.82%\n",
      "Epoch: 0. Batch 630/4317 - Avg Loss: 1.1245 - Accuracy: 34.03%\n",
      "Epoch: 0. Batch 640/4317 - Avg Loss: 1.1241 - Accuracy: 33.94%\n",
      "Epoch: 0. Batch 650/4317 - Avg Loss: 1.1240 - Accuracy: 33.95%\n",
      "Epoch: 0. Batch 660/4317 - Avg Loss: 1.1242 - Accuracy: 33.97%\n",
      "Epoch: 0. Batch 670/4317 - Avg Loss: 1.1238 - Accuracy: 33.91%\n",
      "Epoch: 0. Batch 680/4317 - Avg Loss: 1.1236 - Accuracy: 33.92%\n",
      "Epoch: 0. Batch 690/4317 - Avg Loss: 1.1235 - Accuracy: 34.08%\n",
      "Epoch: 0. Batch 700/4317 - Avg Loss: 1.1232 - Accuracy: 34.00%\n",
      "Epoch: 0. Batch 710/4317 - Avg Loss: 1.1228 - Accuracy: 34.04%\n",
      "Epoch: 0. Batch 720/4317 - Avg Loss: 1.1229 - Accuracy: 34.01%\n",
      "Epoch: 0. Batch 730/4317 - Avg Loss: 1.1228 - Accuracy: 34.14%\n",
      "Epoch: 0. Batch 740/4317 - Avg Loss: 1.1226 - Accuracy: 34.20%\n",
      "Epoch: 0. Batch 750/4317 - Avg Loss: 1.1224 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 760/4317 - Avg Loss: 1.1224 - Accuracy: 34.40%\n",
      "Epoch: 0. Batch 770/4317 - Avg Loss: 1.1224 - Accuracy: 34.23%\n",
      "Epoch: 0. Batch 780/4317 - Avg Loss: 1.1223 - Accuracy: 34.27%\n",
      "Epoch: 0. Batch 790/4317 - Avg Loss: 1.1223 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 800/4317 - Avg Loss: 1.1221 - Accuracy: 34.39%\n",
      "Epoch: 0. Batch 810/4317 - Avg Loss: 1.1218 - Accuracy: 34.46%\n",
      "Epoch: 0. Batch 820/4317 - Avg Loss: 1.1217 - Accuracy: 34.33%\n",
      "Epoch: 0. Batch 830/4317 - Avg Loss: 1.1213 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 840/4317 - Avg Loss: 1.1213 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 850/4317 - Avg Loss: 1.1215 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 860/4317 - Avg Loss: 1.1215 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 870/4317 - Avg Loss: 1.1214 - Accuracy: 34.45%\n",
      "Epoch: 0. Batch 880/4317 - Avg Loss: 1.1214 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 890/4317 - Avg Loss: 1.1211 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 900/4317 - Avg Loss: 1.1210 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 910/4317 - Avg Loss: 1.1207 - Accuracy: 34.75%\n",
      "Epoch: 0. Batch 920/4317 - Avg Loss: 1.1205 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 930/4317 - Avg Loss: 1.1202 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 940/4317 - Avg Loss: 1.1201 - Accuracy: 34.64%\n",
      "Epoch: 0. Batch 950/4317 - Avg Loss: 1.1199 - Accuracy: 34.57%\n",
      "Epoch: 0. Batch 960/4317 - Avg Loss: 1.1197 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 970/4317 - Avg Loss: 1.1196 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 980/4317 - Avg Loss: 1.1194 - Accuracy: 34.70%\n",
      "Epoch: 0. Batch 990/4317 - Avg Loss: 1.1194 - Accuracy: 34.71%\n",
      "Epoch: 0. Batch 1000/4317 - Avg Loss: 1.1194 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 1010/4317 - Avg Loss: 1.1192 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 1020/4317 - Avg Loss: 1.1190 - Accuracy: 34.70%\n",
      "Epoch: 0. Batch 1030/4317 - Avg Loss: 1.1187 - Accuracy: 34.84%\n",
      "Epoch: 0. Batch 1040/4317 - Avg Loss: 1.1186 - Accuracy: 35.00%\n",
      "Epoch: 0. Batch 1050/4317 - Avg Loss: 1.1184 - Accuracy: 35.10%\n",
      "Epoch: 0. Batch 1060/4317 - Avg Loss: 1.1184 - Accuracy: 35.01%\n",
      "Epoch: 0. Batch 1070/4317 - Avg Loss: 1.1182 - Accuracy: 34.93%\n",
      "Epoch: 0. Batch 1080/4317 - Avg Loss: 1.1181 - Accuracy: 34.93%\n",
      "Epoch: 0. Batch 1090/4317 - Avg Loss: 1.1182 - Accuracy: 34.92%\n",
      "Epoch: 0. Batch 1100/4317 - Avg Loss: 1.1183 - Accuracy: 34.88%\n",
      "Epoch: 0. Batch 1110/4317 - Avg Loss: 1.1182 - Accuracy: 34.81%\n",
      "Epoch: 0. Batch 1120/4317 - Avg Loss: 1.1183 - Accuracy: 34.86%\n",
      "Epoch: 0. Batch 1130/4317 - Avg Loss: 1.1182 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1140/4317 - Avg Loss: 1.1180 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1150/4317 - Avg Loss: 1.1179 - Accuracy: 34.66%\n",
      "Epoch: 0. Batch 1160/4317 - Avg Loss: 1.1180 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1170/4317 - Avg Loss: 1.1177 - Accuracy: 34.80%\n",
      "Epoch: 0. Batch 1180/4317 - Avg Loss: 1.1176 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 1190/4317 - Avg Loss: 1.1175 - Accuracy: 34.81%\n",
      "Epoch: 0. Batch 1200/4317 - Avg Loss: 1.1175 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1210/4317 - Avg Loss: 1.1175 - Accuracy: 34.70%\n",
      "Epoch: 0. Batch 1220/4317 - Avg Loss: 1.1174 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1230/4317 - Avg Loss: 1.1175 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 1240/4317 - Avg Loss: 1.1174 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1250/4317 - Avg Loss: 1.1173 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 1260/4317 - Avg Loss: 1.1171 - Accuracy: 34.80%\n",
      "Epoch: 0. Batch 1270/4317 - Avg Loss: 1.1171 - Accuracy: 34.82%\n",
      "Epoch: 0. Batch 1280/4317 - Avg Loss: 1.1170 - Accuracy: 34.85%\n",
      "Epoch: 0. Batch 1290/4317 - Avg Loss: 1.1169 - Accuracy: 34.80%\n",
      "Epoch: 0. Batch 1300/4317 - Avg Loss: 1.1169 - Accuracy: 34.74%\n",
      "Epoch: 0. Batch 1310/4317 - Avg Loss: 1.1168 - Accuracy: 34.81%\n",
      "Epoch: 0. Batch 1320/4317 - Avg Loss: 1.1168 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1330/4317 - Avg Loss: 1.1166 - Accuracy: 34.84%\n",
      "Epoch: 0. Batch 1340/4317 - Avg Loss: 1.1165 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 1350/4317 - Avg Loss: 1.1164 - Accuracy: 34.84%\n",
      "Epoch: 0. Batch 1360/4317 - Avg Loss: 1.1163 - Accuracy: 34.76%\n",
      "Epoch: 0. Batch 1370/4317 - Avg Loss: 1.1163 - Accuracy: 34.71%\n",
      "Epoch: 0. Batch 1380/4317 - Avg Loss: 1.1161 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1390/4317 - Avg Loss: 1.1161 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 1400/4317 - Avg Loss: 1.1161 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 1410/4317 - Avg Loss: 1.1159 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 1420/4317 - Avg Loss: 1.1159 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 1430/4317 - Avg Loss: 1.1158 - Accuracy: 34.71%\n",
      "Epoch: 0. Batch 1440/4317 - Avg Loss: 1.1157 - Accuracy: 34.71%\n",
      "Epoch: 0. Batch 1450/4317 - Avg Loss: 1.1158 - Accuracy: 34.62%\n",
      "Epoch: 0. Batch 1460/4317 - Avg Loss: 1.1157 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 1470/4317 - Avg Loss: 1.1156 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1480/4317 - Avg Loss: 1.1155 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 1490/4317 - Avg Loss: 1.1155 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1500/4317 - Avg Loss: 1.1154 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1510/4317 - Avg Loss: 1.1154 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1520/4317 - Avg Loss: 1.1153 - Accuracy: 34.75%\n",
      "Epoch: 0. Batch 1530/4317 - Avg Loss: 1.1153 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1540/4317 - Avg Loss: 1.1152 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 1550/4317 - Avg Loss: 1.1152 - Accuracy: 34.71%\n",
      "Epoch: 0. Batch 1560/4317 - Avg Loss: 1.1151 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1570/4317 - Avg Loss: 1.1151 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1580/4317 - Avg Loss: 1.1151 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 1590/4317 - Avg Loss: 1.1152 - Accuracy: 34.62%\n",
      "Epoch: 0. Batch 1600/4317 - Avg Loss: 1.1151 - Accuracy: 34.64%\n",
      "Epoch: 0. Batch 1610/4317 - Avg Loss: 1.1150 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 1620/4317 - Avg Loss: 1.1151 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 1630/4317 - Avg Loss: 1.1149 - Accuracy: 34.76%\n",
      "Epoch: 0. Batch 1640/4317 - Avg Loss: 1.1148 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 1650/4317 - Avg Loss: 1.1147 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 1660/4317 - Avg Loss: 1.1148 - Accuracy: 34.75%\n",
      "Epoch: 0. Batch 1670/4317 - Avg Loss: 1.1147 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1680/4317 - Avg Loss: 1.1146 - Accuracy: 34.86%\n",
      "Epoch: 0. Batch 1690/4317 - Avg Loss: 1.1145 - Accuracy: 34.93%\n",
      "Epoch: 0. Batch 1700/4317 - Avg Loss: 1.1146 - Accuracy: 34.94%\n",
      "Epoch: 0. Batch 1710/4317 - Avg Loss: 1.1147 - Accuracy: 34.86%\n",
      "Epoch: 0. Batch 1720/4317 - Avg Loss: 1.1147 - Accuracy: 34.85%\n",
      "Epoch: 0. Batch 1730/4317 - Avg Loss: 1.1146 - Accuracy: 34.87%\n",
      "Epoch: 0. Batch 1740/4317 - Avg Loss: 1.1145 - Accuracy: 34.81%\n",
      "Epoch: 0. Batch 1750/4317 - Avg Loss: 1.1145 - Accuracy: 34.82%\n",
      "Epoch: 0. Batch 1760/4317 - Avg Loss: 1.1146 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 1770/4317 - Avg Loss: 1.1145 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 1780/4317 - Avg Loss: 1.1144 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1790/4317 - Avg Loss: 1.1145 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 1800/4317 - Avg Loss: 1.1144 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 1810/4317 - Avg Loss: 1.1143 - Accuracy: 34.66%\n",
      "Epoch: 0. Batch 1820/4317 - Avg Loss: 1.1144 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 1830/4317 - Avg Loss: 1.1144 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 1840/4317 - Avg Loss: 1.1143 - Accuracy: 34.59%\n",
      "Epoch: 0. Batch 1850/4317 - Avg Loss: 1.1144 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 1860/4317 - Avg Loss: 1.1143 - Accuracy: 34.64%\n",
      "Epoch: 0. Batch 1870/4317 - Avg Loss: 1.1144 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 1880/4317 - Avg Loss: 1.1147 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 1890/4317 - Avg Loss: 1.1147 - Accuracy: 34.55%\n",
      "Epoch: 0. Batch 1900/4317 - Avg Loss: 1.1147 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 1910/4317 - Avg Loss: 1.1148 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 1920/4317 - Avg Loss: 1.1147 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 1930/4317 - Avg Loss: 1.1151 - Accuracy: 34.51%\n",
      "Epoch: 0. Batch 1940/4317 - Avg Loss: 1.1151 - Accuracy: 34.55%\n",
      "Epoch: 0. Batch 1950/4317 - Avg Loss: 1.1153 - Accuracy: 34.55%\n",
      "Epoch: 0. Batch 1960/4317 - Avg Loss: 1.1155 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 1970/4317 - Avg Loss: 1.1158 - Accuracy: 34.46%\n",
      "Epoch: 0. Batch 1980/4317 - Avg Loss: 1.1159 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 1990/4317 - Avg Loss: 1.1159 - Accuracy: 34.47%\n",
      "Epoch: 0. Batch 2000/4317 - Avg Loss: 1.1158 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 2010/4317 - Avg Loss: 1.1157 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 2020/4317 - Avg Loss: 1.1158 - Accuracy: 34.47%\n",
      "Epoch: 0. Batch 2030/4317 - Avg Loss: 1.1158 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 2040/4317 - Avg Loss: 1.1158 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 2050/4317 - Avg Loss: 1.1158 - Accuracy: 34.57%\n",
      "Epoch: 0. Batch 2060/4317 - Avg Loss: 1.1157 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 2070/4317 - Avg Loss: 1.1158 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 2080/4317 - Avg Loss: 1.1158 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 2090/4317 - Avg Loss: 1.1157 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 2100/4317 - Avg Loss: 1.1158 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 2110/4317 - Avg Loss: 1.1159 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 2120/4317 - Avg Loss: 1.1159 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 2130/4317 - Avg Loss: 1.1158 - Accuracy: 34.63%\n",
      "Epoch: 0. Batch 2140/4317 - Avg Loss: 1.1158 - Accuracy: 34.62%\n",
      "Epoch: 0. Batch 2150/4317 - Avg Loss: 1.1158 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2160/4317 - Avg Loss: 1.1157 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2170/4317 - Avg Loss: 1.1157 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 2180/4317 - Avg Loss: 1.1156 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2190/4317 - Avg Loss: 1.1156 - Accuracy: 34.66%\n",
      "Epoch: 0. Batch 2200/4317 - Avg Loss: 1.1155 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 2210/4317 - Avg Loss: 1.1155 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 2220/4317 - Avg Loss: 1.1155 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 2230/4317 - Avg Loss: 1.1155 - Accuracy: 34.70%\n",
      "Epoch: 0. Batch 2240/4317 - Avg Loss: 1.1154 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 2250/4317 - Avg Loss: 1.1154 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 2260/4317 - Avg Loss: 1.1153 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 2270/4317 - Avg Loss: 1.1154 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 2280/4317 - Avg Loss: 1.1153 - Accuracy: 34.74%\n",
      "Epoch: 0. Batch 2290/4317 - Avg Loss: 1.1153 - Accuracy: 34.76%\n",
      "Epoch: 0. Batch 2300/4317 - Avg Loss: 1.1152 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 2310/4317 - Avg Loss: 1.1151 - Accuracy: 34.79%\n",
      "Epoch: 0. Batch 2320/4317 - Avg Loss: 1.1152 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 2330/4317 - Avg Loss: 1.1153 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 2340/4317 - Avg Loss: 1.1153 - Accuracy: 34.81%\n",
      "Epoch: 0. Batch 2350/4317 - Avg Loss: 1.1153 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 2360/4317 - Avg Loss: 1.1154 - Accuracy: 34.76%\n",
      "Epoch: 0. Batch 2370/4317 - Avg Loss: 1.1154 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 2380/4317 - Avg Loss: 1.1154 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 2390/4317 - Avg Loss: 1.1154 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 2400/4317 - Avg Loss: 1.1154 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 2410/4317 - Avg Loss: 1.1153 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 2420/4317 - Avg Loss: 1.1155 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 2430/4317 - Avg Loss: 1.1155 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2440/4317 - Avg Loss: 1.1155 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2450/4317 - Avg Loss: 1.1154 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 2460/4317 - Avg Loss: 1.1155 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2470/4317 - Avg Loss: 1.1155 - Accuracy: 34.66%\n",
      "Epoch: 0. Batch 2480/4317 - Avg Loss: 1.1154 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 2490/4317 - Avg Loss: 1.1154 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2500/4317 - Avg Loss: 1.1154 - Accuracy: 34.66%\n",
      "Epoch: 0. Batch 2510/4317 - Avg Loss: 1.1154 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 2520/4317 - Avg Loss: 1.1154 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 2530/4317 - Avg Loss: 1.1154 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 2540/4317 - Avg Loss: 1.1154 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 2550/4317 - Avg Loss: 1.1154 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2560/4317 - Avg Loss: 1.1154 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2570/4317 - Avg Loss: 1.1153 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2580/4317 - Avg Loss: 1.1153 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2590/4317 - Avg Loss: 1.1153 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 2600/4317 - Avg Loss: 1.1153 - Accuracy: 34.51%\n",
      "Epoch: 0. Batch 2610/4317 - Avg Loss: 1.1153 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 2620/4317 - Avg Loss: 1.1152 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2630/4317 - Avg Loss: 1.1152 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2640/4317 - Avg Loss: 1.1151 - Accuracy: 34.60%\n",
      "Epoch: 0. Batch 2650/4317 - Avg Loss: 1.1152 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 2660/4317 - Avg Loss: 1.1151 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2670/4317 - Avg Loss: 1.1151 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 2680/4317 - Avg Loss: 1.1151 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2690/4317 - Avg Loss: 1.1151 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 2700/4317 - Avg Loss: 1.1151 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 2710/4317 - Avg Loss: 1.1151 - Accuracy: 34.57%\n",
      "Epoch: 0. Batch 2720/4317 - Avg Loss: 1.1151 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 2730/4317 - Avg Loss: 1.1151 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2740/4317 - Avg Loss: 1.1151 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 2750/4317 - Avg Loss: 1.1150 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2760/4317 - Avg Loss: 1.1150 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2770/4317 - Avg Loss: 1.1150 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2780/4317 - Avg Loss: 1.1149 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2790/4317 - Avg Loss: 1.1149 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 2800/4317 - Avg Loss: 1.1149 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2810/4317 - Avg Loss: 1.1149 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2820/4317 - Avg Loss: 1.1148 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 2830/4317 - Avg Loss: 1.1149 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 2840/4317 - Avg Loss: 1.1148 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 2850/4317 - Avg Loss: 1.1148 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 2860/4317 - Avg Loss: 1.1148 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 2870/4317 - Avg Loss: 1.1147 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 2880/4317 - Avg Loss: 1.1147 - Accuracy: 34.50%\n",
      "Epoch: 0. Batch 2890/4317 - Avg Loss: 1.1146 - Accuracy: 34.52%\n",
      "Epoch: 0. Batch 2900/4317 - Avg Loss: 1.1147 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 2910/4317 - Avg Loss: 1.1147 - Accuracy: 34.56%\n",
      "Epoch: 0. Batch 2920/4317 - Avg Loss: 1.1147 - Accuracy: 34.51%\n",
      "Epoch: 0. Batch 2930/4317 - Avg Loss: 1.1147 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 2940/4317 - Avg Loss: 1.1147 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 2950/4317 - Avg Loss: 1.1148 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 2960/4317 - Avg Loss: 1.1150 - Accuracy: 34.43%\n",
      "Epoch: 0. Batch 2970/4317 - Avg Loss: 1.1149 - Accuracy: 34.41%\n",
      "Epoch: 0. Batch 2980/4317 - Avg Loss: 1.1149 - Accuracy: 34.45%\n",
      "Epoch: 0. Batch 2990/4317 - Avg Loss: 1.1149 - Accuracy: 34.45%\n",
      "Epoch: 0. Batch 3000/4317 - Avg Loss: 1.1149 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 3010/4317 - Avg Loss: 1.1148 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 3020/4317 - Avg Loss: 1.1148 - Accuracy: 34.42%\n",
      "Epoch: 0. Batch 3030/4317 - Avg Loss: 1.1148 - Accuracy: 34.41%\n",
      "Epoch: 0. Batch 3040/4317 - Avg Loss: 1.1148 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 3050/4317 - Avg Loss: 1.1147 - Accuracy: 34.42%\n",
      "Epoch: 0. Batch 3060/4317 - Avg Loss: 1.1147 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 3070/4317 - Avg Loss: 1.1148 - Accuracy: 34.42%\n",
      "Epoch: 0. Batch 3080/4317 - Avg Loss: 1.1147 - Accuracy: 34.40%\n",
      "Epoch: 0. Batch 3090/4317 - Avg Loss: 1.1147 - Accuracy: 34.41%\n",
      "Epoch: 0. Batch 3100/4317 - Avg Loss: 1.1147 - Accuracy: 34.37%\n",
      "Epoch: 0. Batch 3110/4317 - Avg Loss: 1.1147 - Accuracy: 34.34%\n",
      "Epoch: 0. Batch 3120/4317 - Avg Loss: 1.1147 - Accuracy: 34.36%\n",
      "Epoch: 0. Batch 3130/4317 - Avg Loss: 1.1147 - Accuracy: 34.36%\n",
      "Epoch: 0. Batch 3140/4317 - Avg Loss: 1.1146 - Accuracy: 34.39%\n",
      "Epoch: 0. Batch 3150/4317 - Avg Loss: 1.1147 - Accuracy: 34.36%\n",
      "Epoch: 0. Batch 3160/4317 - Avg Loss: 1.1147 - Accuracy: 34.34%\n",
      "Epoch: 0. Batch 3170/4317 - Avg Loss: 1.1147 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3180/4317 - Avg Loss: 1.1146 - Accuracy: 34.32%\n",
      "Epoch: 0. Batch 3190/4317 - Avg Loss: 1.1146 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3200/4317 - Avg Loss: 1.1147 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3210/4317 - Avg Loss: 1.1147 - Accuracy: 34.27%\n",
      "Epoch: 0. Batch 3220/4317 - Avg Loss: 1.1147 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 3230/4317 - Avg Loss: 1.1146 - Accuracy: 34.32%\n",
      "Epoch: 0. Batch 3240/4317 - Avg Loss: 1.1146 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3250/4317 - Avg Loss: 1.1146 - Accuracy: 34.33%\n",
      "Epoch: 0. Batch 3260/4317 - Avg Loss: 1.1146 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3270/4317 - Avg Loss: 1.1146 - Accuracy: 34.32%\n",
      "Epoch: 0. Batch 3280/4317 - Avg Loss: 1.1147 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3290/4317 - Avg Loss: 1.1147 - Accuracy: 34.28%\n",
      "Epoch: 0. Batch 3300/4317 - Avg Loss: 1.1147 - Accuracy: 34.29%\n",
      "Epoch: 0. Batch 3310/4317 - Avg Loss: 1.1147 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3320/4317 - Avg Loss: 1.1146 - Accuracy: 34.25%\n",
      "Epoch: 0. Batch 3330/4317 - Avg Loss: 1.1146 - Accuracy: 34.22%\n",
      "Epoch: 0. Batch 3340/4317 - Avg Loss: 1.1146 - Accuracy: 34.21%\n",
      "Epoch: 0. Batch 3350/4317 - Avg Loss: 1.1145 - Accuracy: 34.21%\n",
      "Epoch: 0. Batch 3360/4317 - Avg Loss: 1.1145 - Accuracy: 34.20%\n",
      "Epoch: 0. Batch 3370/4317 - Avg Loss: 1.1145 - Accuracy: 34.17%\n",
      "Epoch: 0. Batch 3380/4317 - Avg Loss: 1.1145 - Accuracy: 34.18%\n",
      "Epoch: 0. Batch 3390/4317 - Avg Loss: 1.1144 - Accuracy: 34.18%\n",
      "Epoch: 0. Batch 3400/4317 - Avg Loss: 1.1144 - Accuracy: 34.16%\n",
      "Epoch: 0. Batch 3410/4317 - Avg Loss: 1.1144 - Accuracy: 34.19%\n",
      "Epoch: 0. Batch 3420/4317 - Avg Loss: 1.1143 - Accuracy: 34.24%\n",
      "Epoch: 0. Batch 3430/4317 - Avg Loss: 1.1142 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3440/4317 - Avg Loss: 1.1142 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3450/4317 - Avg Loss: 1.1142 - Accuracy: 34.24%\n",
      "Epoch: 0. Batch 3460/4317 - Avg Loss: 1.1142 - Accuracy: 34.25%\n",
      "Epoch: 0. Batch 3470/4317 - Avg Loss: 1.1142 - Accuracy: 34.25%\n",
      "Epoch: 0. Batch 3480/4317 - Avg Loss: 1.1141 - Accuracy: 34.24%\n",
      "Epoch: 0. Batch 3490/4317 - Avg Loss: 1.1141 - Accuracy: 34.24%\n",
      "Epoch: 0. Batch 3500/4317 - Avg Loss: 1.1141 - Accuracy: 34.22%\n",
      "Epoch: 0. Batch 3510/4317 - Avg Loss: 1.1141 - Accuracy: 34.24%\n",
      "Epoch: 0. Batch 3520/4317 - Avg Loss: 1.1141 - Accuracy: 34.23%\n",
      "Epoch: 0. Batch 3530/4317 - Avg Loss: 1.1141 - Accuracy: 34.25%\n",
      "Epoch: 0. Batch 3540/4317 - Avg Loss: 1.1140 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3550/4317 - Avg Loss: 1.1140 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 3560/4317 - Avg Loss: 1.1139 - Accuracy: 34.28%\n",
      "Epoch: 0. Batch 3570/4317 - Avg Loss: 1.1138 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 3580/4317 - Avg Loss: 1.1137 - Accuracy: 34.29%\n",
      "Epoch: 0. Batch 3590/4317 - Avg Loss: 1.1136 - Accuracy: 34.29%\n",
      "Epoch: 0. Batch 3600/4317 - Avg Loss: 1.1136 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 3610/4317 - Avg Loss: 1.1136 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3620/4317 - Avg Loss: 1.1136 - Accuracy: 34.29%\n",
      "Epoch: 0. Batch 3630/4317 - Avg Loss: 1.1136 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3640/4317 - Avg Loss: 1.1136 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3650/4317 - Avg Loss: 1.1136 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3660/4317 - Avg Loss: 1.1135 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 3670/4317 - Avg Loss: 1.1134 - Accuracy: 34.30%\n",
      "Epoch: 0. Batch 3680/4317 - Avg Loss: 1.1134 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3690/4317 - Avg Loss: 1.1136 - Accuracy: 34.31%\n",
      "Epoch: 0. Batch 3700/4317 - Avg Loss: 1.1135 - Accuracy: 34.33%\n",
      "Epoch: 0. Batch 3710/4317 - Avg Loss: 1.1134 - Accuracy: 34.33%\n",
      "Epoch: 0. Batch 3720/4317 - Avg Loss: 1.1133 - Accuracy: 34.34%\n",
      "Epoch: 0. Batch 3730/4317 - Avg Loss: 1.1132 - Accuracy: 34.35%\n",
      "Epoch: 0. Batch 3740/4317 - Avg Loss: 1.1131 - Accuracy: 34.37%\n",
      "Epoch: 0. Batch 3750/4317 - Avg Loss: 1.1130 - Accuracy: 34.39%\n",
      "Epoch: 0. Batch 3760/4317 - Avg Loss: 1.1129 - Accuracy: 34.42%\n",
      "Epoch: 0. Batch 3770/4317 - Avg Loss: 1.1126 - Accuracy: 34.45%\n",
      "Epoch: 0. Batch 3780/4317 - Avg Loss: 1.1123 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 3790/4317 - Avg Loss: 1.1124 - Accuracy: 34.51%\n",
      "Epoch: 0. Batch 3800/4317 - Avg Loss: 1.1121 - Accuracy: 34.53%\n",
      "Epoch: 0. Batch 3810/4317 - Avg Loss: 1.1119 - Accuracy: 34.58%\n",
      "Epoch: 0. Batch 3820/4317 - Avg Loss: 1.1117 - Accuracy: 34.61%\n",
      "Epoch: 0. Batch 3830/4317 - Avg Loss: 1.1115 - Accuracy: 34.63%\n",
      "Epoch: 0. Batch 3840/4317 - Avg Loss: 1.1114 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 3850/4317 - Avg Loss: 1.1111 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 3860/4317 - Avg Loss: 1.1110 - Accuracy: 34.70%\n",
      "Epoch: 0. Batch 3870/4317 - Avg Loss: 1.1106 - Accuracy: 34.74%\n",
      "Epoch: 0. Batch 3880/4317 - Avg Loss: 1.1103 - Accuracy: 34.78%\n",
      "Epoch: 0. Batch 3890/4317 - Avg Loss: 1.1101 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 3900/4317 - Avg Loss: 1.1096 - Accuracy: 34.88%\n",
      "Epoch: 0. Batch 3910/4317 - Avg Loss: 1.1094 - Accuracy: 34.92%\n",
      "Epoch: 0. Batch 3920/4317 - Avg Loss: 1.1090 - Accuracy: 34.96%\n",
      "Epoch: 0. Batch 3930/4317 - Avg Loss: 1.1087 - Accuracy: 34.98%\n",
      "Epoch: 0. Batch 3940/4317 - Avg Loss: 1.1084 - Accuracy: 35.01%\n",
      "Epoch: 0. Batch 3950/4317 - Avg Loss: 1.1082 - Accuracy: 35.04%\n",
      "Epoch: 0. Batch 3960/4317 - Avg Loss: 1.1079 - Accuracy: 35.08%\n",
      "Epoch: 0. Batch 3970/4317 - Avg Loss: 1.1077 - Accuracy: 35.10%\n",
      "Epoch: 0. Batch 3980/4317 - Avg Loss: 1.1073 - Accuracy: 35.15%\n",
      "Epoch: 0. Batch 3990/4317 - Avg Loss: 1.1069 - Accuracy: 35.21%\n",
      "Epoch: 0. Batch 4000/4317 - Avg Loss: 1.1065 - Accuracy: 35.24%\n",
      "Epoch: 0. Batch 4010/4317 - Avg Loss: 1.1061 - Accuracy: 35.30%\n",
      "Epoch: 0. Batch 4020/4317 - Avg Loss: 1.1058 - Accuracy: 35.34%\n",
      "Epoch: 0. Batch 4030/4317 - Avg Loss: 1.1055 - Accuracy: 35.36%\n",
      "Epoch: 0. Batch 4040/4317 - Avg Loss: 1.1050 - Accuracy: 35.40%\n",
      "Epoch: 0. Batch 4050/4317 - Avg Loss: 1.1046 - Accuracy: 35.44%\n",
      "Epoch: 0. Batch 4060/4317 - Avg Loss: 1.1041 - Accuracy: 35.48%\n",
      "Epoch: 0. Batch 4070/4317 - Avg Loss: 1.1038 - Accuracy: 35.51%\n",
      "Epoch: 0. Batch 4080/4317 - Avg Loss: 1.1036 - Accuracy: 35.56%\n",
      "Epoch: 0. Batch 4090/4317 - Avg Loss: 1.1033 - Accuracy: 35.58%\n",
      "Epoch: 0. Batch 4100/4317 - Avg Loss: 1.1029 - Accuracy: 35.62%\n",
      "Epoch: 0. Batch 4110/4317 - Avg Loss: 1.1027 - Accuracy: 35.66%\n",
      "Epoch: 0. Batch 4120/4317 - Avg Loss: 1.1024 - Accuracy: 35.70%\n",
      "Epoch: 0. Batch 4130/4317 - Avg Loss: 1.1020 - Accuracy: 35.75%\n",
      "Epoch: 0. Batch 4140/4317 - Avg Loss: 1.1014 - Accuracy: 35.81%\n",
      "Epoch: 0. Batch 4150/4317 - Avg Loss: 1.1010 - Accuracy: 35.86%\n",
      "Epoch: 0. Batch 4160/4317 - Avg Loss: 1.1005 - Accuracy: 35.89%\n",
      "Epoch: 0. Batch 4170/4317 - Avg Loss: 1.1001 - Accuracy: 35.94%\n",
      "Epoch: 0. Batch 4180/4317 - Avg Loss: 1.0998 - Accuracy: 35.99%\n",
      "Epoch: 0. Batch 4190/4317 - Avg Loss: 1.0993 - Accuracy: 36.03%\n",
      "Epoch: 0. Batch 4200/4317 - Avg Loss: 1.0989 - Accuracy: 36.05%\n",
      "Epoch: 0. Batch 4210/4317 - Avg Loss: 1.0986 - Accuracy: 36.09%\n",
      "Epoch: 0. Batch 4220/4317 - Avg Loss: 1.0984 - Accuracy: 36.10%\n",
      "Epoch: 0. Batch 4230/4317 - Avg Loss: 1.0979 - Accuracy: 36.15%\n",
      "Epoch: 0. Batch 4240/4317 - Avg Loss: 1.0975 - Accuracy: 36.20%\n",
      "Epoch: 0. Batch 4250/4317 - Avg Loss: 1.0970 - Accuracy: 36.26%\n",
      "Epoch: 0. Batch 4260/4317 - Avg Loss: 1.0966 - Accuracy: 36.30%\n",
      "Epoch: 0. Batch 4270/4317 - Avg Loss: 1.0961 - Accuracy: 36.33%\n",
      "Epoch: 0. Batch 4280/4317 - Avg Loss: 1.0959 - Accuracy: 36.36%\n",
      "Epoch: 0. Batch 4290/4317 - Avg Loss: 1.0955 - Accuracy: 36.40%\n",
      "Epoch: 0. Batch 4300/4317 - Avg Loss: 1.0951 - Accuracy: 36.44%\n",
      "Epoch: 0. Batch 4310/4317 - Avg Loss: 1.0947 - Accuracy: 36.48%\n",
      "Train loss: 1.0944 - Train accuracy: 36.50%\n",
      "Validation accuracy: 58.5107\n",
      "Epoch: 1. Batch 0/4317 - Avg Loss: 0.6942 - Accuracy: 81.25%\n",
      "Epoch: 1. Batch 10/4317 - Avg Loss: 0.8301 - Accuracy: 64.77%\n",
      "Epoch: 1. Batch 20/4317 - Avg Loss: 0.8548 - Accuracy: 61.31%\n",
      "Epoch: 1. Batch 30/4317 - Avg Loss: 0.8628 - Accuracy: 61.09%\n",
      "Epoch: 1. Batch 40/4317 - Avg Loss: 0.8724 - Accuracy: 60.98%\n",
      "Epoch: 1. Batch 50/4317 - Avg Loss: 0.8742 - Accuracy: 59.93%\n",
      "Epoch: 1. Batch 60/4317 - Avg Loss: 0.8745 - Accuracy: 59.22%\n",
      "Epoch: 1. Batch 70/4317 - Avg Loss: 0.8824 - Accuracy: 59.51%\n",
      "Epoch: 1. Batch 80/4317 - Avg Loss: 0.8860 - Accuracy: 59.18%\n",
      "Epoch: 1. Batch 90/4317 - Avg Loss: 0.8922 - Accuracy: 58.65%\n",
      "Epoch: 1. Batch 100/4317 - Avg Loss: 0.8820 - Accuracy: 59.28%\n",
      "Epoch: 1. Batch 110/4317 - Avg Loss: 0.8828 - Accuracy: 58.95%\n",
      "Epoch: 1. Batch 120/4317 - Avg Loss: 0.8840 - Accuracy: 58.52%\n",
      "Epoch: 1. Batch 130/4317 - Avg Loss: 0.8834 - Accuracy: 58.59%\n",
      "Epoch: 1. Batch 140/4317 - Avg Loss: 0.8817 - Accuracy: 58.69%\n",
      "Epoch: 1. Batch 150/4317 - Avg Loss: 0.8845 - Accuracy: 58.90%\n",
      "Epoch: 1. Batch 160/4317 - Avg Loss: 0.8822 - Accuracy: 59.01%\n",
      "Epoch: 1. Batch 170/4317 - Avg Loss: 0.8806 - Accuracy: 59.03%\n",
      "Epoch: 1. Batch 180/4317 - Avg Loss: 0.8842 - Accuracy: 58.81%\n",
      "Epoch: 1. Batch 190/4317 - Avg Loss: 0.8840 - Accuracy: 58.67%\n",
      "Epoch: 1. Batch 200/4317 - Avg Loss: 0.8852 - Accuracy: 58.49%\n",
      "Epoch: 1. Batch 210/4317 - Avg Loss: 0.8809 - Accuracy: 58.53%\n",
      "Epoch: 1. Batch 220/4317 - Avg Loss: 0.8794 - Accuracy: 58.40%\n",
      "Epoch: 1. Batch 230/4317 - Avg Loss: 0.8818 - Accuracy: 58.14%\n",
      "Epoch: 1. Batch 240/4317 - Avg Loss: 0.8795 - Accuracy: 58.14%\n",
      "Epoch: 1. Batch 250/4317 - Avg Loss: 0.8778 - Accuracy: 58.64%\n",
      "Epoch: 1. Batch 260/4317 - Avg Loss: 0.8765 - Accuracy: 58.67%\n",
      "Epoch: 1. Batch 270/4317 - Avg Loss: 0.8774 - Accuracy: 58.46%\n",
      "Epoch: 1. Batch 280/4317 - Avg Loss: 0.8771 - Accuracy: 58.45%\n",
      "Epoch: 1. Batch 290/4317 - Avg Loss: 0.8751 - Accuracy: 58.55%\n",
      "Epoch: 1. Batch 300/4317 - Avg Loss: 0.8755 - Accuracy: 58.58%\n",
      "Epoch: 1. Batch 310/4317 - Avg Loss: 0.8765 - Accuracy: 58.32%\n",
      "Epoch: 1. Batch 320/4317 - Avg Loss: 0.8747 - Accuracy: 58.39%\n",
      "Epoch: 1. Batch 330/4317 - Avg Loss: 0.8733 - Accuracy: 58.50%\n",
      "Epoch: 1. Batch 340/4317 - Avg Loss: 0.8698 - Accuracy: 58.63%\n",
      "Epoch: 1. Batch 350/4317 - Avg Loss: 0.8705 - Accuracy: 58.64%\n",
      "Epoch: 1. Batch 360/4317 - Avg Loss: 0.8691 - Accuracy: 58.59%\n",
      "Epoch: 1. Batch 370/4317 - Avg Loss: 0.8698 - Accuracy: 58.57%\n",
      "Epoch: 1. Batch 380/4317 - Avg Loss: 0.8698 - Accuracy: 58.53%\n",
      "Epoch: 1. Batch 390/4317 - Avg Loss: 0.8691 - Accuracy: 58.55%\n",
      "Epoch: 1. Batch 400/4317 - Avg Loss: 0.8703 - Accuracy: 58.49%\n",
      "Epoch: 1. Batch 410/4317 - Avg Loss: 0.8702 - Accuracy: 58.36%\n",
      "Epoch: 1. Batch 420/4317 - Avg Loss: 0.8696 - Accuracy: 58.39%\n",
      "Epoch: 1. Batch 430/4317 - Avg Loss: 0.8686 - Accuracy: 58.54%\n",
      "Epoch: 1. Batch 440/4317 - Avg Loss: 0.8690 - Accuracy: 58.46%\n",
      "Epoch: 1. Batch 450/4317 - Avg Loss: 0.8691 - Accuracy: 58.36%\n",
      "Epoch: 1. Batch 460/4317 - Avg Loss: 0.8680 - Accuracy: 58.46%\n",
      "Epoch: 1. Batch 470/4317 - Avg Loss: 0.8685 - Accuracy: 58.45%\n",
      "Epoch: 1. Batch 480/4317 - Avg Loss: 0.8657 - Accuracy: 58.64%\n",
      "Epoch: 1. Batch 490/4317 - Avg Loss: 0.8652 - Accuracy: 58.77%\n",
      "Epoch: 1. Batch 500/4317 - Avg Loss: 0.8641 - Accuracy: 58.88%\n",
      "Epoch: 1. Batch 510/4317 - Avg Loss: 0.8633 - Accuracy: 58.92%\n",
      "Epoch: 1. Batch 520/4317 - Avg Loss: 0.8634 - Accuracy: 59.03%\n",
      "Epoch: 1. Batch 530/4317 - Avg Loss: 0.8625 - Accuracy: 59.03%\n",
      "Epoch: 1. Batch 540/4317 - Avg Loss: 0.8617 - Accuracy: 59.13%\n",
      "Epoch: 1. Batch 550/4317 - Avg Loss: 0.8611 - Accuracy: 59.17%\n",
      "Epoch: 1. Batch 560/4317 - Avg Loss: 0.8608 - Accuracy: 59.18%\n",
      "Epoch: 1. Batch 570/4317 - Avg Loss: 0.8603 - Accuracy: 59.18%\n",
      "Epoch: 1. Batch 580/4317 - Avg Loss: 0.8595 - Accuracy: 59.28%\n",
      "Epoch: 1. Batch 590/4317 - Avg Loss: 0.8564 - Accuracy: 59.50%\n",
      "Epoch: 1. Batch 600/4317 - Avg Loss: 0.8565 - Accuracy: 59.48%\n",
      "Epoch: 1. Batch 610/4317 - Avg Loss: 0.8546 - Accuracy: 59.61%\n",
      "Epoch: 1. Batch 620/4317 - Avg Loss: 0.8540 - Accuracy: 59.59%\n",
      "Epoch: 1. Batch 630/4317 - Avg Loss: 0.8538 - Accuracy: 59.59%\n",
      "Epoch: 1. Batch 640/4317 - Avg Loss: 0.8525 - Accuracy: 59.63%\n",
      "Epoch: 1. Batch 650/4317 - Avg Loss: 0.8520 - Accuracy: 59.68%\n",
      "Epoch: 1. Batch 660/4317 - Avg Loss: 0.8544 - Accuracy: 59.52%\n",
      "Epoch: 1. Batch 670/4317 - Avg Loss: 0.8541 - Accuracy: 59.56%\n",
      "Epoch: 1. Batch 680/4317 - Avg Loss: 0.8534 - Accuracy: 59.58%\n",
      "Epoch: 1. Batch 690/4317 - Avg Loss: 0.8519 - Accuracy: 59.62%\n",
      "Epoch: 1. Batch 700/4317 - Avg Loss: 0.8511 - Accuracy: 59.65%\n",
      "Epoch: 1. Batch 710/4317 - Avg Loss: 0.8519 - Accuracy: 59.58%\n",
      "Epoch: 1. Batch 720/4317 - Avg Loss: 0.8526 - Accuracy: 59.59%\n",
      "Epoch: 1. Batch 730/4317 - Avg Loss: 0.8519 - Accuracy: 59.61%\n",
      "Epoch: 1. Batch 740/4317 - Avg Loss: 0.8510 - Accuracy: 59.67%\n",
      "Epoch: 1. Batch 750/4317 - Avg Loss: 0.8506 - Accuracy: 59.70%\n",
      "Epoch: 1. Batch 760/4317 - Avg Loss: 0.8505 - Accuracy: 59.70%\n",
      "Epoch: 1. Batch 770/4317 - Avg Loss: 0.8505 - Accuracy: 59.70%\n",
      "Epoch: 1. Batch 780/4317 - Avg Loss: 0.8495 - Accuracy: 59.72%\n",
      "Epoch: 1. Batch 790/4317 - Avg Loss: 0.8487 - Accuracy: 59.79%\n",
      "Epoch: 1. Batch 800/4317 - Avg Loss: 0.8483 - Accuracy: 59.75%\n",
      "Epoch: 1. Batch 810/4317 - Avg Loss: 0.8493 - Accuracy: 59.69%\n",
      "Epoch: 1. Batch 820/4317 - Avg Loss: 0.8493 - Accuracy: 59.68%\n",
      "Epoch: 1. Batch 830/4317 - Avg Loss: 0.8481 - Accuracy: 59.79%\n",
      "Epoch: 1. Batch 840/4317 - Avg Loss: 0.8480 - Accuracy: 59.79%\n",
      "Epoch: 1. Batch 850/4317 - Avg Loss: 0.8466 - Accuracy: 59.83%\n",
      "Epoch: 1. Batch 860/4317 - Avg Loss: 0.8460 - Accuracy: 59.86%\n",
      "Epoch: 1. Batch 870/4317 - Avg Loss: 0.8442 - Accuracy: 59.92%\n",
      "Epoch: 1. Batch 880/4317 - Avg Loss: 0.8434 - Accuracy: 59.97%\n",
      "Epoch: 1. Batch 890/4317 - Avg Loss: 0.8438 - Accuracy: 59.95%\n",
      "Epoch: 1. Batch 900/4317 - Avg Loss: 0.8438 - Accuracy: 59.96%\n",
      "Epoch: 1. Batch 910/4317 - Avg Loss: 0.8430 - Accuracy: 60.02%\n",
      "Epoch: 1. Batch 920/4317 - Avg Loss: 0.8427 - Accuracy: 60.05%\n",
      "Epoch: 1. Batch 930/4317 - Avg Loss: 0.8423 - Accuracy: 60.08%\n",
      "Epoch: 1. Batch 940/4317 - Avg Loss: 0.8424 - Accuracy: 60.04%\n",
      "Epoch: 1. Batch 950/4317 - Avg Loss: 0.8418 - Accuracy: 60.09%\n",
      "Epoch: 1. Batch 960/4317 - Avg Loss: 0.8431 - Accuracy: 60.05%\n",
      "Epoch: 1. Batch 970/4317 - Avg Loss: 0.8430 - Accuracy: 60.03%\n",
      "Epoch: 1. Batch 980/4317 - Avg Loss: 0.8435 - Accuracy: 59.97%\n",
      "Epoch: 1. Batch 990/4317 - Avg Loss: 0.8432 - Accuracy: 59.98%\n",
      "Epoch: 1. Batch 1000/4317 - Avg Loss: 0.8426 - Accuracy: 60.03%\n",
      "Epoch: 1. Batch 1010/4317 - Avg Loss: 0.8420 - Accuracy: 60.11%\n",
      "Epoch: 1. Batch 1020/4317 - Avg Loss: 0.8414 - Accuracy: 60.16%\n",
      "Epoch: 1. Batch 1030/4317 - Avg Loss: 0.8406 - Accuracy: 60.21%\n",
      "Epoch: 1. Batch 1040/4317 - Avg Loss: 0.8396 - Accuracy: 60.26%\n",
      "Epoch: 1. Batch 1050/4317 - Avg Loss: 0.8395 - Accuracy: 60.26%\n",
      "Epoch: 1. Batch 1060/4317 - Avg Loss: 0.8388 - Accuracy: 60.29%\n",
      "Epoch: 1. Batch 1070/4317 - Avg Loss: 0.8389 - Accuracy: 60.29%\n",
      "Epoch: 1. Batch 1080/4317 - Avg Loss: 0.8381 - Accuracy: 60.34%\n",
      "Epoch: 1. Batch 1090/4317 - Avg Loss: 0.8382 - Accuracy: 60.27%\n",
      "Epoch: 1. Batch 1100/4317 - Avg Loss: 0.8377 - Accuracy: 60.32%\n",
      "Epoch: 1. Batch 1110/4317 - Avg Loss: 0.8385 - Accuracy: 60.26%\n",
      "Epoch: 1. Batch 1120/4317 - Avg Loss: 0.8377 - Accuracy: 60.31%\n",
      "Epoch: 1. Batch 1130/4317 - Avg Loss: 0.8370 - Accuracy: 60.39%\n",
      "Epoch: 1. Batch 1140/4317 - Avg Loss: 0.8367 - Accuracy: 60.40%\n",
      "Epoch: 1. Batch 1150/4317 - Avg Loss: 0.8359 - Accuracy: 60.42%\n",
      "Epoch: 1. Batch 1160/4317 - Avg Loss: 0.8361 - Accuracy: 60.37%\n",
      "Epoch: 1. Batch 1170/4317 - Avg Loss: 0.8363 - Accuracy: 60.38%\n",
      "Epoch: 1. Batch 1180/4317 - Avg Loss: 0.8353 - Accuracy: 60.45%\n",
      "Epoch: 1. Batch 1190/4317 - Avg Loss: 0.8352 - Accuracy: 60.40%\n",
      "Epoch: 1. Batch 1200/4317 - Avg Loss: 0.8354 - Accuracy: 60.41%\n",
      "Epoch: 1. Batch 1210/4317 - Avg Loss: 0.8353 - Accuracy: 60.39%\n",
      "Epoch: 1. Batch 1220/4317 - Avg Loss: 0.8353 - Accuracy: 60.36%\n",
      "Epoch: 1. Batch 1230/4317 - Avg Loss: 0.8346 - Accuracy: 60.42%\n",
      "Epoch: 1. Batch 1240/4317 - Avg Loss: 0.8345 - Accuracy: 60.43%\n",
      "Epoch: 1. Batch 1250/4317 - Avg Loss: 0.8341 - Accuracy: 60.42%\n",
      "Epoch: 1. Batch 1260/4317 - Avg Loss: 0.8346 - Accuracy: 60.39%\n",
      "Epoch: 1. Batch 1270/4317 - Avg Loss: 0.8351 - Accuracy: 60.39%\n",
      "Epoch: 1. Batch 1280/4317 - Avg Loss: 0.8353 - Accuracy: 60.38%\n",
      "Epoch: 1. Batch 1290/4317 - Avg Loss: 0.8346 - Accuracy: 60.41%\n",
      "Epoch: 1. Batch 1300/4317 - Avg Loss: 0.8342 - Accuracy: 60.44%\n",
      "Epoch: 1. Batch 1310/4317 - Avg Loss: 0.8344 - Accuracy: 60.41%\n",
      "Epoch: 1. Batch 1320/4317 - Avg Loss: 0.8348 - Accuracy: 60.38%\n",
      "Epoch: 1. Batch 1330/4317 - Avg Loss: 0.8344 - Accuracy: 60.39%\n",
      "Epoch: 1. Batch 1340/4317 - Avg Loss: 0.8339 - Accuracy: 60.44%\n",
      "Epoch: 1. Batch 1350/4317 - Avg Loss: 0.8336 - Accuracy: 60.47%\n",
      "Epoch: 1. Batch 1360/4317 - Avg Loss: 0.8338 - Accuracy: 60.47%\n",
      "Epoch: 1. Batch 1370/4317 - Avg Loss: 0.8332 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1380/4317 - Avg Loss: 0.8323 - Accuracy: 60.53%\n",
      "Epoch: 1. Batch 1390/4317 - Avg Loss: 0.8325 - Accuracy: 60.51%\n",
      "Epoch: 1. Batch 1400/4317 - Avg Loss: 0.8326 - Accuracy: 60.51%\n",
      "Epoch: 1. Batch 1410/4317 - Avg Loss: 0.8322 - Accuracy: 60.52%\n",
      "Epoch: 1. Batch 1420/4317 - Avg Loss: 0.8324 - Accuracy: 60.50%\n",
      "Epoch: 1. Batch 1430/4317 - Avg Loss: 0.8325 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1440/4317 - Avg Loss: 0.8322 - Accuracy: 60.47%\n",
      "Epoch: 1. Batch 1450/4317 - Avg Loss: 0.8316 - Accuracy: 60.48%\n",
      "Epoch: 1. Batch 1460/4317 - Avg Loss: 0.8312 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1470/4317 - Avg Loss: 0.8320 - Accuracy: 60.46%\n",
      "Epoch: 1. Batch 1480/4317 - Avg Loss: 0.8316 - Accuracy: 60.48%\n",
      "Epoch: 1. Batch 1490/4317 - Avg Loss: 0.8317 - Accuracy: 60.53%\n",
      "Epoch: 1. Batch 1500/4317 - Avg Loss: 0.8311 - Accuracy: 60.56%\n",
      "Epoch: 1. Batch 1510/4317 - Avg Loss: 0.8315 - Accuracy: 60.52%\n",
      "Epoch: 1. Batch 1520/4317 - Avg Loss: 0.8317 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1530/4317 - Avg Loss: 0.8315 - Accuracy: 60.45%\n",
      "Epoch: 1. Batch 1540/4317 - Avg Loss: 0.8312 - Accuracy: 60.48%\n",
      "Epoch: 1. Batch 1550/4317 - Avg Loss: 0.8312 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1560/4317 - Avg Loss: 0.8312 - Accuracy: 60.47%\n",
      "Epoch: 1. Batch 1570/4317 - Avg Loss: 0.8311 - Accuracy: 60.48%\n",
      "Epoch: 1. Batch 1580/4317 - Avg Loss: 0.8309 - Accuracy: 60.49%\n",
      "Epoch: 1. Batch 1590/4317 - Avg Loss: 0.8306 - Accuracy: 60.51%\n",
      "Epoch: 1. Batch 1600/4317 - Avg Loss: 0.8306 - Accuracy: 60.51%\n",
      "Epoch: 1. Batch 1610/4317 - Avg Loss: 0.8304 - Accuracy: 60.55%\n",
      "Epoch: 1. Batch 1620/4317 - Avg Loss: 0.8299 - Accuracy: 60.58%\n",
      "Epoch: 1. Batch 1630/4317 - Avg Loss: 0.8299 - Accuracy: 60.55%\n",
      "Epoch: 1. Batch 1640/4317 - Avg Loss: 0.8301 - Accuracy: 60.54%\n",
      "Epoch: 1. Batch 1650/4317 - Avg Loss: 0.8298 - Accuracy: 60.57%\n",
      "Epoch: 1. Batch 1660/4317 - Avg Loss: 0.8304 - Accuracy: 60.52%\n",
      "Epoch: 1. Batch 1670/4317 - Avg Loss: 0.8304 - Accuracy: 60.53%\n",
      "Epoch: 1. Batch 1680/4317 - Avg Loss: 0.8304 - Accuracy: 60.54%\n",
      "Epoch: 1. Batch 1690/4317 - Avg Loss: 0.8303 - Accuracy: 60.57%\n",
      "Epoch: 1. Batch 1700/4317 - Avg Loss: 0.8296 - Accuracy: 60.62%\n",
      "Epoch: 1. Batch 1710/4317 - Avg Loss: 0.8289 - Accuracy: 60.66%\n",
      "Epoch: 1. Batch 1720/4317 - Avg Loss: 0.8286 - Accuracy: 60.66%\n",
      "Epoch: 1. Batch 1730/4317 - Avg Loss: 0.8277 - Accuracy: 60.70%\n",
      "Epoch: 1. Batch 1740/4317 - Avg Loss: 0.8277 - Accuracy: 60.70%\n",
      "Epoch: 1. Batch 1750/4317 - Avg Loss: 0.8276 - Accuracy: 60.71%\n",
      "Epoch: 1. Batch 1760/4317 - Avg Loss: 0.8273 - Accuracy: 60.71%\n",
      "Epoch: 1. Batch 1770/4317 - Avg Loss: 0.8270 - Accuracy: 60.72%\n",
      "Epoch: 1. Batch 1780/4317 - Avg Loss: 0.8270 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1790/4317 - Avg Loss: 0.8269 - Accuracy: 60.76%\n",
      "Epoch: 1. Batch 1800/4317 - Avg Loss: 0.8265 - Accuracy: 60.77%\n",
      "Epoch: 1. Batch 1810/4317 - Avg Loss: 0.8270 - Accuracy: 60.75%\n",
      "Epoch: 1. Batch 1820/4317 - Avg Loss: 0.8270 - Accuracy: 60.74%\n",
      "Epoch: 1. Batch 1830/4317 - Avg Loss: 0.8272 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1840/4317 - Avg Loss: 0.8268 - Accuracy: 60.74%\n",
      "Epoch: 1. Batch 1850/4317 - Avg Loss: 0.8268 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1860/4317 - Avg Loss: 0.8273 - Accuracy: 60.69%\n",
      "Epoch: 1. Batch 1870/4317 - Avg Loss: 0.8269 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1880/4317 - Avg Loss: 0.8268 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1890/4317 - Avg Loss: 0.8267 - Accuracy: 60.70%\n",
      "Epoch: 1. Batch 1900/4317 - Avg Loss: 0.8266 - Accuracy: 60.71%\n",
      "Epoch: 1. Batch 1910/4317 - Avg Loss: 0.8268 - Accuracy: 60.70%\n",
      "Epoch: 1. Batch 1920/4317 - Avg Loss: 0.8267 - Accuracy: 60.71%\n",
      "Epoch: 1. Batch 1930/4317 - Avg Loss: 0.8265 - Accuracy: 60.69%\n",
      "Epoch: 1. Batch 1940/4317 - Avg Loss: 0.8260 - Accuracy: 60.70%\n",
      "Epoch: 1. Batch 1950/4317 - Avg Loss: 0.8257 - Accuracy: 60.72%\n",
      "Epoch: 1. Batch 1960/4317 - Avg Loss: 0.8252 - Accuracy: 60.73%\n",
      "Epoch: 1. Batch 1970/4317 - Avg Loss: 0.8251 - Accuracy: 60.75%\n",
      "Epoch: 1. Batch 1980/4317 - Avg Loss: 0.8249 - Accuracy: 60.77%\n",
      "Epoch: 1. Batch 1990/4317 - Avg Loss: 0.8250 - Accuracy: 60.78%\n",
      "Epoch: 1. Batch 2000/4317 - Avg Loss: 0.8250 - Accuracy: 60.77%\n",
      "Epoch: 1. Batch 2010/4317 - Avg Loss: 0.8251 - Accuracy: 60.77%\n",
      "Epoch: 1. Batch 2020/4317 - Avg Loss: 0.8248 - Accuracy: 60.81%\n",
      "Epoch: 1. Batch 2030/4317 - Avg Loss: 0.8250 - Accuracy: 60.81%\n",
      "Epoch: 1. Batch 2040/4317 - Avg Loss: 0.8248 - Accuracy: 60.80%\n",
      "Epoch: 1. Batch 2050/4317 - Avg Loss: 0.8242 - Accuracy: 60.82%\n",
      "Epoch: 1. Batch 2060/4317 - Avg Loss: 0.8240 - Accuracy: 60.83%\n",
      "Epoch: 1. Batch 2070/4317 - Avg Loss: 0.8236 - Accuracy: 60.84%\n",
      "Epoch: 1. Batch 2080/4317 - Avg Loss: 0.8236 - Accuracy: 60.83%\n",
      "Epoch: 1. Batch 2090/4317 - Avg Loss: 0.8232 - Accuracy: 60.85%\n",
      "Epoch: 1. Batch 2100/4317 - Avg Loss: 0.8230 - Accuracy: 60.88%\n",
      "Epoch: 1. Batch 2110/4317 - Avg Loss: 0.8227 - Accuracy: 60.89%\n",
      "Epoch: 1. Batch 2120/4317 - Avg Loss: 0.8227 - Accuracy: 60.88%\n",
      "Epoch: 1. Batch 2130/4317 - Avg Loss: 0.8224 - Accuracy: 60.90%\n",
      "Epoch: 1. Batch 2140/4317 - Avg Loss: 0.8219 - Accuracy: 60.92%\n",
      "Epoch: 1. Batch 2150/4317 - Avg Loss: 0.8216 - Accuracy: 60.94%\n",
      "Epoch: 1. Batch 2160/4317 - Avg Loss: 0.8208 - Accuracy: 60.98%\n",
      "Epoch: 1. Batch 2170/4317 - Avg Loss: 0.8205 - Accuracy: 61.01%\n",
      "Epoch: 1. Batch 2180/4317 - Avg Loss: 0.8200 - Accuracy: 61.03%\n",
      "Epoch: 1. Batch 2190/4317 - Avg Loss: 0.8199 - Accuracy: 61.05%\n",
      "Epoch: 1. Batch 2200/4317 - Avg Loss: 0.8195 - Accuracy: 61.08%\n",
      "Epoch: 1. Batch 2210/4317 - Avg Loss: 0.8199 - Accuracy: 61.07%\n",
      "Epoch: 1. Batch 2220/4317 - Avg Loss: 0.8201 - Accuracy: 61.08%\n",
      "Epoch: 1. Batch 2230/4317 - Avg Loss: 0.8196 - Accuracy: 61.11%\n",
      "Epoch: 1. Batch 2240/4317 - Avg Loss: 0.8198 - Accuracy: 61.09%\n",
      "Epoch: 1. Batch 2250/4317 - Avg Loss: 0.8194 - Accuracy: 61.11%\n",
      "Epoch: 1. Batch 2260/4317 - Avg Loss: 0.8190 - Accuracy: 61.13%\n",
      "Epoch: 1. Batch 2270/4317 - Avg Loss: 0.8189 - Accuracy: 61.12%\n",
      "Epoch: 1. Batch 2280/4317 - Avg Loss: 0.8185 - Accuracy: 61.14%\n",
      "Epoch: 1. Batch 2290/4317 - Avg Loss: 0.8180 - Accuracy: 61.17%\n",
      "Epoch: 1. Batch 2300/4317 - Avg Loss: 0.8180 - Accuracy: 61.18%\n",
      "Epoch: 1. Batch 2310/4317 - Avg Loss: 0.8179 - Accuracy: 61.20%\n",
      "Epoch: 1. Batch 2320/4317 - Avg Loss: 0.8173 - Accuracy: 61.25%\n",
      "Epoch: 1. Batch 2330/4317 - Avg Loss: 0.8172 - Accuracy: 61.26%\n",
      "Epoch: 1. Batch 2340/4317 - Avg Loss: 0.8166 - Accuracy: 61.29%\n",
      "Epoch: 1. Batch 2350/4317 - Avg Loss: 0.8165 - Accuracy: 61.31%\n",
      "Epoch: 1. Batch 2360/4317 - Avg Loss: 0.8166 - Accuracy: 61.31%\n",
      "Epoch: 1. Batch 2370/4317 - Avg Loss: 0.8167 - Accuracy: 61.30%\n",
      "Epoch: 1. Batch 2380/4317 - Avg Loss: 0.8165 - Accuracy: 61.32%\n",
      "Epoch: 1. Batch 2390/4317 - Avg Loss: 0.8166 - Accuracy: 61.32%\n",
      "Epoch: 1. Batch 2400/4317 - Avg Loss: 0.8164 - Accuracy: 61.35%\n",
      "Epoch: 1. Batch 2410/4317 - Avg Loss: 0.8164 - Accuracy: 61.36%\n",
      "Epoch: 1. Batch 2420/4317 - Avg Loss: 0.8161 - Accuracy: 61.35%\n",
      "Epoch: 1. Batch 2430/4317 - Avg Loss: 0.8158 - Accuracy: 61.37%\n",
      "Epoch: 1. Batch 2440/4317 - Avg Loss: 0.8154 - Accuracy: 61.38%\n",
      "Epoch: 1. Batch 2450/4317 - Avg Loss: 0.8155 - Accuracy: 61.39%\n",
      "Epoch: 1. Batch 2460/4317 - Avg Loss: 0.8151 - Accuracy: 61.42%\n",
      "Epoch: 1. Batch 2470/4317 - Avg Loss: 0.8151 - Accuracy: 61.42%\n",
      "Epoch: 1. Batch 2480/4317 - Avg Loss: 0.8148 - Accuracy: 61.43%\n",
      "Epoch: 1. Batch 2490/4317 - Avg Loss: 0.8144 - Accuracy: 61.45%\n",
      "Epoch: 1. Batch 2500/4317 - Avg Loss: 0.8141 - Accuracy: 61.47%\n",
      "Epoch: 1. Batch 2510/4317 - Avg Loss: 0.8141 - Accuracy: 61.47%\n",
      "Epoch: 1. Batch 2520/4317 - Avg Loss: 0.8136 - Accuracy: 61.50%\n",
      "Epoch: 1. Batch 2530/4317 - Avg Loss: 0.8135 - Accuracy: 61.50%\n",
      "Epoch: 1. Batch 2540/4317 - Avg Loss: 0.8135 - Accuracy: 61.53%\n",
      "Epoch: 1. Batch 2550/4317 - Avg Loss: 0.8134 - Accuracy: 61.53%\n",
      "Epoch: 1. Batch 2560/4317 - Avg Loss: 0.8131 - Accuracy: 61.54%\n",
      "Epoch: 1. Batch 2570/4317 - Avg Loss: 0.8133 - Accuracy: 61.52%\n",
      "Epoch: 1. Batch 2580/4317 - Avg Loss: 0.8136 - Accuracy: 61.49%\n",
      "Epoch: 1. Batch 2590/4317 - Avg Loss: 0.8134 - Accuracy: 61.50%\n",
      "Epoch: 1. Batch 2600/4317 - Avg Loss: 0.8133 - Accuracy: 61.50%\n",
      "Epoch: 1. Batch 2610/4317 - Avg Loss: 0.8136 - Accuracy: 61.50%\n",
      "Epoch: 1. Batch 2620/4317 - Avg Loss: 0.8133 - Accuracy: 61.52%\n",
      "Epoch: 1. Batch 2630/4317 - Avg Loss: 0.8131 - Accuracy: 61.55%\n",
      "Epoch: 1. Batch 2640/4317 - Avg Loss: 0.8128 - Accuracy: 61.57%\n",
      "Epoch: 1. Batch 2650/4317 - Avg Loss: 0.8128 - Accuracy: 61.57%\n",
      "Epoch: 1. Batch 2660/4317 - Avg Loss: 0.8126 - Accuracy: 61.58%\n",
      "Epoch: 1. Batch 2670/4317 - Avg Loss: 0.8125 - Accuracy: 61.59%\n",
      "Epoch: 1. Batch 2680/4317 - Avg Loss: 0.8124 - Accuracy: 61.60%\n",
      "Epoch: 1. Batch 2690/4317 - Avg Loss: 0.8119 - Accuracy: 61.63%\n",
      "Epoch: 1. Batch 2700/4317 - Avg Loss: 0.8118 - Accuracy: 61.64%\n",
      "Epoch: 1. Batch 2710/4317 - Avg Loss: 0.8120 - Accuracy: 61.62%\n",
      "Epoch: 1. Batch 2720/4317 - Avg Loss: 0.8120 - Accuracy: 61.61%\n",
      "Epoch: 1. Batch 2730/4317 - Avg Loss: 0.8116 - Accuracy: 61.63%\n",
      "Epoch: 1. Batch 2740/4317 - Avg Loss: 0.8114 - Accuracy: 61.66%\n",
      "Epoch: 1. Batch 2750/4317 - Avg Loss: 0.8108 - Accuracy: 61.69%\n",
      "Epoch: 1. Batch 2760/4317 - Avg Loss: 0.8109 - Accuracy: 61.67%\n",
      "Epoch: 1. Batch 2770/4317 - Avg Loss: 0.8108 - Accuracy: 61.70%\n",
      "Epoch: 1. Batch 2780/4317 - Avg Loss: 0.8109 - Accuracy: 61.70%\n",
      "Epoch: 1. Batch 2790/4317 - Avg Loss: 0.8107 - Accuracy: 61.70%\n",
      "Epoch: 1. Batch 2800/4317 - Avg Loss: 0.8105 - Accuracy: 61.71%\n",
      "Epoch: 1. Batch 2810/4317 - Avg Loss: 0.8104 - Accuracy: 61.72%\n",
      "Epoch: 1. Batch 2820/4317 - Avg Loss: 0.8104 - Accuracy: 61.70%\n",
      "Epoch: 1. Batch 2830/4317 - Avg Loss: 0.8100 - Accuracy: 61.74%\n",
      "Epoch: 1. Batch 2840/4317 - Avg Loss: 0.8097 - Accuracy: 61.76%\n",
      "Epoch: 1. Batch 2850/4317 - Avg Loss: 0.8098 - Accuracy: 61.76%\n",
      "Epoch: 1. Batch 2860/4317 - Avg Loss: 0.8102 - Accuracy: 61.74%\n",
      "Epoch: 1. Batch 2870/4317 - Avg Loss: 0.8100 - Accuracy: 61.76%\n",
      "Epoch: 1. Batch 2880/4317 - Avg Loss: 0.8097 - Accuracy: 61.77%\n",
      "Epoch: 1. Batch 2890/4317 - Avg Loss: 0.8096 - Accuracy: 61.78%\n",
      "Epoch: 1. Batch 2900/4317 - Avg Loss: 0.8092 - Accuracy: 61.79%\n",
      "Epoch: 1. Batch 2910/4317 - Avg Loss: 0.8092 - Accuracy: 61.79%\n",
      "Epoch: 1. Batch 2920/4317 - Avg Loss: 0.8092 - Accuracy: 61.80%\n",
      "Epoch: 1. Batch 2930/4317 - Avg Loss: 0.8088 - Accuracy: 61.82%\n",
      "Epoch: 1. Batch 2940/4317 - Avg Loss: 0.8088 - Accuracy: 61.83%\n",
      "Epoch: 1. Batch 2950/4317 - Avg Loss: 0.8091 - Accuracy: 61.83%\n",
      "Epoch: 1. Batch 2960/4317 - Avg Loss: 0.8089 - Accuracy: 61.85%\n",
      "Epoch: 1. Batch 2970/4317 - Avg Loss: 0.8090 - Accuracy: 61.84%\n",
      "Epoch: 1. Batch 2980/4317 - Avg Loss: 0.8088 - Accuracy: 61.86%\n",
      "Epoch: 1. Batch 2990/4317 - Avg Loss: 0.8085 - Accuracy: 61.89%\n",
      "Epoch: 1. Batch 3000/4317 - Avg Loss: 0.8083 - Accuracy: 61.91%\n",
      "Epoch: 1. Batch 3010/4317 - Avg Loss: 0.8080 - Accuracy: 61.91%\n",
      "Epoch: 1. Batch 3020/4317 - Avg Loss: 0.8081 - Accuracy: 61.89%\n",
      "Epoch: 1. Batch 3030/4317 - Avg Loss: 0.8079 - Accuracy: 61.91%\n",
      "Epoch: 1. Batch 3040/4317 - Avg Loss: 0.8079 - Accuracy: 61.92%\n",
      "Epoch: 1. Batch 3050/4317 - Avg Loss: 0.8077 - Accuracy: 61.93%\n",
      "Epoch: 1. Batch 3060/4317 - Avg Loss: 0.8076 - Accuracy: 61.94%\n",
      "Epoch: 1. Batch 3070/4317 - Avg Loss: 0.8076 - Accuracy: 61.94%\n",
      "Epoch: 1. Batch 3080/4317 - Avg Loss: 0.8076 - Accuracy: 61.93%\n",
      "Epoch: 1. Batch 3090/4317 - Avg Loss: 0.8076 - Accuracy: 61.93%\n",
      "Epoch: 1. Batch 3100/4317 - Avg Loss: 0.8074 - Accuracy: 61.94%\n",
      "Epoch: 1. Batch 3110/4317 - Avg Loss: 0.8071 - Accuracy: 61.95%\n",
      "Epoch: 1. Batch 3120/4317 - Avg Loss: 0.8066 - Accuracy: 61.98%\n",
      "Epoch: 1. Batch 3130/4317 - Avg Loss: 0.8062 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3140/4317 - Avg Loss: 0.8062 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3150/4317 - Avg Loss: 0.8062 - Accuracy: 62.01%\n",
      "Epoch: 1. Batch 3160/4317 - Avg Loss: 0.8061 - Accuracy: 62.01%\n",
      "Epoch: 1. Batch 3170/4317 - Avg Loss: 0.8062 - Accuracy: 62.01%\n",
      "Epoch: 1. Batch 3180/4317 - Avg Loss: 0.8063 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3190/4317 - Avg Loss: 0.8064 - Accuracy: 61.99%\n",
      "Epoch: 1. Batch 3200/4317 - Avg Loss: 0.8064 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3210/4317 - Avg Loss: 0.8062 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3220/4317 - Avg Loss: 0.8060 - Accuracy: 62.00%\n",
      "Epoch: 1. Batch 3230/4317 - Avg Loss: 0.8053 - Accuracy: 62.03%\n",
      "Epoch: 1. Batch 3240/4317 - Avg Loss: 0.8053 - Accuracy: 62.05%\n",
      "Epoch: 1. Batch 3250/4317 - Avg Loss: 0.8051 - Accuracy: 62.06%\n",
      "Epoch: 1. Batch 3260/4317 - Avg Loss: 0.8053 - Accuracy: 62.05%\n",
      "Epoch: 1. Batch 3270/4317 - Avg Loss: 0.8051 - Accuracy: 62.06%\n",
      "Epoch: 1. Batch 3280/4317 - Avg Loss: 0.8052 - Accuracy: 62.06%\n",
      "Epoch: 1. Batch 3290/4317 - Avg Loss: 0.8053 - Accuracy: 62.05%\n",
      "Epoch: 1. Batch 3300/4317 - Avg Loss: 0.8052 - Accuracy: 62.06%\n",
      "Epoch: 1. Batch 3310/4317 - Avg Loss: 0.8050 - Accuracy: 62.08%\n",
      "Epoch: 1. Batch 3320/4317 - Avg Loss: 0.8048 - Accuracy: 62.09%\n",
      "Epoch: 1. Batch 3330/4317 - Avg Loss: 0.8047 - Accuracy: 62.10%\n",
      "Epoch: 1. Batch 3340/4317 - Avg Loss: 0.8047 - Accuracy: 62.10%\n",
      "Epoch: 1. Batch 3350/4317 - Avg Loss: 0.8045 - Accuracy: 62.12%\n",
      "Epoch: 1. Batch 3360/4317 - Avg Loss: 0.8043 - Accuracy: 62.13%\n",
      "Epoch: 1. Batch 3370/4317 - Avg Loss: 0.8044 - Accuracy: 62.14%\n",
      "Epoch: 1. Batch 3380/4317 - Avg Loss: 0.8042 - Accuracy: 62.14%\n",
      "Epoch: 1. Batch 3390/4317 - Avg Loss: 0.8041 - Accuracy: 62.16%\n",
      "Epoch: 1. Batch 3400/4317 - Avg Loss: 0.8037 - Accuracy: 62.18%\n",
      "Epoch: 1. Batch 3410/4317 - Avg Loss: 0.8034 - Accuracy: 62.19%\n",
      "Epoch: 1. Batch 3420/4317 - Avg Loss: 0.8033 - Accuracy: 62.18%\n",
      "Epoch: 1. Batch 3430/4317 - Avg Loss: 0.8036 - Accuracy: 62.17%\n",
      "Epoch: 1. Batch 3440/4317 - Avg Loss: 0.8035 - Accuracy: 62.17%\n",
      "Epoch: 1. Batch 3450/4317 - Avg Loss: 0.8031 - Accuracy: 62.20%\n",
      "Epoch: 1. Batch 3460/4317 - Avg Loss: 0.8026 - Accuracy: 62.22%\n",
      "Epoch: 1. Batch 3470/4317 - Avg Loss: 0.8023 - Accuracy: 62.23%\n",
      "Epoch: 1. Batch 3480/4317 - Avg Loss: 0.8020 - Accuracy: 62.25%\n",
      "Epoch: 1. Batch 3490/4317 - Avg Loss: 0.8022 - Accuracy: 62.22%\n",
      "Epoch: 1. Batch 3500/4317 - Avg Loss: 0.8020 - Accuracy: 62.23%\n",
      "Epoch: 1. Batch 3510/4317 - Avg Loss: 0.8020 - Accuracy: 62.24%\n",
      "Epoch: 1. Batch 3520/4317 - Avg Loss: 0.8018 - Accuracy: 62.24%\n",
      "Epoch: 1. Batch 3530/4317 - Avg Loss: 0.8019 - Accuracy: 62.24%\n",
      "Epoch: 1. Batch 3540/4317 - Avg Loss: 0.8017 - Accuracy: 62.26%\n",
      "Epoch: 1. Batch 3550/4317 - Avg Loss: 0.8015 - Accuracy: 62.28%\n",
      "Epoch: 1. Batch 3560/4317 - Avg Loss: 0.8014 - Accuracy: 62.29%\n",
      "Epoch: 1. Batch 3570/4317 - Avg Loss: 0.8015 - Accuracy: 62.28%\n",
      "Epoch: 1. Batch 3580/4317 - Avg Loss: 0.8015 - Accuracy: 62.28%\n",
      "Epoch: 1. Batch 3590/4317 - Avg Loss: 0.8013 - Accuracy: 62.29%\n",
      "Epoch: 1. Batch 3600/4317 - Avg Loss: 0.8012 - Accuracy: 62.30%\n",
      "Epoch: 1. Batch 3610/4317 - Avg Loss: 0.8010 - Accuracy: 62.30%\n",
      "Epoch: 1. Batch 3620/4317 - Avg Loss: 0.8006 - Accuracy: 62.32%\n",
      "Epoch: 1. Batch 3630/4317 - Avg Loss: 0.8003 - Accuracy: 62.33%\n",
      "Epoch: 1. Batch 3640/4317 - Avg Loss: 0.8000 - Accuracy: 62.35%\n",
      "Epoch: 1. Batch 3650/4317 - Avg Loss: 0.7997 - Accuracy: 62.36%\n",
      "Epoch: 1. Batch 3660/4317 - Avg Loss: 0.7993 - Accuracy: 62.39%\n",
      "Epoch: 1. Batch 3670/4317 - Avg Loss: 0.7993 - Accuracy: 62.39%\n",
      "Epoch: 1. Batch 3680/4317 - Avg Loss: 0.7992 - Accuracy: 62.40%\n",
      "Epoch: 1. Batch 3690/4317 - Avg Loss: 0.7993 - Accuracy: 62.39%\n",
      "Epoch: 1. Batch 3700/4317 - Avg Loss: 0.7992 - Accuracy: 62.39%\n",
      "Epoch: 1. Batch 3710/4317 - Avg Loss: 0.7990 - Accuracy: 62.39%\n",
      "Epoch: 1. Batch 3720/4317 - Avg Loss: 0.7988 - Accuracy: 62.41%\n",
      "Epoch: 1. Batch 3730/4317 - Avg Loss: 0.7985 - Accuracy: 62.44%\n",
      "Epoch: 1. Batch 3740/4317 - Avg Loss: 0.7985 - Accuracy: 62.43%\n",
      "Epoch: 1. Batch 3750/4317 - Avg Loss: 0.7983 - Accuracy: 62.44%\n",
      "Epoch: 1. Batch 3760/4317 - Avg Loss: 0.7981 - Accuracy: 62.45%\n",
      "Epoch: 1. Batch 3770/4317 - Avg Loss: 0.7979 - Accuracy: 62.45%\n",
      "Epoch: 1. Batch 3780/4317 - Avg Loss: 0.7980 - Accuracy: 62.46%\n",
      "Epoch: 1. Batch 3790/4317 - Avg Loss: 0.7978 - Accuracy: 62.47%\n",
      "Epoch: 1. Batch 3800/4317 - Avg Loss: 0.7976 - Accuracy: 62.48%\n",
      "Epoch: 1. Batch 3810/4317 - Avg Loss: 0.7973 - Accuracy: 62.50%\n",
      "Epoch: 1. Batch 3820/4317 - Avg Loss: 0.7972 - Accuracy: 62.50%\n",
      "Epoch: 1. Batch 3830/4317 - Avg Loss: 0.7972 - Accuracy: 62.49%\n",
      "Epoch: 1. Batch 3840/4317 - Avg Loss: 0.7972 - Accuracy: 62.48%\n",
      "Epoch: 1. Batch 3850/4317 - Avg Loss: 0.7971 - Accuracy: 62.49%\n",
      "Epoch: 1. Batch 3860/4317 - Avg Loss: 0.7972 - Accuracy: 62.48%\n",
      "Epoch: 1. Batch 3870/4317 - Avg Loss: 0.7972 - Accuracy: 62.47%\n",
      "Epoch: 1. Batch 3880/4317 - Avg Loss: 0.7969 - Accuracy: 62.49%\n",
      "Epoch: 1. Batch 3890/4317 - Avg Loss: 0.7967 - Accuracy: 62.50%\n",
      "Epoch: 1. Batch 3900/4317 - Avg Loss: 0.7966 - Accuracy: 62.51%\n",
      "Epoch: 1. Batch 3910/4317 - Avg Loss: 0.7963 - Accuracy: 62.53%\n",
      "Epoch: 1. Batch 3920/4317 - Avg Loss: 0.7963 - Accuracy: 62.53%\n",
      "Epoch: 1. Batch 3930/4317 - Avg Loss: 0.7962 - Accuracy: 62.53%\n",
      "Epoch: 1. Batch 3940/4317 - Avg Loss: 0.7962 - Accuracy: 62.54%\n",
      "Epoch: 1. Batch 3950/4317 - Avg Loss: 0.7960 - Accuracy: 62.54%\n",
      "Epoch: 1. Batch 3960/4317 - Avg Loss: 0.7960 - Accuracy: 62.54%\n",
      "Epoch: 1. Batch 3970/4317 - Avg Loss: 0.7961 - Accuracy: 62.53%\n",
      "Epoch: 1. Batch 3980/4317 - Avg Loss: 0.7960 - Accuracy: 62.54%\n",
      "Epoch: 1. Batch 3990/4317 - Avg Loss: 0.7959 - Accuracy: 62.54%\n",
      "Epoch: 1. Batch 4000/4317 - Avg Loss: 0.7957 - Accuracy: 62.55%\n",
      "Epoch: 1. Batch 4010/4317 - Avg Loss: 0.7957 - Accuracy: 62.55%\n",
      "Epoch: 1. Batch 4020/4317 - Avg Loss: 0.7954 - Accuracy: 62.56%\n",
      "Epoch: 1. Batch 4030/4317 - Avg Loss: 0.7954 - Accuracy: 62.56%\n",
      "Epoch: 1. Batch 4040/4317 - Avg Loss: 0.7953 - Accuracy: 62.58%\n",
      "Epoch: 1. Batch 4050/4317 - Avg Loss: 0.7951 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4060/4317 - Avg Loss: 0.7950 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4070/4317 - Avg Loss: 0.7951 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4080/4317 - Avg Loss: 0.7952 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4090/4317 - Avg Loss: 0.7952 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4100/4317 - Avg Loss: 0.7950 - Accuracy: 62.62%\n",
      "Epoch: 1. Batch 4110/4317 - Avg Loss: 0.7949 - Accuracy: 62.62%\n",
      "Epoch: 1. Batch 4120/4317 - Avg Loss: 0.7948 - Accuracy: 62.61%\n",
      "Epoch: 1. Batch 4130/4317 - Avg Loss: 0.7948 - Accuracy: 62.61%\n",
      "Epoch: 1. Batch 4140/4317 - Avg Loss: 0.7948 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4150/4317 - Avg Loss: 0.7946 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4160/4317 - Avg Loss: 0.7946 - Accuracy: 62.60%\n",
      "Epoch: 1. Batch 4170/4317 - Avg Loss: 0.7944 - Accuracy: 62.61%\n",
      "Epoch: 1. Batch 4180/4317 - Avg Loss: 0.7940 - Accuracy: 62.63%\n",
      "Epoch: 1. Batch 4190/4317 - Avg Loss: 0.7940 - Accuracy: 62.62%\n",
      "Epoch: 1. Batch 4200/4317 - Avg Loss: 0.7939 - Accuracy: 62.63%\n",
      "Epoch: 1. Batch 4210/4317 - Avg Loss: 0.7936 - Accuracy: 62.65%\n",
      "Epoch: 1. Batch 4220/4317 - Avg Loss: 0.7938 - Accuracy: 62.63%\n",
      "Epoch: 1. Batch 4230/4317 - Avg Loss: 0.7938 - Accuracy: 62.63%\n",
      "Epoch: 1. Batch 4240/4317 - Avg Loss: 0.7936 - Accuracy: 62.64%\n",
      "Epoch: 1. Batch 4250/4317 - Avg Loss: 0.7932 - Accuracy: 62.66%\n",
      "Epoch: 1. Batch 4260/4317 - Avg Loss: 0.7932 - Accuracy: 62.66%\n",
      "Epoch: 1. Batch 4270/4317 - Avg Loss: 0.7931 - Accuracy: 62.67%\n",
      "Epoch: 1. Batch 4280/4317 - Avg Loss: 0.7929 - Accuracy: 62.69%\n",
      "Epoch: 1. Batch 4290/4317 - Avg Loss: 0.7927 - Accuracy: 62.71%\n",
      "Epoch: 1. Batch 4300/4317 - Avg Loss: 0.7926 - Accuracy: 62.70%\n",
      "Epoch: 1. Batch 4310/4317 - Avg Loss: 0.7926 - Accuracy: 62.70%\n",
      "Train loss: 0.7926 - Train accuracy: 62.71%\n",
      "Validation accuracy: 65.0517\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.6284%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 36.50%        | 58.51%              |\n",
    "| **Epoch 2** | 62.71%        | 65.05%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_sentiment_model/gru_sentiment_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
