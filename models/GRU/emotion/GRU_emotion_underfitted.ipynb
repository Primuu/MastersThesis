{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdbbd0282f43527",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.8697 - Accuracy: 18.75%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 1.9055 - Accuracy: 18.18%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 1.9460 - Accuracy: 15.18%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 1.9141 - Accuracy: 15.73%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.8950 - Accuracy: 15.09%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.8864 - Accuracy: 15.20%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.8823 - Accuracy: 15.88%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.8767 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.8744 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.8697 - Accuracy: 16.48%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.8656 - Accuracy: 16.83%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.8697 - Accuracy: 16.55%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.8694 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.8658 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.8617 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.8582 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.8576 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.8564 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.8540 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.8511 - Accuracy: 16.39%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.8515 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.8486 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.8486 - Accuracy: 16.06%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.8470 - Accuracy: 16.13%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.8462 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.8440 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.8431 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.8413 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.8396 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.8383 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 1.8368 - Accuracy: 16.57%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 1.8351 - Accuracy: 16.64%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 1.8345 - Accuracy: 16.69%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 1.8323 - Accuracy: 16.99%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 1.8309 - Accuracy: 17.10%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 1.8284 - Accuracy: 17.20%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 1.8257 - Accuracy: 17.56%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 1.8221 - Accuracy: 17.91%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 1.8190 - Accuracy: 18.13%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 1.8128 - Accuracy: 18.61%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 1.8068 - Accuracy: 18.84%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 1.7998 - Accuracy: 19.13%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 1.7922 - Accuracy: 19.43%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 1.7831 - Accuracy: 19.81%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 1.7789 - Accuracy: 19.97%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 1.7705 - Accuracy: 20.32%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 1.7649 - Accuracy: 20.47%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 1.7566 - Accuracy: 20.77%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 1.7492 - Accuracy: 21.09%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 1.7433 - Accuracy: 21.35%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 1.7355 - Accuracy: 21.72%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 1.7317 - Accuracy: 21.97%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 1.7278 - Accuracy: 22.19%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 1.7214 - Accuracy: 22.52%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 1.7176 - Accuracy: 22.75%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 1.7126 - Accuracy: 23.07%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 1.7067 - Accuracy: 23.38%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 1.7009 - Accuracy: 23.70%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 1.6954 - Accuracy: 23.94%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 1.6897 - Accuracy: 24.19%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 1.6844 - Accuracy: 24.52%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 1.6786 - Accuracy: 24.84%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 1.6729 - Accuracy: 25.07%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 1.6665 - Accuracy: 25.49%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 1.6591 - Accuracy: 25.77%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 1.6539 - Accuracy: 26.03%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 1.6473 - Accuracy: 26.37%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 1.6411 - Accuracy: 26.71%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 1.6349 - Accuracy: 27.06%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 1.6284 - Accuracy: 27.38%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 1.6211 - Accuracy: 27.75%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 1.6142 - Accuracy: 28.13%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 1.6074 - Accuracy: 28.53%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 1.5995 - Accuracy: 28.96%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 1.5918 - Accuracy: 29.36%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 1.5846 - Accuracy: 29.76%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 1.5771 - Accuracy: 30.20%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 1.5702 - Accuracy: 30.56%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 1.5627 - Accuracy: 31.03%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 1.5547 - Accuracy: 31.52%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 1.5464 - Accuracy: 32.00%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 1.5385 - Accuracy: 32.41%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 1.5296 - Accuracy: 32.86%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 1.5208 - Accuracy: 33.29%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 1.5128 - Accuracy: 33.71%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 1.5050 - Accuracy: 34.08%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 1.4966 - Accuracy: 34.49%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 1.4884 - Accuracy: 34.87%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 1.4798 - Accuracy: 35.33%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 1.4709 - Accuracy: 35.73%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 1.4630 - Accuracy: 36.15%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 1.4551 - Accuracy: 36.55%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 1.4467 - Accuracy: 36.92%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 1.4389 - Accuracy: 37.30%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 1.4309 - Accuracy: 37.66%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 1.4222 - Accuracy: 38.08%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 1.4134 - Accuracy: 38.49%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 1.4045 - Accuracy: 38.92%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 1.3969 - Accuracy: 39.30%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 1.3881 - Accuracy: 39.78%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 1.3797 - Accuracy: 40.15%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 1.3720 - Accuracy: 40.54%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 1.3638 - Accuracy: 40.94%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 1.3567 - Accuracy: 41.29%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 1.3508 - Accuracy: 41.58%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 1.3431 - Accuracy: 41.92%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 1.3353 - Accuracy: 42.30%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 1.3277 - Accuracy: 42.66%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 1.3193 - Accuracy: 43.07%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 1.3114 - Accuracy: 43.43%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 1.3053 - Accuracy: 43.72%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 1.2972 - Accuracy: 44.13%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 1.2893 - Accuracy: 44.50%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 1.2827 - Accuracy: 44.82%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 1.2753 - Accuracy: 45.17%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 1.2680 - Accuracy: 45.52%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 1.2601 - Accuracy: 45.89%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 1.2523 - Accuracy: 46.24%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 1.2450 - Accuracy: 46.58%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 1.2381 - Accuracy: 46.89%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 1.2309 - Accuracy: 47.22%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 1.2241 - Accuracy: 47.53%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 1.2169 - Accuracy: 47.87%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 1.2105 - Accuracy: 48.16%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 1.2038 - Accuracy: 48.47%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 1.1967 - Accuracy: 48.81%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 1.1900 - Accuracy: 49.10%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 1.1834 - Accuracy: 49.40%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 1.1774 - Accuracy: 49.66%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 1.1704 - Accuracy: 49.99%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 1.1635 - Accuracy: 50.29%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 1.1576 - Accuracy: 50.57%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 1.1522 - Accuracy: 50.82%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 1.1465 - Accuracy: 51.10%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 1.1396 - Accuracy: 51.41%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 1.1344 - Accuracy: 51.65%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 1.1282 - Accuracy: 51.93%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 1.1225 - Accuracy: 52.18%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 1.1166 - Accuracy: 52.45%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 1.1104 - Accuracy: 52.72%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 1.1050 - Accuracy: 52.97%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 1.0991 - Accuracy: 53.23%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 1.0943 - Accuracy: 53.45%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 1.0895 - Accuracy: 53.67%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 1.0839 - Accuracy: 53.93%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 1.0784 - Accuracy: 54.19%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 1.0731 - Accuracy: 54.43%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 1.0681 - Accuracy: 54.66%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 1.0629 - Accuracy: 54.87%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 1.0577 - Accuracy: 55.11%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 1.0524 - Accuracy: 55.34%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 1.0469 - Accuracy: 55.58%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 1.0420 - Accuracy: 55.82%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 1.0373 - Accuracy: 56.03%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 1.0322 - Accuracy: 56.26%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 1.0264 - Accuracy: 56.51%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 1.0215 - Accuracy: 56.74%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 1.0163 - Accuracy: 56.97%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 1.0109 - Accuracy: 57.19%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 1.0060 - Accuracy: 57.40%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 1.0017 - Accuracy: 57.59%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 0.9970 - Accuracy: 57.79%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 0.9925 - Accuracy: 57.99%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 0.9879 - Accuracy: 58.20%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 0.9833 - Accuracy: 58.39%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 0.9793 - Accuracy: 58.57%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 0.9749 - Accuracy: 58.78%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 0.9705 - Accuracy: 58.97%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 0.9658 - Accuracy: 59.19%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 0.9613 - Accuracy: 59.40%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 0.9574 - Accuracy: 59.58%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 0.9533 - Accuracy: 59.77%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 0.9489 - Accuracy: 59.95%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 0.9447 - Accuracy: 60.14%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 0.9407 - Accuracy: 60.32%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 0.9364 - Accuracy: 60.50%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 0.9323 - Accuracy: 60.69%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 0.9282 - Accuracy: 60.87%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 0.9243 - Accuracy: 61.03%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 0.9206 - Accuracy: 61.20%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 0.9174 - Accuracy: 61.34%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 0.9138 - Accuracy: 61.50%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 0.9096 - Accuracy: 61.68%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 0.9060 - Accuracy: 61.84%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 0.9026 - Accuracy: 61.99%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 0.8985 - Accuracy: 62.17%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 0.8946 - Accuracy: 62.36%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 0.8908 - Accuracy: 62.51%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 0.8870 - Accuracy: 62.67%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 0.8834 - Accuracy: 62.82%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 0.8801 - Accuracy: 62.95%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 0.8770 - Accuracy: 63.09%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 0.8734 - Accuracy: 63.24%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 0.8702 - Accuracy: 63.37%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 0.8670 - Accuracy: 63.50%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 0.8634 - Accuracy: 63.66%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 0.8601 - Accuracy: 63.80%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 0.8572 - Accuracy: 63.92%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 0.8537 - Accuracy: 64.07%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 0.8507 - Accuracy: 64.20%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 0.8471 - Accuracy: 64.36%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 0.8439 - Accuracy: 64.49%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 0.8405 - Accuracy: 64.64%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 0.8375 - Accuracy: 64.76%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 0.8346 - Accuracy: 64.89%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 0.8317 - Accuracy: 65.02%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 0.8287 - Accuracy: 65.15%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 0.8259 - Accuracy: 65.28%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 0.8231 - Accuracy: 65.40%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 0.8205 - Accuracy: 65.52%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 0.8180 - Accuracy: 65.62%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 0.8153 - Accuracy: 65.73%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 0.8125 - Accuracy: 65.85%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 0.8094 - Accuracy: 65.98%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 0.8064 - Accuracy: 66.11%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 0.8037 - Accuracy: 66.22%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 0.8010 - Accuracy: 66.34%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 0.7986 - Accuracy: 66.46%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 0.7957 - Accuracy: 66.59%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 0.7930 - Accuracy: 66.71%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 0.7902 - Accuracy: 66.84%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 0.7876 - Accuracy: 66.94%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 0.7852 - Accuracy: 67.05%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 0.7826 - Accuracy: 67.17%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 0.7801 - Accuracy: 67.27%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 0.7775 - Accuracy: 67.38%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 0.7747 - Accuracy: 67.50%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 0.7721 - Accuracy: 67.62%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 0.7694 - Accuracy: 67.73%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 0.7671 - Accuracy: 67.85%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 0.7645 - Accuracy: 67.96%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 0.7621 - Accuracy: 68.06%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 0.7596 - Accuracy: 68.17%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 0.7569 - Accuracy: 68.29%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 0.7545 - Accuracy: 68.39%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 0.7519 - Accuracy: 68.51%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 0.7493 - Accuracy: 68.63%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 0.7469 - Accuracy: 68.73%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 0.7443 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 0.7422 - Accuracy: 68.93%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 0.7399 - Accuracy: 69.03%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 0.7375 - Accuracy: 69.13%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 0.7353 - Accuracy: 69.22%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 0.7331 - Accuracy: 69.32%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 0.7309 - Accuracy: 69.41%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 0.7286 - Accuracy: 69.49%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 0.7265 - Accuracy: 69.59%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 0.7243 - Accuracy: 69.67%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 0.7226 - Accuracy: 69.76%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 0.7205 - Accuracy: 69.84%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 0.7183 - Accuracy: 69.94%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 0.7162 - Accuracy: 70.03%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 0.7142 - Accuracy: 70.12%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 0.7123 - Accuracy: 70.19%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 0.7105 - Accuracy: 70.27%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 0.7082 - Accuracy: 70.37%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 0.7061 - Accuracy: 70.46%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 0.7039 - Accuracy: 70.56%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 0.7017 - Accuracy: 70.65%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 0.6998 - Accuracy: 70.73%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 0.6979 - Accuracy: 70.81%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 0.6958 - Accuracy: 70.90%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 0.6939 - Accuracy: 70.98%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 0.6918 - Accuracy: 71.07%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 0.6898 - Accuracy: 71.14%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 0.6877 - Accuracy: 71.22%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 0.6858 - Accuracy: 71.31%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 0.6841 - Accuracy: 71.38%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 0.6821 - Accuracy: 71.46%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 0.6802 - Accuracy: 71.55%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 0.6786 - Accuracy: 71.62%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 0.6769 - Accuracy: 71.70%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 0.6750 - Accuracy: 71.78%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 0.6730 - Accuracy: 71.86%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 0.6711 - Accuracy: 71.94%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 0.6692 - Accuracy: 72.02%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 0.6676 - Accuracy: 72.10%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 0.6656 - Accuracy: 72.19%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 0.6639 - Accuracy: 72.27%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 0.6623 - Accuracy: 72.32%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 0.6604 - Accuracy: 72.40%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 0.6586 - Accuracy: 72.48%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 0.6566 - Accuracy: 72.57%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 0.6548 - Accuracy: 72.64%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 0.6532 - Accuracy: 72.71%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 0.6517 - Accuracy: 72.76%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 0.6500 - Accuracy: 72.83%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 0.6483 - Accuracy: 72.90%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 0.6465 - Accuracy: 72.98%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 0.6446 - Accuracy: 73.06%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 0.6431 - Accuracy: 73.12%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 0.6413 - Accuracy: 73.20%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 0.6398 - Accuracy: 73.26%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 0.6382 - Accuracy: 73.33%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 0.6366 - Accuracy: 73.40%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 0.6350 - Accuracy: 73.47%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 0.6334 - Accuracy: 73.54%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 0.6318 - Accuracy: 73.61%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 0.6301 - Accuracy: 73.69%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 0.6285 - Accuracy: 73.76%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 0.6268 - Accuracy: 73.83%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 0.6253 - Accuracy: 73.90%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 0.6239 - Accuracy: 73.96%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 0.6223 - Accuracy: 74.03%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 0.6209 - Accuracy: 74.09%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 0.6197 - Accuracy: 74.14%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 0.6180 - Accuracy: 74.21%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 0.6165 - Accuracy: 74.28%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 0.6149 - Accuracy: 74.34%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 0.6133 - Accuracy: 74.41%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 0.6119 - Accuracy: 74.47%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 0.6106 - Accuracy: 74.53%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 0.6092 - Accuracy: 74.59%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 0.6079 - Accuracy: 74.64%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 0.6065 - Accuracy: 74.70%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 0.6050 - Accuracy: 74.77%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 0.6038 - Accuracy: 74.82%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 0.6024 - Accuracy: 74.88%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 0.6010 - Accuracy: 74.94%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 0.5996 - Accuracy: 74.99%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 0.5983 - Accuracy: 75.05%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 0.5971 - Accuracy: 75.10%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 0.5957 - Accuracy: 75.16%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 0.5942 - Accuracy: 75.22%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 0.5930 - Accuracy: 75.28%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 0.5917 - Accuracy: 75.33%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 0.5905 - Accuracy: 75.38%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 0.5890 - Accuracy: 75.45%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 0.5877 - Accuracy: 75.50%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 0.5863 - Accuracy: 75.55%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 0.5850 - Accuracy: 75.61%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 0.5838 - Accuracy: 75.66%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 0.5825 - Accuracy: 75.72%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 0.5810 - Accuracy: 75.78%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 0.5799 - Accuracy: 75.83%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 0.5788 - Accuracy: 75.87%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 0.5777 - Accuracy: 75.92%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 0.5766 - Accuracy: 75.96%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 0.5755 - Accuracy: 76.01%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 0.5743 - Accuracy: 76.06%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 0.5731 - Accuracy: 76.11%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 0.5718 - Accuracy: 76.17%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 0.5705 - Accuracy: 76.23%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 0.5693 - Accuracy: 76.28%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 0.5681 - Accuracy: 76.34%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 0.5670 - Accuracy: 76.38%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 0.5658 - Accuracy: 76.43%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 0.5645 - Accuracy: 76.48%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 0.5634 - Accuracy: 76.53%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 0.5621 - Accuracy: 76.59%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 0.5609 - Accuracy: 76.64%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 0.5597 - Accuracy: 76.69%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 0.5586 - Accuracy: 76.73%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 0.5574 - Accuracy: 76.78%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 0.5565 - Accuracy: 76.82%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 0.5556 - Accuracy: 76.86%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 0.5547 - Accuracy: 76.90%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 0.5536 - Accuracy: 76.94%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 0.5524 - Accuracy: 76.99%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 0.5512 - Accuracy: 77.04%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 0.5500 - Accuracy: 77.09%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 0.5490 - Accuracy: 77.14%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 0.5480 - Accuracy: 77.18%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 0.5468 - Accuracy: 77.23%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 0.5458 - Accuracy: 77.28%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 0.5445 - Accuracy: 77.33%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 0.5436 - Accuracy: 77.37%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 0.5425 - Accuracy: 77.42%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 0.5414 - Accuracy: 77.46%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 0.5404 - Accuracy: 77.51%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 0.5393 - Accuracy: 77.56%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 0.5382 - Accuracy: 77.61%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 0.5372 - Accuracy: 77.65%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 0.5361 - Accuracy: 77.69%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 0.5351 - Accuracy: 77.74%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 0.5340 - Accuracy: 77.78%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 0.5329 - Accuracy: 77.82%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 0.5321 - Accuracy: 77.86%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 0.5309 - Accuracy: 77.91%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 0.5297 - Accuracy: 77.96%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 0.5290 - Accuracy: 77.99%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 0.5280 - Accuracy: 78.03%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 0.5270 - Accuracy: 78.07%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 0.5263 - Accuracy: 78.10%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 0.5255 - Accuracy: 78.13%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 0.5246 - Accuracy: 78.16%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 0.5238 - Accuracy: 78.19%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 0.5228 - Accuracy: 78.23%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 0.5217 - Accuracy: 78.27%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 0.5207 - Accuracy: 78.31%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 0.5197 - Accuracy: 78.35%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 0.5188 - Accuracy: 78.40%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 0.5178 - Accuracy: 78.44%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 0.5168 - Accuracy: 78.49%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 0.5160 - Accuracy: 78.52%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 0.5150 - Accuracy: 78.56%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 0.5139 - Accuracy: 78.60%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 0.5130 - Accuracy: 78.64%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 0.5120 - Accuracy: 78.68%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 0.5111 - Accuracy: 78.71%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 0.5103 - Accuracy: 78.75%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 0.5095 - Accuracy: 78.78%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 0.5085 - Accuracy: 78.82%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 0.5076 - Accuracy: 78.86%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 0.5067 - Accuracy: 78.90%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 0.5057 - Accuracy: 78.94%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 0.5049 - Accuracy: 78.97%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 0.5039 - Accuracy: 79.01%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 0.5030 - Accuracy: 79.05%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 0.5021 - Accuracy: 79.08%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 0.5012 - Accuracy: 79.12%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 0.5004 - Accuracy: 79.16%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 0.4995 - Accuracy: 79.19%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 0.4987 - Accuracy: 79.22%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 0.4980 - Accuracy: 79.26%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 0.4971 - Accuracy: 79.30%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 0.4963 - Accuracy: 79.33%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 0.4955 - Accuracy: 79.37%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 0.4947 - Accuracy: 79.40%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 0.4939 - Accuracy: 79.43%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 0.4931 - Accuracy: 79.47%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 0.4921 - Accuracy: 79.51%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 0.4914 - Accuracy: 79.54%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 0.4906 - Accuracy: 79.58%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 0.4898 - Accuracy: 79.61%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 0.4892 - Accuracy: 79.63%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 0.4884 - Accuracy: 79.66%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 0.4876 - Accuracy: 79.69%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 0.4869 - Accuracy: 79.72%\n",
      "Train loss: 0.4861 - Train accuracy: 79.75%\n",
      "Validation accuracy: 93.0437\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 0.0546 - Accuracy: 100.00%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 0.0893 - Accuracy: 96.02%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 0.1327 - Accuracy: 94.05%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 0.1352 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 0.1272 - Accuracy: 94.66%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 0.1255 - Accuracy: 94.61%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 0.1269 - Accuracy: 94.57%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 0.1295 - Accuracy: 94.63%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 0.1344 - Accuracy: 94.52%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 0.1334 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 0.1306 - Accuracy: 94.55%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 0.1271 - Accuracy: 94.71%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 0.1327 - Accuracy: 94.63%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 0.1321 - Accuracy: 94.56%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 0.1312 - Accuracy: 94.46%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 0.1291 - Accuracy: 94.54%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 0.1275 - Accuracy: 94.64%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 0.1272 - Accuracy: 94.70%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 0.1270 - Accuracy: 94.72%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 0.1257 - Accuracy: 94.76%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 0.1279 - Accuracy: 94.62%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 0.1269 - Accuracy: 94.67%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 0.1263 - Accuracy: 94.65%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 0.1262 - Accuracy: 94.59%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 0.1262 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 0.1259 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 0.1244 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 0.1269 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 0.1268 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 0.1268 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 0.1251 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 0.1263 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 0.1241 - Accuracy: 94.53%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 0.1237 - Accuracy: 94.58%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 0.1251 - Accuracy: 94.48%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 0.1255 - Accuracy: 94.48%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 0.1255 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 0.1259 - Accuracy: 94.47%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 0.1253 - Accuracy: 94.47%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 0.1270 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 0.1277 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 0.1278 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 0.1280 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 0.1288 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 0.1284 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 0.1291 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 0.1299 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 0.1297 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 0.1295 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 0.1288 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 0.1277 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 0.1268 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 0.1262 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 0.1262 - Accuracy: 94.48%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 0.1272 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 0.1275 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 0.1269 - Accuracy: 94.46%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 0.1263 - Accuracy: 94.48%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 0.1268 - Accuracy: 94.46%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 0.1268 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 0.1274 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 0.1268 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 0.1269 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 0.1269 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 0.1279 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 0.1279 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 0.1273 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 0.1267 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 0.1264 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 0.1265 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 0.1266 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 0.1266 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 0.1271 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 0.1278 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 0.1279 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 0.1283 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 0.1277 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 0.1271 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 0.1277 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 0.1274 - Accuracy: 94.38%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 0.1278 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 0.1283 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 0.1282 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 0.1284 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 0.1287 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 0.1289 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 0.1287 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 0.1287 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 0.1296 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 0.1291 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 0.1289 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 0.1292 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 0.1292 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 0.1300 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 0.1298 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 0.1295 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 0.1290 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 0.1291 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 0.1291 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 0.1291 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 0.1293 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 0.1291 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 0.1297 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 0.1294 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 0.1288 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 0.1293 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 0.1292 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 0.1294 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 0.1297 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 0.1304 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 0.1308 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 0.1313 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 0.1316 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 0.1315 - Accuracy: 94.16%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 0.1317 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 0.1317 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 0.1313 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 0.1314 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 0.1312 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 0.1311 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 0.1312 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 0.1317 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 0.1315 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 0.1316 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 0.1316 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 0.1315 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 0.1314 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 0.1315 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 0.1311 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 0.1315 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 0.1312 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 0.1310 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 0.1307 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 0.1305 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 0.1303 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 0.1311 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 0.1309 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 0.1308 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 0.1307 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 0.1306 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 0.1304 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 0.1305 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 0.1305 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 0.1306 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 0.1307 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 0.1311 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 0.1315 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 0.1314 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 0.1314 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 0.1315 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 0.1312 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 0.1310 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 0.1312 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 0.1311 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 0.1312 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 0.1309 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 0.1311 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 0.1315 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 0.1320 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 0.1319 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 0.1317 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 0.1321 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 0.1318 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 0.1318 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 0.1316 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 0.1317 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 0.1318 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 0.1316 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 0.1314 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 0.1314 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 0.1312 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 0.1311 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 0.1313 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 0.1312 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 0.1314 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 0.1313 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 0.1312 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 0.1311 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 0.1310 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 0.1309 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 0.1307 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 0.1310 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 0.1311 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 0.1312 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 0.1312 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 0.1310 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 0.1309 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 0.1311 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 0.1310 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 0.1308 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 0.1310 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 0.1314 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 0.1313 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 0.1316 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 0.1315 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 0.1313 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 0.1317 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 0.1315 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 0.1318 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 0.1318 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 0.1322 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 0.1321 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 0.1321 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 0.1320 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 0.1319 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 0.1319 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 0.1317 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 0.1320 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 0.1321 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 0.1326 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 0.1325 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 0.1328 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 0.1327 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 0.1328 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 0.1326 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 0.1327 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 0.1327 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 0.1328 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 0.1328 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 0.1326 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 0.1324 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 0.1323 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 0.1320 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 0.1319 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 0.1319 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 0.1319 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 0.1318 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 0.1319 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 0.1319 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 0.1323 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 0.1325 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 0.1324 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 0.1324 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 0.1324 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 0.1321 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 0.1320 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 0.1320 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 0.1322 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 0.1321 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 0.1321 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 0.1322 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 0.1320 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 0.1321 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 0.1319 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 0.1319 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 0.1320 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 0.1320 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 0.1321 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 0.1323 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 0.1323 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 0.1324 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 0.1322 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 0.1323 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 0.1321 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 0.1319 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 0.1318 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 0.1317 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 0.1318 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 0.1319 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 0.1319 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 0.1316 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 0.1318 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 0.1319 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 0.1318 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 0.1319 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 0.1321 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 0.1321 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 0.1319 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 0.1317 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 0.1318 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 0.1316 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 0.1315 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 0.1317 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 0.1316 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 0.1317 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 0.1315 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 0.1315 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 0.1313 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 0.1314 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 0.1314 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 0.1314 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 0.1313 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 0.1313 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 0.1314 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 0.1313 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 0.1315 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 0.1315 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 0.1315 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 0.1314 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 0.1313 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 0.1313 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 0.1315 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 0.1315 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 0.1315 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 0.1309 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 0.1309 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 0.1309 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 0.1316 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 0.1315 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 0.1314 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 0.1313 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 0.1312 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 0.1309 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 0.1308 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 0.1309 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 0.1309 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 0.1313 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 0.1316 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 0.1315 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 0.1317 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 0.1317 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 0.1318 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 0.1317 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 0.1317 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 0.1318 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 0.1318 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 0.1319 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 0.1319 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 0.1318 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 0.1317 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 0.1317 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 0.1317 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 0.1316 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 0.1316 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 0.1317 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 0.1318 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 0.1316 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 0.1316 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 0.1315 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 0.1314 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 0.1313 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 0.1312 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 0.1311 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 0.1310 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 0.1311 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 0.1310 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 0.1309 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 0.1310 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 0.1311 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 0.1310 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 0.1310 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 0.1310 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 0.1311 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 0.1311 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 0.1313 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 0.1312 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 0.1313 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 0.1311 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 0.1314 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 0.1313 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 0.1313 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 0.1313 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 0.1312 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 0.1313 - Accuracy: 94.34%\n",
      "Train loss: 0.1313 - Train accuracy: 94.34%\n",
      "Validation accuracy: 94.6824\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.4784%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 79.75%        | 93.04%              |\n",
    "| **Epoch 2** | 94.34%        | 94.68%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** remains nearly constant (~94), with a slight **increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_emotion_model_underfitted/gru_emotion_model_underfitted.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
