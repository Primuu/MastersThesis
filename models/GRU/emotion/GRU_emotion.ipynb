{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bdbbd0282f43527",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.8006 - Accuracy: 25.00%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 2.0444 - Accuracy: 17.05%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 2.0114 - Accuracy: 17.26%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 2.0095 - Accuracy: 16.53%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.9718 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.9465 - Accuracy: 15.69%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.9307 - Accuracy: 16.09%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.9176 - Accuracy: 15.85%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.9073 - Accuracy: 15.90%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.8986 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.8912 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.8857 - Accuracy: 16.84%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.8837 - Accuracy: 16.48%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.8784 - Accuracy: 16.75%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.8775 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.8750 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.8714 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.8685 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.8663 - Accuracy: 16.57%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.8642 - Accuracy: 16.82%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.8610 - Accuracy: 16.82%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.8575 - Accuracy: 16.65%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.8554 - Accuracy: 16.66%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.8555 - Accuracy: 16.86%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.8545 - Accuracy: 16.73%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.8523 - Accuracy: 16.76%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.8516 - Accuracy: 16.67%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.8497 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.8491 - Accuracy: 16.68%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.8478 - Accuracy: 16.90%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 1.8469 - Accuracy: 16.90%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 1.8459 - Accuracy: 16.78%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 1.8453 - Accuracy: 16.67%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 1.8440 - Accuracy: 16.64%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 1.8427 - Accuracy: 16.72%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 1.8415 - Accuracy: 16.77%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 1.8415 - Accuracy: 16.78%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 1.8405 - Accuracy: 16.73%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 1.8412 - Accuracy: 16.63%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 1.8406 - Accuracy: 16.66%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 1.8403 - Accuracy: 16.55%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 1.8396 - Accuracy: 16.51%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 1.8388 - Accuracy: 16.54%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 1.8378 - Accuracy: 16.55%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 1.8371 - Accuracy: 16.64%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 1.8366 - Accuracy: 16.64%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 1.8365 - Accuracy: 16.55%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 1.8361 - Accuracy: 16.63%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 1.8358 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 1.8352 - Accuracy: 16.57%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 1.8344 - Accuracy: 16.65%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 1.8339 - Accuracy: 16.65%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 1.8339 - Accuracy: 16.53%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 1.8330 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 1.8331 - Accuracy: 16.62%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 1.8325 - Accuracy: 16.61%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 1.8320 - Accuracy: 16.64%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 1.8318 - Accuracy: 16.60%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 1.8314 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 1.8311 - Accuracy: 16.57%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 1.8313 - Accuracy: 16.52%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 1.8311 - Accuracy: 16.50%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 1.8306 - Accuracy: 16.47%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 1.8303 - Accuracy: 16.53%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 1.8301 - Accuracy: 16.54%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 1.8299 - Accuracy: 16.46%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 1.8297 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 1.8293 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 1.8291 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 1.8289 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 1.8292 - Accuracy: 16.43%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 1.8291 - Accuracy: 16.43%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 1.8290 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 1.8288 - Accuracy: 16.42%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 1.8285 - Accuracy: 16.46%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 1.8283 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 1.8279 - Accuracy: 16.43%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 1.8274 - Accuracy: 16.46%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 1.8270 - Accuracy: 16.48%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 1.8268 - Accuracy: 16.49%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 1.8265 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 1.8264 - Accuracy: 16.43%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 1.8259 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 1.8261 - Accuracy: 16.40%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 1.8259 - Accuracy: 16.42%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 1.8255 - Accuracy: 16.50%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 1.8256 - Accuracy: 16.50%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 1.8255 - Accuracy: 16.45%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 1.8253 - Accuracy: 16.42%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 1.8251 - Accuracy: 16.41%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 1.8245 - Accuracy: 16.43%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 1.8243 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 1.8241 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 1.8235 - Accuracy: 16.48%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 1.8228 - Accuracy: 16.58%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 1.8224 - Accuracy: 16.65%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 1.8218 - Accuracy: 16.69%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 1.8209 - Accuracy: 16.83%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 1.8200 - Accuracy: 16.92%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 1.8185 - Accuracy: 17.04%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 1.8169 - Accuracy: 17.16%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 1.8149 - Accuracy: 17.22%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 1.8124 - Accuracy: 17.40%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 1.8096 - Accuracy: 17.55%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 1.8069 - Accuracy: 17.65%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 1.8046 - Accuracy: 17.79%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 1.8013 - Accuracy: 18.00%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 1.7981 - Accuracy: 18.11%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 1.7946 - Accuracy: 18.29%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 1.7915 - Accuracy: 18.45%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 1.7888 - Accuracy: 18.60%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 1.7850 - Accuracy: 18.77%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 1.7821 - Accuracy: 18.88%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 1.7788 - Accuracy: 19.10%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 1.7743 - Accuracy: 19.34%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 1.7704 - Accuracy: 19.59%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 1.7663 - Accuracy: 19.82%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 1.7620 - Accuracy: 20.12%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 1.7572 - Accuracy: 20.39%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 1.7523 - Accuracy: 20.63%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 1.7474 - Accuracy: 20.89%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 1.7416 - Accuracy: 21.20%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 1.7382 - Accuracy: 21.43%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 1.7328 - Accuracy: 21.72%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 1.7274 - Accuracy: 21.95%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 1.7217 - Accuracy: 22.24%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 1.7152 - Accuracy: 22.59%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 1.7094 - Accuracy: 22.95%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 1.7033 - Accuracy: 23.26%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 1.6975 - Accuracy: 23.63%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 1.6913 - Accuracy: 23.98%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 1.6857 - Accuracy: 24.27%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 1.6804 - Accuracy: 24.57%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 1.6747 - Accuracy: 24.88%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 1.6680 - Accuracy: 25.25%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 1.6615 - Accuracy: 25.58%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 1.6557 - Accuracy: 25.87%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 1.6486 - Accuracy: 26.24%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 1.6419 - Accuracy: 26.59%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 1.6355 - Accuracy: 26.92%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 1.6288 - Accuracy: 27.24%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 1.6222 - Accuracy: 27.58%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 1.6157 - Accuracy: 27.90%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 1.6082 - Accuracy: 28.28%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 1.6007 - Accuracy: 28.67%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 1.5936 - Accuracy: 29.03%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 1.5871 - Accuracy: 29.37%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 1.5799 - Accuracy: 29.76%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 1.5728 - Accuracy: 30.10%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 1.5664 - Accuracy: 30.41%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 1.5596 - Accuracy: 30.76%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 1.5526 - Accuracy: 31.11%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 1.5464 - Accuracy: 31.42%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 1.5393 - Accuracy: 31.76%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 1.5327 - Accuracy: 32.07%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 1.5259 - Accuracy: 32.40%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 1.5192 - Accuracy: 32.73%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 1.5135 - Accuracy: 33.02%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 1.5069 - Accuracy: 33.35%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 1.5012 - Accuracy: 33.61%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 1.4944 - Accuracy: 33.93%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 1.4879 - Accuracy: 34.23%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 1.4818 - Accuracy: 34.54%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 1.4749 - Accuracy: 34.86%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 1.4690 - Accuracy: 35.16%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 1.4626 - Accuracy: 35.46%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 1.4559 - Accuracy: 35.77%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 1.4508 - Accuracy: 36.02%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 1.4440 - Accuracy: 36.34%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 1.4381 - Accuracy: 36.62%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 1.4320 - Accuracy: 36.91%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 1.4258 - Accuracy: 37.21%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 1.4202 - Accuracy: 37.47%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 1.4142 - Accuracy: 37.75%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 1.4086 - Accuracy: 38.01%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 1.4025 - Accuracy: 38.30%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 1.3971 - Accuracy: 38.55%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 1.3911 - Accuracy: 38.83%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 1.3855 - Accuracy: 39.08%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 1.3797 - Accuracy: 39.36%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 1.3744 - Accuracy: 39.61%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 1.3689 - Accuracy: 39.87%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 1.3630 - Accuracy: 40.15%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 1.3576 - Accuracy: 40.40%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 1.3523 - Accuracy: 40.66%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 1.3466 - Accuracy: 40.91%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 1.3413 - Accuracy: 41.16%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 1.3358 - Accuracy: 41.42%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 1.3306 - Accuracy: 41.65%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 1.3261 - Accuracy: 41.87%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 1.3203 - Accuracy: 42.13%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 1.3154 - Accuracy: 42.36%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 1.3105 - Accuracy: 42.59%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 1.3057 - Accuracy: 42.80%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 1.3005 - Accuracy: 43.04%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 1.2954 - Accuracy: 43.29%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 1.2901 - Accuracy: 43.53%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 1.2851 - Accuracy: 43.76%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 1.2806 - Accuracy: 43.97%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 1.2756 - Accuracy: 44.19%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 1.2706 - Accuracy: 44.42%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 1.2658 - Accuracy: 44.65%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 1.2608 - Accuracy: 44.88%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 1.2561 - Accuracy: 45.09%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 1.2513 - Accuracy: 45.31%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 1.2465 - Accuracy: 45.51%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 1.2416 - Accuracy: 45.73%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 1.2366 - Accuracy: 45.96%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 1.2323 - Accuracy: 46.17%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 1.2276 - Accuracy: 46.37%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 1.2235 - Accuracy: 46.57%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 1.2193 - Accuracy: 46.76%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 1.2149 - Accuracy: 46.97%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 1.2103 - Accuracy: 47.18%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 1.2058 - Accuracy: 47.39%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 1.2014 - Accuracy: 47.59%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 1.1972 - Accuracy: 47.79%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 1.1926 - Accuracy: 48.00%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 1.1884 - Accuracy: 48.19%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 1.1840 - Accuracy: 48.39%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 1.1796 - Accuracy: 48.59%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 1.1754 - Accuracy: 48.78%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 1.1711 - Accuracy: 48.97%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 1.1666 - Accuracy: 49.17%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 1.1626 - Accuracy: 49.35%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 1.1586 - Accuracy: 49.52%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 1.1548 - Accuracy: 49.70%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 1.1509 - Accuracy: 49.88%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 1.1470 - Accuracy: 50.06%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 1.1425 - Accuracy: 50.26%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 1.1385 - Accuracy: 50.44%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 1.1344 - Accuracy: 50.62%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 1.1305 - Accuracy: 50.79%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 1.1264 - Accuracy: 50.98%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 1.1225 - Accuracy: 51.15%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 1.1187 - Accuracy: 51.33%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 1.1149 - Accuracy: 51.50%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 1.1115 - Accuracy: 51.65%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 1.1080 - Accuracy: 51.80%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 1.1044 - Accuracy: 51.96%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 1.1006 - Accuracy: 52.13%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 1.0970 - Accuracy: 52.29%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 1.0932 - Accuracy: 52.46%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 1.0895 - Accuracy: 52.61%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 1.0858 - Accuracy: 52.77%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 1.0822 - Accuracy: 52.93%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 1.0787 - Accuracy: 53.09%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 1.0752 - Accuracy: 53.24%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 1.0716 - Accuracy: 53.40%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 1.0683 - Accuracy: 53.55%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 1.0650 - Accuracy: 53.71%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 1.0621 - Accuracy: 53.84%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 1.0586 - Accuracy: 54.00%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 1.0553 - Accuracy: 54.15%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 1.0520 - Accuracy: 54.30%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 1.0484 - Accuracy: 54.45%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 1.0448 - Accuracy: 54.61%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 1.0414 - Accuracy: 54.76%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 1.0388 - Accuracy: 54.88%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 1.0354 - Accuracy: 55.02%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 1.0322 - Accuracy: 55.17%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 1.0288 - Accuracy: 55.32%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 1.0260 - Accuracy: 55.46%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 1.0226 - Accuracy: 55.61%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 1.0192 - Accuracy: 55.77%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 1.0160 - Accuracy: 55.91%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 1.0127 - Accuracy: 56.06%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 1.0097 - Accuracy: 56.20%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 1.0064 - Accuracy: 56.34%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 1.0034 - Accuracy: 56.48%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 1.0005 - Accuracy: 56.61%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 0.9976 - Accuracy: 56.74%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 0.9946 - Accuracy: 56.88%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 0.9918 - Accuracy: 57.01%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 0.9890 - Accuracy: 57.13%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 0.9860 - Accuracy: 57.26%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 0.9830 - Accuracy: 57.39%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 0.9801 - Accuracy: 57.51%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 0.9773 - Accuracy: 57.64%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 0.9747 - Accuracy: 57.77%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 0.9716 - Accuracy: 57.91%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 0.9689 - Accuracy: 58.03%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 0.9662 - Accuracy: 58.15%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 0.9635 - Accuracy: 58.27%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 0.9606 - Accuracy: 58.39%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 0.9580 - Accuracy: 58.50%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 0.9556 - Accuracy: 58.61%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 0.9530 - Accuracy: 58.72%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 0.9505 - Accuracy: 58.83%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 0.9477 - Accuracy: 58.95%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 0.9451 - Accuracy: 59.07%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 0.9426 - Accuracy: 59.19%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 0.9399 - Accuracy: 59.32%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 0.9373 - Accuracy: 59.44%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 0.9348 - Accuracy: 59.54%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 0.9323 - Accuracy: 59.65%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 0.9297 - Accuracy: 59.77%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 0.9272 - Accuracy: 59.88%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 0.9247 - Accuracy: 59.99%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 0.9222 - Accuracy: 60.10%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 0.9197 - Accuracy: 60.21%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 0.9171 - Accuracy: 60.32%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 0.9147 - Accuracy: 60.43%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 0.9121 - Accuracy: 60.54%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 0.9098 - Accuracy: 60.65%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 0.9077 - Accuracy: 60.75%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 0.9054 - Accuracy: 60.85%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 0.9030 - Accuracy: 60.95%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 0.9006 - Accuracy: 61.06%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 0.8982 - Accuracy: 61.16%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 0.8961 - Accuracy: 61.26%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 0.8939 - Accuracy: 61.35%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 0.8918 - Accuracy: 61.44%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 0.8894 - Accuracy: 61.55%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 0.8873 - Accuracy: 61.65%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 0.8851 - Accuracy: 61.74%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 0.8831 - Accuracy: 61.82%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 0.8811 - Accuracy: 61.90%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 0.8789 - Accuracy: 62.00%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 0.8772 - Accuracy: 62.08%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 0.8750 - Accuracy: 62.18%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 0.8728 - Accuracy: 62.27%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 0.8705 - Accuracy: 62.38%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 0.8683 - Accuracy: 62.48%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 0.8660 - Accuracy: 62.58%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 0.8639 - Accuracy: 62.67%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 0.8619 - Accuracy: 62.76%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 0.8599 - Accuracy: 62.85%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 0.8579 - Accuracy: 62.95%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 0.8561 - Accuracy: 63.03%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 0.8541 - Accuracy: 63.12%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 0.8522 - Accuracy: 63.20%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 0.8502 - Accuracy: 63.29%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 0.8482 - Accuracy: 63.38%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 0.8464 - Accuracy: 63.46%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 0.8446 - Accuracy: 63.53%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 0.8427 - Accuracy: 63.61%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 0.8406 - Accuracy: 63.70%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 0.8385 - Accuracy: 63.79%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 0.8365 - Accuracy: 63.87%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 0.8347 - Accuracy: 63.95%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 0.8328 - Accuracy: 64.02%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 0.8310 - Accuracy: 64.09%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 0.8292 - Accuracy: 64.17%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 0.8272 - Accuracy: 64.26%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 0.8253 - Accuracy: 64.35%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 0.8234 - Accuracy: 64.43%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 0.8217 - Accuracy: 64.50%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 0.8197 - Accuracy: 64.59%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 0.8179 - Accuracy: 64.67%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 0.8162 - Accuracy: 64.75%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 0.8143 - Accuracy: 64.84%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 0.8128 - Accuracy: 64.91%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 0.8110 - Accuracy: 64.99%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 0.8094 - Accuracy: 65.06%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 0.8078 - Accuracy: 65.13%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 0.8058 - Accuracy: 65.22%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 0.8041 - Accuracy: 65.30%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 0.8023 - Accuracy: 65.38%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 0.8003 - Accuracy: 65.47%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 0.7985 - Accuracy: 65.54%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 0.7967 - Accuracy: 65.62%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 0.7950 - Accuracy: 65.69%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 0.7932 - Accuracy: 65.77%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 0.7915 - Accuracy: 65.85%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 0.7899 - Accuracy: 65.92%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 0.7882 - Accuracy: 65.99%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 0.7864 - Accuracy: 66.07%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 0.7846 - Accuracy: 66.15%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 0.7831 - Accuracy: 66.22%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 0.7815 - Accuracy: 66.29%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 0.7800 - Accuracy: 66.37%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 0.7783 - Accuracy: 66.44%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 0.7765 - Accuracy: 66.51%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 0.7749 - Accuracy: 66.59%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 0.7735 - Accuracy: 66.65%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 0.7719 - Accuracy: 66.71%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 0.7701 - Accuracy: 66.79%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 0.7685 - Accuracy: 66.85%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 0.7670 - Accuracy: 66.91%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 0.7654 - Accuracy: 66.98%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 0.7639 - Accuracy: 67.04%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 0.7625 - Accuracy: 67.10%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 0.7608 - Accuracy: 67.17%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 0.7590 - Accuracy: 67.25%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 0.7574 - Accuracy: 67.32%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 0.7560 - Accuracy: 67.38%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 0.7544 - Accuracy: 67.46%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 0.7530 - Accuracy: 67.51%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 0.7516 - Accuracy: 67.58%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 0.7501 - Accuracy: 67.64%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 0.7487 - Accuracy: 67.70%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 0.7473 - Accuracy: 67.76%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 0.7460 - Accuracy: 67.82%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 0.7444 - Accuracy: 67.89%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 0.7429 - Accuracy: 67.95%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 0.7415 - Accuracy: 68.01%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 0.7400 - Accuracy: 68.08%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 0.7387 - Accuracy: 68.14%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 0.7371 - Accuracy: 68.21%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 0.7358 - Accuracy: 68.27%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 0.7342 - Accuracy: 68.34%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 0.7329 - Accuracy: 68.40%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 0.7315 - Accuracy: 68.46%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 0.7300 - Accuracy: 68.53%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 0.7288 - Accuracy: 68.58%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 0.7274 - Accuracy: 68.64%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 0.7260 - Accuracy: 68.69%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 0.7247 - Accuracy: 68.75%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 0.7234 - Accuracy: 68.80%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 0.7221 - Accuracy: 68.86%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 0.7210 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 0.7196 - Accuracy: 68.97%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 0.7182 - Accuracy: 69.03%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 0.7168 - Accuracy: 69.09%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 0.7155 - Accuracy: 69.14%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 0.7143 - Accuracy: 69.20%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 0.7132 - Accuracy: 69.25%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 0.7119 - Accuracy: 69.31%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 0.7106 - Accuracy: 69.37%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 0.7092 - Accuracy: 69.43%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 0.7079 - Accuracy: 69.49%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 0.7066 - Accuracy: 69.54%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 0.7052 - Accuracy: 69.61%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 0.7040 - Accuracy: 69.66%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 0.7028 - Accuracy: 69.71%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 0.7015 - Accuracy: 69.76%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 0.7004 - Accuracy: 69.81%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 0.6990 - Accuracy: 69.86%\n",
      "Train loss: 0.6979 - Train accuracy: 69.90%\n",
      "Validation accuracy: 94.3084\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 0.0355 - Accuracy: 100.00%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 0.1059 - Accuracy: 95.45%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 0.1060 - Accuracy: 95.54%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 0.1257 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 0.1249 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 0.1277 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 0.1319 - Accuracy: 94.57%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 0.1328 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 0.1390 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 0.1441 - Accuracy: 93.34%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 0.1464 - Accuracy: 93.38%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 0.1453 - Accuracy: 93.41%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 0.1441 - Accuracy: 93.70%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 0.1449 - Accuracy: 93.61%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 0.1441 - Accuracy: 93.66%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 0.1453 - Accuracy: 93.50%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 0.1455 - Accuracy: 93.56%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 0.1453 - Accuracy: 93.60%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 0.1473 - Accuracy: 93.68%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 0.1428 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 0.1398 - Accuracy: 94.09%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 0.1408 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 0.1382 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 0.1373 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 0.1368 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 0.1353 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 0.1334 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 0.1355 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 0.1353 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 0.1344 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 0.1347 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 0.1347 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 0.1329 - Accuracy: 94.47%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 0.1310 - Accuracy: 94.54%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 0.1323 - Accuracy: 94.50%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 0.1306 - Accuracy: 94.57%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 0.1316 - Accuracy: 94.55%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 0.1322 - Accuracy: 94.56%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 0.1311 - Accuracy: 94.59%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 0.1311 - Accuracy: 94.60%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 0.1301 - Accuracy: 94.64%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 0.1300 - Accuracy: 94.65%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 0.1297 - Accuracy: 94.66%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 0.1306 - Accuracy: 94.59%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 0.1320 - Accuracy: 94.52%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 0.1317 - Accuracy: 94.57%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 0.1332 - Accuracy: 94.50%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 0.1332 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 0.1330 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 0.1324 - Accuracy: 94.51%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 0.1326 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 0.1333 - Accuracy: 94.46%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 0.1325 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 0.1323 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 0.1318 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 0.1331 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 0.1332 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 0.1336 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 0.1332 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 0.1343 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 0.1339 - Accuracy: 94.38%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 0.1329 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 0.1327 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 0.1333 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 0.1332 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 0.1333 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 0.1330 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 0.1331 - Accuracy: 94.44%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 0.1332 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 0.1326 - Accuracy: 94.46%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 0.1323 - Accuracy: 94.49%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 0.1329 - Accuracy: 94.45%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 0.1325 - Accuracy: 94.47%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 0.1338 - Accuracy: 94.43%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 0.1336 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 0.1336 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 0.1346 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 0.1348 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 0.1346 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 0.1340 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 0.1338 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 0.1337 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 0.1340 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 0.1340 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 0.1339 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 0.1338 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 0.1341 - Accuracy: 94.38%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 0.1341 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 0.1343 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 0.1341 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 0.1339 - Accuracy: 94.42%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 0.1341 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 0.1341 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 0.1344 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 0.1349 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 0.1346 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 0.1351 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 0.1347 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 0.1346 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 0.1341 - Accuracy: 94.38%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 0.1343 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 0.1346 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 0.1344 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 0.1346 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 0.1348 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 0.1347 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 0.1347 - Accuracy: 94.36%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 0.1346 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 0.1343 - Accuracy: 94.37%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 0.1337 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 0.1343 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 0.1339 - Accuracy: 94.41%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 0.1340 - Accuracy: 94.40%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 0.1343 - Accuracy: 94.39%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 0.1348 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 0.1352 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 0.1352 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 0.1352 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 0.1351 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 0.1352 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 0.1349 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 0.1349 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 0.1350 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 0.1350 - Accuracy: 94.32%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 0.1348 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 0.1344 - Accuracy: 94.35%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 0.1347 - Accuracy: 94.34%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 0.1348 - Accuracy: 94.33%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 0.1352 - Accuracy: 94.30%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 0.1356 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 0.1352 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 0.1353 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 0.1356 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 0.1358 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 0.1355 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 0.1355 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 0.1352 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 0.1351 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 0.1351 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 0.1349 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 0.1346 - Accuracy: 94.29%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 0.1342 - Accuracy: 94.31%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 0.1348 - Accuracy: 94.27%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 0.1350 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 0.1351 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 0.1351 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 0.1352 - Accuracy: 94.25%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 0.1349 - Accuracy: 94.26%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 0.1352 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 0.1352 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 0.1350 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 0.1349 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 0.1350 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 0.1350 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 0.1349 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 0.1350 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 0.1352 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 0.1355 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 0.1354 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 0.1354 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 0.1352 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 0.1353 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 0.1357 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 0.1355 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 0.1356 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 0.1355 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 0.1353 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 0.1354 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 0.1354 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 0.1353 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 0.1356 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 0.1358 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 0.1357 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 0.1358 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 0.1355 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 0.1356 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 0.1357 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 0.1356 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 0.1355 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 0.1354 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 0.1354 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 0.1356 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 0.1357 - Accuracy: 94.15%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 0.1360 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 0.1359 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 0.1357 - Accuracy: 94.15%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 0.1360 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 0.1360 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 0.1358 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 0.1356 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 0.1357 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 0.1360 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 0.1361 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 0.1359 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 0.1360 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 0.1361 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 0.1361 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 0.1359 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 0.1358 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 0.1360 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 0.1360 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 0.1357 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 0.1359 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 0.1357 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 0.1358 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 0.1356 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 0.1356 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 0.1354 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 0.1356 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 0.1359 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 0.1357 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 0.1357 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 0.1356 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 0.1356 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 0.1357 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 0.1355 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 0.1356 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 0.1357 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 0.1358 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 0.1360 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 0.1358 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 0.1360 - Accuracy: 94.09%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 0.1358 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 0.1356 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 0.1359 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 0.1359 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 0.1361 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 0.1365 - Accuracy: 94.09%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 0.1368 - Accuracy: 94.07%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 0.1368 - Accuracy: 94.06%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 0.1366 - Accuracy: 94.07%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 0.1368 - Accuracy: 94.05%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 0.1371 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 0.1371 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 0.1370 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 0.1371 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 0.1372 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 0.1373 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 0.1370 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 0.1372 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 0.1370 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 0.1368 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 0.1367 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 0.1366 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 0.1367 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 0.1367 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 0.1365 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 0.1365 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 0.1367 - Accuracy: 94.00%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 0.1365 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 0.1366 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 0.1364 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 0.1365 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 0.1365 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 0.1365 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 0.1365 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 0.1364 - Accuracy: 94.04%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 0.1364 - Accuracy: 94.04%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 0.1363 - Accuracy: 94.05%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 0.1360 - Accuracy: 94.06%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 0.1359 - Accuracy: 94.07%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 0.1358 - Accuracy: 94.07%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 0.1359 - Accuracy: 94.07%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 0.1359 - Accuracy: 94.08%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 0.1358 - Accuracy: 94.08%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 0.1357 - Accuracy: 94.08%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 0.1357 - Accuracy: 94.08%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 0.1357 - Accuracy: 94.08%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 0.1354 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 0.1354 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 0.1354 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 0.1355 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 0.1356 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 0.1355 - Accuracy: 94.10%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 0.1354 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 0.1354 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 0.1353 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 0.1352 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 0.1351 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 0.1351 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 0.1351 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 0.1352 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 0.1351 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 0.1350 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 0.1350 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 0.1350 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 0.1350 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 0.1350 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 0.1348 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 0.1348 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 0.1350 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 0.1349 - Accuracy: 94.11%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 0.1348 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 0.1348 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 0.1349 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 0.1347 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 0.1346 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 0.1346 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 0.1344 - Accuracy: 94.15%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 0.1343 - Accuracy: 94.16%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 0.1343 - Accuracy: 94.16%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 0.1341 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 0.1339 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 0.1337 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 0.1333 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 0.1331 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 0.1332 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 0.1331 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 0.1331 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 0.1331 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 0.1332 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 0.1332 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 0.1333 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 0.1334 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 0.1335 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 0.1336 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 0.1333 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 0.1332 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 0.1332 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 0.1334 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 0.1334 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 0.1333 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 0.1336 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 0.1336 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 0.1334 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 0.1335 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 0.1336 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 0.1337 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 0.1338 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 0.1338 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 0.1339 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 0.1338 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 0.1337 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 0.1337 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 0.1337 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 0.1339 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 0.1337 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 0.1336 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 0.1335 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 0.1334 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 0.1332 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 0.1333 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 0.1332 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 0.1332 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 0.1333 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 0.1334 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 0.1334 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 0.1332 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 0.1331 - Accuracy: 94.24%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 0.1333 - Accuracy: 94.23%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 0.1334 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 0.1334 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 0.1334 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 0.1334 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 0.1335 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 0.1335 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 0.1334 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 0.1333 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 0.1334 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 0.1333 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 0.1335 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 0.1335 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 0.1335 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 0.1335 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 0.1333 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 0.1332 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 0.1331 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 0.1331 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 0.1330 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 0.1332 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 0.1331 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 0.1332 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 0.1333 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 0.1334 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 0.1334 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 0.1334 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 0.1335 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 0.1339 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 0.1339 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 0.1337 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 0.1337 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 0.1337 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 0.1338 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 0.1337 - Accuracy: 94.17%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 0.1336 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 0.1336 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 0.1336 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 0.1335 - Accuracy: 94.18%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 0.1332 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 0.1331 - Accuracy: 94.20%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 0.1329 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 0.1328 - Accuracy: 94.21%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 0.1327 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 0.1326 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 0.1327 - Accuracy: 94.22%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 0.1327 - Accuracy: 94.22%\n",
      "Train loss: 0.1327 - Train accuracy: 94.22%\n",
      "Validation accuracy: 94.5260\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.3356%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 69.90%        | 94.31%              |\n",
    "| **Epoch 2** | 94.22%        | 94.53%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** remains nearly constant (~94), with a slight **increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_emotion_model/gru_emotion_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
