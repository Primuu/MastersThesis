{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41942f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        pooled = torch.mean(gru_out, dim=1)\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(self.relu(dropped))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.7457 - Accuracy: 12.50%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 1.8542 - Accuracy: 17.05%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 1.8553 - Accuracy: 18.15%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 1.8513 - Accuracy: 16.94%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.8477 - Accuracy: 17.07%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.8405 - Accuracy: 17.65%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.8396 - Accuracy: 16.80%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.8436 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.8410 - Accuracy: 16.98%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.8455 - Accuracy: 16.76%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.8406 - Accuracy: 17.08%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.8432 - Accuracy: 16.72%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.8414 - Accuracy: 16.63%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.8389 - Accuracy: 16.60%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.8388 - Accuracy: 16.53%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.8365 - Accuracy: 16.60%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.8333 - Accuracy: 16.69%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.8327 - Accuracy: 16.56%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.8328 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.8314 - Accuracy: 16.62%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.8335 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.8324 - Accuracy: 16.44%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.8324 - Accuracy: 16.40%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.8323 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.8336 - Accuracy: 16.08%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.8330 - Accuracy: 16.11%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.8321 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.8319 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.8309 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.8304 - Accuracy: 16.49%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 1.8288 - Accuracy: 16.72%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 1.8286 - Accuracy: 16.66%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 1.8276 - Accuracy: 16.80%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 1.8283 - Accuracy: 16.77%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 1.8278 - Accuracy: 16.83%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 1.8276 - Accuracy: 16.79%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 1.8274 - Accuracy: 16.83%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 1.8267 - Accuracy: 16.86%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 1.8259 - Accuracy: 17.03%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 1.8248 - Accuracy: 17.10%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 1.8228 - Accuracy: 17.38%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 1.8211 - Accuracy: 17.59%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 1.8188 - Accuracy: 17.76%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 1.8146 - Accuracy: 18.04%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 1.8114 - Accuracy: 18.27%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 1.8058 - Accuracy: 18.58%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 1.7984 - Accuracy: 18.91%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 1.7945 - Accuracy: 19.13%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 1.7858 - Accuracy: 19.49%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 1.7797 - Accuracy: 19.77%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 1.7749 - Accuracy: 20.02%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 1.7694 - Accuracy: 20.35%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 1.7643 - Accuracy: 20.55%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 1.7576 - Accuracy: 20.87%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 1.7509 - Accuracy: 21.25%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 1.7461 - Accuracy: 21.51%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 1.7441 - Accuracy: 21.54%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 1.7384 - Accuracy: 21.74%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 1.7306 - Accuracy: 22.00%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 1.7255 - Accuracy: 22.25%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 1.7196 - Accuracy: 22.54%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 1.7143 - Accuracy: 22.79%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 1.7094 - Accuracy: 22.99%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 1.7060 - Accuracy: 23.07%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 1.6989 - Accuracy: 23.45%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 1.6935 - Accuracy: 23.67%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 1.6880 - Accuracy: 23.95%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 1.6829 - Accuracy: 24.18%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 1.6791 - Accuracy: 24.38%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 1.6738 - Accuracy: 24.60%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 1.6677 - Accuracy: 24.97%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 1.6610 - Accuracy: 25.36%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 1.6540 - Accuracy: 25.67%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 1.6492 - Accuracy: 25.92%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 1.6464 - Accuracy: 26.07%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 1.6413 - Accuracy: 26.37%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 1.6361 - Accuracy: 26.71%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 1.6309 - Accuracy: 26.99%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 1.6245 - Accuracy: 27.43%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 1.6178 - Accuracy: 27.83%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 1.6107 - Accuracy: 28.13%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 1.6061 - Accuracy: 28.34%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 1.6015 - Accuracy: 28.56%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 1.5945 - Accuracy: 28.90%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 1.5897 - Accuracy: 29.18%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 1.5845 - Accuracy: 29.44%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 1.5803 - Accuracy: 29.69%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 1.5756 - Accuracy: 29.93%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 1.5711 - Accuracy: 30.11%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 1.5655 - Accuracy: 30.37%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 1.5590 - Accuracy: 30.66%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 1.5542 - Accuracy: 30.93%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 1.5485 - Accuracy: 31.24%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 1.5429 - Accuracy: 31.51%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 1.5375 - Accuracy: 31.78%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 1.5319 - Accuracy: 32.06%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 1.5263 - Accuracy: 32.33%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 1.5206 - Accuracy: 32.61%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 1.5157 - Accuracy: 32.87%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 1.5089 - Accuracy: 33.23%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 1.5041 - Accuracy: 33.46%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 1.4975 - Accuracy: 33.80%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 1.4921 - Accuracy: 34.10%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 1.4872 - Accuracy: 34.32%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 1.4807 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 1.4752 - Accuracy: 35.01%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 1.4688 - Accuracy: 35.34%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 1.4624 - Accuracy: 35.66%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 1.4573 - Accuracy: 35.94%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 1.4510 - Accuracy: 36.28%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 1.4467 - Accuracy: 36.52%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 1.4396 - Accuracy: 36.86%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 1.4336 - Accuracy: 37.16%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 1.4264 - Accuracy: 37.49%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 1.4196 - Accuracy: 37.85%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 1.4126 - Accuracy: 38.21%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 1.4062 - Accuracy: 38.52%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 1.3996 - Accuracy: 38.86%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 1.3939 - Accuracy: 39.17%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 1.3873 - Accuracy: 39.49%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 1.3811 - Accuracy: 39.81%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 1.3754 - Accuracy: 40.10%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 1.3699 - Accuracy: 40.39%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 1.3635 - Accuracy: 40.69%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 1.3572 - Accuracy: 40.99%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 1.3512 - Accuracy: 41.29%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 1.3446 - Accuracy: 41.61%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 1.3383 - Accuracy: 41.95%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 1.3328 - Accuracy: 42.23%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 1.3272 - Accuracy: 42.50%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 1.3211 - Accuracy: 42.78%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 1.3147 - Accuracy: 43.09%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 1.3093 - Accuracy: 43.38%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 1.3037 - Accuracy: 43.67%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 1.2975 - Accuracy: 43.98%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 1.2919 - Accuracy: 44.27%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 1.2867 - Accuracy: 44.52%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 1.2803 - Accuracy: 44.83%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 1.2744 - Accuracy: 45.13%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 1.2683 - Accuracy: 45.40%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 1.2629 - Accuracy: 45.66%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 1.2583 - Accuracy: 45.89%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 1.2533 - Accuracy: 46.16%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 1.2488 - Accuracy: 46.41%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 1.2442 - Accuracy: 46.65%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 1.2393 - Accuracy: 46.92%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 1.2344 - Accuracy: 47.15%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 1.2287 - Accuracy: 47.43%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 1.2234 - Accuracy: 47.69%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 1.2186 - Accuracy: 47.91%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 1.2133 - Accuracy: 48.17%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 1.2083 - Accuracy: 48.40%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 1.2036 - Accuracy: 48.63%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 1.1981 - Accuracy: 48.88%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 1.1927 - Accuracy: 49.12%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 1.1879 - Accuracy: 49.34%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 1.1825 - Accuracy: 49.59%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 1.1779 - Accuracy: 49.81%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 1.1734 - Accuracy: 50.04%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 1.1693 - Accuracy: 50.22%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 1.1646 - Accuracy: 50.43%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 1.1602 - Accuracy: 50.66%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 1.1553 - Accuracy: 50.89%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 1.1503 - Accuracy: 51.12%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 1.1457 - Accuracy: 51.32%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 1.1408 - Accuracy: 51.55%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 1.1368 - Accuracy: 51.75%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 1.1323 - Accuracy: 51.96%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 1.1276 - Accuracy: 52.18%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 1.1232 - Accuracy: 52.38%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 1.1187 - Accuracy: 52.59%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 1.1142 - Accuracy: 52.79%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 1.1098 - Accuracy: 52.98%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 1.1058 - Accuracy: 53.17%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 1.1019 - Accuracy: 53.35%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 1.0973 - Accuracy: 53.55%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 1.0930 - Accuracy: 53.74%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 1.0889 - Accuracy: 53.93%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 1.0850 - Accuracy: 54.12%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 1.0805 - Accuracy: 54.32%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 1.0766 - Accuracy: 54.49%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 1.0725 - Accuracy: 54.69%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 1.0682 - Accuracy: 54.89%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 1.0640 - Accuracy: 55.08%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 1.0591 - Accuracy: 55.31%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 1.0549 - Accuracy: 55.49%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 1.0509 - Accuracy: 55.67%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 1.0466 - Accuracy: 55.86%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 1.0423 - Accuracy: 56.05%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 1.0383 - Accuracy: 56.24%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 1.0347 - Accuracy: 56.41%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 1.0312 - Accuracy: 56.58%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 1.0270 - Accuracy: 56.77%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 1.0231 - Accuracy: 56.94%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 1.0195 - Accuracy: 57.11%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 1.0154 - Accuracy: 57.29%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 1.0116 - Accuracy: 57.46%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 1.0078 - Accuracy: 57.62%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 1.0046 - Accuracy: 57.78%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 1.0007 - Accuracy: 57.96%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 0.9975 - Accuracy: 58.12%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 0.9937 - Accuracy: 58.29%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 0.9904 - Accuracy: 58.44%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 0.9868 - Accuracy: 58.60%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 0.9835 - Accuracy: 58.76%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 0.9800 - Accuracy: 58.92%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 0.9769 - Accuracy: 59.08%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 0.9738 - Accuracy: 59.22%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 0.9701 - Accuracy: 59.37%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 0.9667 - Accuracy: 59.53%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 0.9635 - Accuracy: 59.67%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 0.9605 - Accuracy: 59.81%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 0.9572 - Accuracy: 59.96%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 0.9539 - Accuracy: 60.12%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 0.9510 - Accuracy: 60.26%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 0.9480 - Accuracy: 60.39%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 0.9450 - Accuracy: 60.52%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 0.9416 - Accuracy: 60.67%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 0.9388 - Accuracy: 60.80%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 0.9360 - Accuracy: 60.93%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 0.9336 - Accuracy: 61.05%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 0.9307 - Accuracy: 61.19%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 0.9274 - Accuracy: 61.34%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 0.9243 - Accuracy: 61.48%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 0.9215 - Accuracy: 61.61%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 0.9183 - Accuracy: 61.75%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 0.9157 - Accuracy: 61.87%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 0.9125 - Accuracy: 62.01%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 0.9095 - Accuracy: 62.14%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 0.9066 - Accuracy: 62.27%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 0.9040 - Accuracy: 62.39%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 0.9011 - Accuracy: 62.53%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 0.8984 - Accuracy: 62.66%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 0.8956 - Accuracy: 62.78%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 0.8928 - Accuracy: 62.90%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 0.8898 - Accuracy: 63.04%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 0.8869 - Accuracy: 63.16%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 0.8841 - Accuracy: 63.29%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 0.8815 - Accuracy: 63.39%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 0.8786 - Accuracy: 63.52%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 0.8758 - Accuracy: 63.63%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 0.8732 - Accuracy: 63.75%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 0.8706 - Accuracy: 63.88%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 0.8679 - Accuracy: 64.00%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 0.8656 - Accuracy: 64.09%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 0.8627 - Accuracy: 64.22%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 0.8600 - Accuracy: 64.33%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 0.8572 - Accuracy: 64.45%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 0.8545 - Accuracy: 64.57%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 0.8519 - Accuracy: 64.69%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 0.8493 - Accuracy: 64.80%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 0.8470 - Accuracy: 64.90%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 0.8446 - Accuracy: 65.02%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 0.8424 - Accuracy: 65.12%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 0.8400 - Accuracy: 65.22%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 0.8377 - Accuracy: 65.32%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 0.8353 - Accuracy: 65.42%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 0.8328 - Accuracy: 65.53%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 0.8307 - Accuracy: 65.63%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 0.8283 - Accuracy: 65.73%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 0.8257 - Accuracy: 65.84%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 0.8235 - Accuracy: 65.93%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 0.8212 - Accuracy: 66.04%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 0.8188 - Accuracy: 66.14%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 0.8164 - Accuracy: 66.24%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 0.8142 - Accuracy: 66.33%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 0.8120 - Accuracy: 66.42%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 0.8104 - Accuracy: 66.49%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 0.8080 - Accuracy: 66.60%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 0.8058 - Accuracy: 66.70%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 0.8034 - Accuracy: 66.80%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 0.8013 - Accuracy: 66.89%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 0.7991 - Accuracy: 66.99%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 0.7970 - Accuracy: 67.08%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 0.7949 - Accuracy: 67.18%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 0.7930 - Accuracy: 67.27%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 0.7906 - Accuracy: 67.37%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 0.7885 - Accuracy: 67.47%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 0.7867 - Accuracy: 67.55%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 0.7848 - Accuracy: 67.64%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 0.7826 - Accuracy: 67.74%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 0.7808 - Accuracy: 67.82%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 0.7788 - Accuracy: 67.90%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 0.7769 - Accuracy: 67.98%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 0.7751 - Accuracy: 68.06%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 0.7731 - Accuracy: 68.14%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 0.7715 - Accuracy: 68.21%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 0.7697 - Accuracy: 68.30%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 0.7679 - Accuracy: 68.38%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 0.7659 - Accuracy: 68.46%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 0.7638 - Accuracy: 68.55%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 0.7619 - Accuracy: 68.63%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 0.7596 - Accuracy: 68.73%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 0.7582 - Accuracy: 68.80%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 0.7561 - Accuracy: 68.89%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 0.7542 - Accuracy: 68.96%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 0.7522 - Accuracy: 69.05%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 0.7502 - Accuracy: 69.15%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 0.7483 - Accuracy: 69.23%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 0.7466 - Accuracy: 69.30%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 0.7447 - Accuracy: 69.38%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 0.7428 - Accuracy: 69.46%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 0.7410 - Accuracy: 69.53%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 0.7393 - Accuracy: 69.60%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 0.7375 - Accuracy: 69.68%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 0.7356 - Accuracy: 69.76%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 0.7339 - Accuracy: 69.84%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 0.7322 - Accuracy: 69.92%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 0.7304 - Accuracy: 69.99%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 0.7286 - Accuracy: 70.07%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 0.7271 - Accuracy: 70.13%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 0.7256 - Accuracy: 70.21%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 0.7237 - Accuracy: 70.28%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 0.7221 - Accuracy: 70.35%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 0.7204 - Accuracy: 70.42%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 0.7187 - Accuracy: 70.51%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 0.7172 - Accuracy: 70.57%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 0.7154 - Accuracy: 70.65%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 0.7140 - Accuracy: 70.71%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 0.7126 - Accuracy: 70.78%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 0.7112 - Accuracy: 70.83%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 0.7095 - Accuracy: 70.90%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 0.7079 - Accuracy: 70.97%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 0.7062 - Accuracy: 71.04%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 0.7046 - Accuracy: 71.11%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 0.7031 - Accuracy: 71.17%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 0.7018 - Accuracy: 71.23%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 0.7002 - Accuracy: 71.29%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 0.6989 - Accuracy: 71.35%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 0.6973 - Accuracy: 71.42%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 0.6957 - Accuracy: 71.48%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 0.6945 - Accuracy: 71.53%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 0.6929 - Accuracy: 71.59%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 0.6915 - Accuracy: 71.65%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 0.6900 - Accuracy: 71.71%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 0.6883 - Accuracy: 71.78%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 0.6869 - Accuracy: 71.83%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 0.6853 - Accuracy: 71.90%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 0.6837 - Accuracy: 71.97%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 0.6823 - Accuracy: 72.03%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 0.6809 - Accuracy: 72.08%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 0.6797 - Accuracy: 72.13%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 0.6786 - Accuracy: 72.19%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 0.6770 - Accuracy: 72.26%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 0.6757 - Accuracy: 72.31%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 0.6744 - Accuracy: 72.36%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 0.6729 - Accuracy: 72.43%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 0.6716 - Accuracy: 72.48%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 0.6701 - Accuracy: 72.55%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 0.6688 - Accuracy: 72.60%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 0.6674 - Accuracy: 72.66%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 0.6660 - Accuracy: 72.72%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 0.6646 - Accuracy: 72.79%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 0.6631 - Accuracy: 72.85%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 0.6618 - Accuracy: 72.90%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 0.6603 - Accuracy: 72.96%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 0.6589 - Accuracy: 73.02%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 0.6575 - Accuracy: 73.08%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 0.6561 - Accuracy: 73.14%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 0.6551 - Accuracy: 73.19%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 0.6538 - Accuracy: 73.25%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 0.6523 - Accuracy: 73.31%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 0.6511 - Accuracy: 73.37%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 0.6498 - Accuracy: 73.43%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 0.6483 - Accuracy: 73.48%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 0.6470 - Accuracy: 73.53%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 0.6458 - Accuracy: 73.59%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 0.6445 - Accuracy: 73.64%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 0.6435 - Accuracy: 73.68%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 0.6424 - Accuracy: 73.72%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 0.6413 - Accuracy: 73.77%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 0.6405 - Accuracy: 73.81%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 0.6396 - Accuracy: 73.86%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 0.6382 - Accuracy: 73.92%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 0.6372 - Accuracy: 73.96%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 0.6361 - Accuracy: 74.00%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 0.6349 - Accuracy: 74.06%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 0.6338 - Accuracy: 74.10%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 0.6327 - Accuracy: 74.15%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 0.6314 - Accuracy: 74.20%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 0.6302 - Accuracy: 74.25%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 0.6291 - Accuracy: 74.30%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 0.6280 - Accuracy: 74.35%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 0.6267 - Accuracy: 74.40%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 0.6256 - Accuracy: 74.45%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 0.6245 - Accuracy: 74.49%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 0.6234 - Accuracy: 74.53%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 0.6221 - Accuracy: 74.58%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 0.6210 - Accuracy: 74.63%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 0.6198 - Accuracy: 74.68%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 0.6187 - Accuracy: 74.73%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 0.6177 - Accuracy: 74.76%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 0.6165 - Accuracy: 74.82%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 0.6154 - Accuracy: 74.86%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 0.6144 - Accuracy: 74.90%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 0.6133 - Accuracy: 74.95%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 0.6122 - Accuracy: 75.00%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 0.6110 - Accuracy: 75.05%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 0.6101 - Accuracy: 75.10%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 0.6090 - Accuracy: 75.15%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 0.6078 - Accuracy: 75.20%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 0.6071 - Accuracy: 75.23%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 0.6060 - Accuracy: 75.27%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 0.6053 - Accuracy: 75.31%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 0.6044 - Accuracy: 75.35%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 0.6034 - Accuracy: 75.40%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 0.6023 - Accuracy: 75.43%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 0.6013 - Accuracy: 75.48%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 0.6001 - Accuracy: 75.52%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 0.5992 - Accuracy: 75.55%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 0.5981 - Accuracy: 75.60%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 0.5971 - Accuracy: 75.64%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 0.5961 - Accuracy: 75.68%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 0.5951 - Accuracy: 75.72%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 0.5940 - Accuracy: 75.76%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 0.5928 - Accuracy: 75.81%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 0.5918 - Accuracy: 75.85%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 0.5908 - Accuracy: 75.89%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 0.5899 - Accuracy: 75.92%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 0.5890 - Accuracy: 75.96%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 0.5881 - Accuracy: 76.00%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 0.5872 - Accuracy: 76.04%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 0.5862 - Accuracy: 76.08%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 0.5852 - Accuracy: 76.12%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 0.5842 - Accuracy: 76.16%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 0.5832 - Accuracy: 76.20%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 0.5821 - Accuracy: 76.25%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 0.5813 - Accuracy: 76.28%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 0.5804 - Accuracy: 76.32%\n",
      "Train loss: 0.5799 - Train accuracy: 76.35%\n",
      "Validation accuracy: 94.2200\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 0.0968 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 0.1588 - Accuracy: 92.61%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 0.1778 - Accuracy: 92.26%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 0.1866 - Accuracy: 92.14%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 0.1790 - Accuracy: 92.53%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 0.1779 - Accuracy: 92.52%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 0.1741 - Accuracy: 92.83%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 0.1636 - Accuracy: 93.49%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 0.1611 - Accuracy: 93.44%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 0.1570 - Accuracy: 93.68%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 0.1615 - Accuracy: 93.50%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 0.1598 - Accuracy: 93.52%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 0.1574 - Accuracy: 93.65%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 0.1561 - Accuracy: 93.56%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 0.1520 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 0.1534 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 0.1485 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 0.1477 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 0.1481 - Accuracy: 94.13%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 0.1466 - Accuracy: 94.14%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 0.1450 - Accuracy: 94.28%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 0.1479 - Accuracy: 94.19%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 0.1499 - Accuracy: 94.12%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 0.1489 - Accuracy: 94.16%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 0.1510 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 0.1525 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 0.1529 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 0.1537 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 0.1531 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 0.1547 - Accuracy: 93.69%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 0.1530 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 0.1513 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 0.1509 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 0.1497 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 0.1506 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 0.1529 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 0.1548 - Accuracy: 93.72%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 0.1526 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 0.1536 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 0.1543 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 0.1535 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 0.1524 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 0.1505 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 0.1511 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 0.1515 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 0.1510 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 0.1523 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 0.1529 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 0.1538 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 0.1541 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 0.1554 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 0.1566 - Accuracy: 93.74%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 0.1566 - Accuracy: 93.74%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 0.1558 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 0.1548 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 0.1535 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 0.1531 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 0.1522 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 0.1523 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 0.1519 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 0.1521 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 0.1521 - Accuracy: 94.04%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 0.1521 - Accuracy: 94.04%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 0.1525 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 0.1528 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 0.1520 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 0.1525 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 0.1525 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 0.1519 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 0.1526 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 0.1526 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 0.1530 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 0.1537 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 0.1533 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 0.1534 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 0.1528 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 0.1526 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 0.1524 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 0.1515 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 0.1518 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 0.1512 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 0.1506 - Accuracy: 94.03%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 0.1508 - Accuracy: 94.02%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 0.1510 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 0.1509 - Accuracy: 94.01%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 0.1514 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 0.1512 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 0.1510 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 0.1510 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 0.1512 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 0.1514 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 0.1518 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 0.1524 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 0.1518 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 0.1520 - Accuracy: 93.96%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 0.1517 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 0.1517 - Accuracy: 93.96%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 0.1514 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 0.1513 - Accuracy: 93.97%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 0.1508 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 0.1508 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 0.1509 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 0.1504 - Accuracy: 93.99%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 0.1510 - Accuracy: 93.98%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 0.1516 - Accuracy: 93.96%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 0.1524 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 0.1524 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 0.1528 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 0.1529 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 0.1533 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 0.1534 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 0.1530 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 0.1531 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 0.1535 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 0.1538 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 0.1543 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 0.1545 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 0.1540 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 0.1543 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 0.1552 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 0.1547 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 0.1544 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 0.1544 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 0.1539 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 0.1543 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 0.1542 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 0.1541 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 0.1545 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 0.1543 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 0.1542 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 0.1539 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 0.1536 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 0.1535 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 0.1540 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 0.1539 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 0.1539 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 0.1538 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 0.1536 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 0.1539 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 0.1537 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 0.1538 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 0.1541 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 0.1550 - Accuracy: 93.76%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 0.1550 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 0.1546 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 0.1543 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 0.1541 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 0.1541 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 0.1542 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 0.1540 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 0.1538 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 0.1535 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 0.1537 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 0.1537 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 0.1540 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 0.1544 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 0.1545 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 0.1546 - Accuracy: 93.76%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 0.1544 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 0.1541 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 0.1542 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 0.1542 - Accuracy: 93.78%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 0.1545 - Accuracy: 93.77%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 0.1542 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 0.1541 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 0.1538 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 0.1539 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 0.1542 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 0.1538 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 0.1541 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 0.1543 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 0.1547 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 0.1547 - Accuracy: 93.79%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 0.1544 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 0.1544 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 0.1542 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 0.1544 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 0.1538 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 0.1533 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 0.1538 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 0.1537 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 0.1535 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 0.1536 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 0.1535 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 0.1532 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 0.1532 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 0.1534 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 0.1532 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 0.1534 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 0.1536 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 0.1536 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 0.1535 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 0.1533 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 0.1532 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 0.1530 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 0.1531 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 0.1534 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 0.1536 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 0.1537 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 0.1536 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 0.1539 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 0.1538 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 0.1538 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 0.1539 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 0.1540 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 0.1542 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 0.1541 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 0.1541 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 0.1540 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 0.1538 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 0.1536 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 0.1535 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 0.1535 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 0.1532 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 0.1535 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 0.1533 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 0.1538 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 0.1542 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 0.1539 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 0.1540 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 0.1537 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 0.1535 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 0.1536 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 0.1535 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 0.1533 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 0.1536 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 0.1533 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 0.1531 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 0.1532 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 0.1530 - Accuracy: 93.95%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 0.1530 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 0.1530 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 0.1533 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 0.1532 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 0.1533 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 0.1532 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 0.1532 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 0.1530 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 0.1531 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 0.1530 - Accuracy: 93.94%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 0.1532 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 0.1531 - Accuracy: 93.93%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 0.1532 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 0.1532 - Accuracy: 93.92%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 0.1534 - Accuracy: 93.91%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 0.1536 - Accuracy: 93.90%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 0.1536 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 0.1537 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 0.1537 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 0.1536 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 0.1536 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 0.1534 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 0.1534 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 0.1534 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 0.1533 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 0.1535 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 0.1535 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 0.1536 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 0.1535 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 0.1534 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 0.1533 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 0.1531 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 0.1529 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 0.1528 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 0.1529 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 0.1528 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 0.1530 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 0.1529 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 0.1530 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 0.1532 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 0.1531 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 0.1528 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 0.1527 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 0.1526 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 0.1525 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 0.1526 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 0.1525 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 0.1526 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 0.1528 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 0.1527 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 0.1528 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 0.1529 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 0.1528 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 0.1528 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 0.1530 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 0.1531 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 0.1531 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 0.1533 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 0.1532 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 0.1532 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 0.1532 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 0.1529 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 0.1529 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 0.1531 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 0.1533 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 0.1532 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 0.1530 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 0.1530 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 0.1531 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 0.1531 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 0.1529 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 0.1528 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 0.1527 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 0.1527 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 0.1526 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 0.1525 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 0.1526 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 0.1526 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 0.1527 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 0.1528 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 0.1530 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 0.1530 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 0.1531 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 0.1529 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 0.1528 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 0.1527 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 0.1527 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 0.1524 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 0.1523 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 0.1523 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 0.1521 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 0.1520 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 0.1519 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 0.1520 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 0.1520 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 0.1519 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 0.1519 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 0.1520 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 0.1519 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 0.1520 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 0.1519 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 0.1519 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 0.1518 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 0.1517 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 0.1516 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 0.1517 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 0.1516 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 0.1516 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 0.1515 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 0.1515 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 0.1517 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 0.1515 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 0.1515 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 0.1514 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 0.1513 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 0.1512 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 0.1512 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 0.1512 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 0.1511 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 0.1511 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 0.1510 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 0.1510 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 0.1512 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 0.1514 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 0.1515 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 0.1515 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 0.1514 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 0.1515 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 0.1515 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 0.1515 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 0.1517 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 0.1517 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 0.1517 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 0.1518 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 0.1517 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 0.1520 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 0.1519 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 0.1520 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 0.1519 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 0.1519 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 0.1521 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 0.1521 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 0.1519 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 0.1520 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 0.1523 - Accuracy: 93.80%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 0.1522 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 0.1520 - Accuracy: 93.81%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 0.1519 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 0.1518 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 0.1517 - Accuracy: 93.82%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 0.1516 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 0.1515 - Accuracy: 93.83%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 0.1515 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 0.1514 - Accuracy: 93.84%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 0.1513 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 0.1512 - Accuracy: 93.85%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 0.1511 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 0.1511 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 0.1509 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 0.1509 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 0.1510 - Accuracy: 93.86%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 0.1509 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 0.1508 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 0.1507 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 0.1508 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 0.1505 - Accuracy: 93.89%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 0.1505 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 0.1505 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 0.1507 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 0.1506 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 0.1506 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 0.1506 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 0.1506 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 0.1506 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 0.1505 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 0.1504 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 0.1504 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 0.1503 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 0.1503 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 0.1502 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 0.1504 - Accuracy: 93.88%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 0.1505 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 0.1504 - Accuracy: 93.87%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 0.1503 - Accuracy: 93.87%\n",
      "Train loss: 0.1503 - Train accuracy: 93.87%\n",
      "Validation accuracy: 94.3288\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.0092%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 76.35%        | 94.22%              |\n",
    "| **Epoch 2** | 93.87%        | 94.33%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** remains nearly constant (~94), with a slight **increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_emotion_model/gru_emotion_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
