{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: GRU (multitasking)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13933c462f86673",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file_sentiment = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file_sentiment = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file_sentiment = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file_sentiment), os.path.exists(val_file_sentiment), os.path.exists(test_file_sentiment)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data_sentiment, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data_sentiment, test_data_sentiment = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data_sentiment.to_csv(train_file_sentiment, index=False)\n",
    "    val_data_sentiment.to_csv(val_file_sentiment, index=False)\n",
    "    test_data_sentiment.to_csv(test_file_sentiment, index=False)\n",
    "else:\n",
    "    train_data_sentiment = pd.read_csv(train_file_sentiment)\n",
    "    val_data_sentiment = pd.read_csv(val_file_sentiment)\n",
    "    test_data_sentiment = pd.read_csv(test_file_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820b3c03ae6c2296",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file_emotion = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file_emotion = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file_emotion = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file_emotion), os.path.exists(val_file_emotion), os.path.exists(test_file_emotion)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data_emotion, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data_emotion, test_data_emotion = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data_emotion.to_csv(train_file_emotion, index=False)\n",
    "    val_data_emotion.to_csv(val_file_emotion, index=False)\n",
    "    test_data_emotion.to_csv(test_file_emotion, index=False)\n",
    "else:\n",
    "    train_data_emotion = pd.read_csv(train_file_emotion)\n",
    "    val_data_emotion = pd.read_csv(val_file_emotion)\n",
    "    test_data_emotion = pd.read_csv(test_file_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data_sentiment['text'].tolist() + train_data_emotion['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_sentiment = encode_texts(train_data_sentiment['text'])\n",
    "X_val_sentiment = encode_texts(val_data_sentiment['text'])\n",
    "X_test_sentiment = encode_texts(test_data_sentiment['text'])\n",
    "\n",
    "y_train_sentiment = train_data_sentiment['label'].values\n",
    "y_val_sentiment = val_data_sentiment['label'].values\n",
    "y_test_sentiment = test_data_sentiment['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8374315175a0bf2b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_emotion = encode_texts(train_data_emotion['text'])\n",
    "X_val_emotion = encode_texts(val_data_emotion['text'])\n",
    "X_test_emotion = encode_texts(test_data_emotion['text'])\n",
    "\n",
    "y_train_emotion = train_data_emotion['label'].values\n",
    "y_val_emotion = val_data_emotion['label'].values\n",
    "y_test_emotion = test_data_emotion['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9c0843e7e4aac5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights_sentiment = torch.tensor(compute_class_weight('balanced', classes=np.unique(y_train_sentiment), y=y_train_sentiment), dtype=torch.float)\n",
    "class_weights_emotion = torch.tensor(compute_class_weight('balanced', classes=np.unique(y_train_emotion), y=y_train_emotion), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, X_sent, y_sent, X_emot, y_emot):\n",
    "        self.X_sent = torch.tensor(X_sent, dtype=torch.long)\n",
    "        self.y_sent = torch.tensor(y_sent, dtype=torch.long)\n",
    "        self.X_emot = torch.tensor(X_emot, dtype=torch.long)\n",
    "        self.y_emot = torch.tensor(y_emot, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.y_sent), len(self.y_emot))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids_sent': self.X_sent[idx],\n",
    "            'label_sent': self.y_sent[idx],\n",
    "            'input_ids_emot': self.X_emot[idx],\n",
    "            'label_emot': self.y_emot[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MultiTaskDataset(X_train_sentiment, y_train_sentiment, X_train_emotion, y_train_emotion)\n",
    "val_dataset = MultiTaskDataset(X_val_sentiment, y_val_sentiment, X_val_emotion, y_val_emotion)\n",
    "test_dataset = MultiTaskDataset(X_test_sentiment, y_test_sentiment, X_test_emotion, y_test_emotion)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a5a3df2e3ce62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiTaskGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels_sentiment, num_labels_emotion):\n",
    "        super(MultiTaskGRU, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc_sentiment = nn.Linear(hidden_dim * 2, num_labels_sentiment)\n",
    "        self.fc_emotion = nn.Linear(hidden_dim * 2, num_labels_emotion)\n",
    "    \n",
    "    def forward(self, input_ids_sent, input_ids_emot):\n",
    "        embedded_sent = self.embedding(input_ids_sent)\n",
    "        embedded_emot = self.embedding(input_ids_emot)\n",
    "\n",
    "        gru_out_sent, _ = self.gru(embedded_sent)\n",
    "        gru_out_emot, _ = self.gru(embedded_emot)\n",
    "\n",
    "        pooled_sent = torch.mean(gru_out_sent, dim=1)\n",
    "        pooled_emot = torch.mean(gru_out_emot, dim=1)\n",
    "\n",
    "        dropped_sent = self.dropout(pooled_sent)\n",
    "        dropped_emot = self.dropout(pooled_emot)\n",
    "\n",
    "        out_sentiment = self.fc_sentiment(self.relu(dropped_sent))\n",
    "        out_emotion = self.fc_emotion(self.relu(dropped_emot))\n",
    "\n",
    "        return out_sentiment, out_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultiTaskGRU(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_labels_sentiment=3, num_labels_emotion=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn_sentiment = nn.CrossEntropyLoss(weight=class_weights_sentiment.to(device))\n",
    "loss_fn_emotion = nn.CrossEntropyLoss(weight=class_weights_emotion.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn_sent, loss_fn_emot, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_sentiment = 0\n",
    "    correct_emotion = 0\n",
    "    total_sentiment_samples = 0\n",
    "    total_emotion_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids_sent = batch['input_ids_sent'].to(device)\n",
    "        labels_sent = batch['label_sent'].to(device)\n",
    "        input_ids_emot = batch['input_ids_emot'].to(device)\n",
    "        labels_emot = batch['label_emot'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_sent, logits_emot = model(input_ids_sent, input_ids_emot)\n",
    "\n",
    "        loss_sent = loss_fn_sent(logits_sent, labels_sent)\n",
    "        loss_emot = loss_fn_emot(logits_emot, labels_emot)\n",
    "\n",
    "        loss = loss_sent + loss_emot\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, preds_sent = torch.max(logits_sent, dim=1)\n",
    "        _, preds_emot = torch.max(logits_emot, dim=1)\n",
    "\n",
    "        correct_sentiment += (preds_sent == labels_sent).sum().item()\n",
    "        correct_emotion += (preds_emot == labels_emot).sum().item()\n",
    "        total_sentiment_samples += labels_sent.size(0)\n",
    "        total_emotion_samples += labels_emot.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            acc_sent = 100. * correct_sentiment / total_sentiment_samples\n",
    "            acc_emot = 100. * correct_emotion / total_emotion_samples\n",
    "            print(f\"Epoch {epoch}. Batch {batch_idx}/{len(data_loader)}: \"\n",
    "                  f\"AvgLoss: {avg_loss:.2f}, S.Acc: {acc_sent:.2f}%, E.Acc: {acc_emot:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy_sentiment = 100. * correct_sentiment / total_sentiment_samples\n",
    "    accuracy_emotion = 100. * correct_emotion / total_emotion_samples\n",
    "    return avg_loss, accuracy_sentiment, accuracy_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_sentiment = 0\n",
    "    correct_emotion = 0\n",
    "    total_sentiment_samples = 0\n",
    "    total_emotion_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids_sent = batch['input_ids_sent'].to(device)\n",
    "            labels_sent = batch['label_sent'].to(device)\n",
    "            input_ids_emot = batch['input_ids_emot'].to(device)\n",
    "            labels_emot = batch['label_emot'].to(device)\n",
    "\n",
    "            logits_sent, logits_emot = model(input_ids_sent, input_ids_emot)\n",
    "\n",
    "            _, preds_sent = torch.max(logits_sent, dim=1)\n",
    "            _, preds_emot = torch.max(logits_emot, dim=1)\n",
    "\n",
    "            correct_sentiment += (preds_sent == labels_sent).sum().item()\n",
    "            correct_emotion += (preds_emot == labels_emot).sum().item()\n",
    "            total_sentiment_samples += labels_sent.size(0)\n",
    "            total_emotion_samples += labels_emot.size(0)\n",
    "\n",
    "    accuracy_sentiment = 100. * correct_sentiment / total_sentiment_samples\n",
    "    accuracy_emotion = 100. * correct_emotion / total_emotion_samples\n",
    "    return accuracy_sentiment, accuracy_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8dfbc7755b916d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Batch 0/4290: AvgLoss: 2.99, S.Acc: 43.75%, E.Acc: 18.75%\n",
      "Epoch 0. Batch 10/4290: AvgLoss: 2.99, S.Acc: 42.61%, E.Acc: 14.77%\n",
      "Epoch 0. Batch 20/4290: AvgLoss: 2.97, S.Acc: 39.88%, E.Acc: 16.96%\n",
      "Epoch 0. Batch 30/4290: AvgLoss: 2.97, S.Acc: 37.30%, E.Acc: 18.95%\n",
      "Epoch 0. Batch 40/4290: AvgLoss: 2.98, S.Acc: 36.28%, E.Acc: 17.68%\n",
      "Epoch 0. Batch 50/4290: AvgLoss: 2.98, S.Acc: 35.78%, E.Acc: 16.67%\n",
      "Epoch 0. Batch 60/4290: AvgLoss: 2.99, S.Acc: 34.84%, E.Acc: 16.29%\n",
      "Epoch 0. Batch 70/4290: AvgLoss: 2.98, S.Acc: 35.48%, E.Acc: 16.64%\n",
      "Epoch 0. Batch 80/4290: AvgLoss: 2.98, S.Acc: 35.42%, E.Acc: 16.36%\n",
      "Epoch 0. Batch 90/4290: AvgLoss: 2.97, S.Acc: 35.99%, E.Acc: 16.90%\n",
      "Epoch 0. Batch 100/4290: AvgLoss: 2.97, S.Acc: 35.83%, E.Acc: 16.65%\n",
      "Epoch 0. Batch 110/4290: AvgLoss: 2.97, S.Acc: 35.02%, E.Acc: 16.72%\n",
      "Epoch 0. Batch 120/4290: AvgLoss: 2.97, S.Acc: 35.38%, E.Acc: 16.89%\n",
      "Epoch 0. Batch 130/4290: AvgLoss: 2.97, S.Acc: 34.92%, E.Acc: 16.79%\n",
      "Epoch 0. Batch 140/4290: AvgLoss: 2.97, S.Acc: 34.26%, E.Acc: 16.58%\n",
      "Epoch 0. Batch 150/4290: AvgLoss: 2.97, S.Acc: 34.44%, E.Acc: 16.68%\n",
      "Epoch 0. Batch 160/4290: AvgLoss: 2.96, S.Acc: 34.36%, E.Acc: 16.89%\n",
      "Epoch 0. Batch 170/4290: AvgLoss: 2.96, S.Acc: 34.47%, E.Acc: 16.81%\n",
      "Epoch 0. Batch 180/4290: AvgLoss: 2.96, S.Acc: 34.53%, E.Acc: 16.89%\n",
      "Epoch 0. Batch 190/4290: AvgLoss: 2.96, S.Acc: 33.97%, E.Acc: 16.82%\n",
      "Epoch 0. Batch 200/4290: AvgLoss: 2.96, S.Acc: 33.83%, E.Acc: 16.73%\n",
      "Epoch 0. Batch 210/4290: AvgLoss: 2.96, S.Acc: 33.92%, E.Acc: 16.88%\n",
      "Epoch 0. Batch 220/4290: AvgLoss: 2.96, S.Acc: 34.33%, E.Acc: 16.94%\n",
      "Epoch 0. Batch 230/4290: AvgLoss: 2.95, S.Acc: 34.42%, E.Acc: 16.96%\n",
      "Epoch 0. Batch 240/4290: AvgLoss: 2.95, S.Acc: 34.49%, E.Acc: 16.91%\n",
      "Epoch 0. Batch 250/4290: AvgLoss: 2.95, S.Acc: 34.44%, E.Acc: 17.06%\n",
      "Epoch 0. Batch 260/4290: AvgLoss: 2.95, S.Acc: 34.55%, E.Acc: 16.88%\n",
      "Epoch 0. Batch 270/4290: AvgLoss: 2.95, S.Acc: 34.50%, E.Acc: 17.00%\n",
      "Epoch 0. Batch 280/4290: AvgLoss: 2.95, S.Acc: 34.63%, E.Acc: 17.17%\n",
      "Epoch 0. Batch 290/4290: AvgLoss: 2.95, S.Acc: 34.66%, E.Acc: 17.03%\n",
      "Epoch 0. Batch 300/4290: AvgLoss: 2.95, S.Acc: 34.93%, E.Acc: 17.17%\n",
      "Epoch 0. Batch 310/4290: AvgLoss: 2.95, S.Acc: 34.99%, E.Acc: 17.10%\n",
      "Epoch 0. Batch 320/4290: AvgLoss: 2.95, S.Acc: 35.05%, E.Acc: 16.94%\n",
      "Epoch 0. Batch 330/4290: AvgLoss: 2.94, S.Acc: 34.86%, E.Acc: 17.01%\n",
      "Epoch 0. Batch 340/4290: AvgLoss: 2.95, S.Acc: 34.59%, E.Acc: 16.99%\n",
      "Epoch 0. Batch 350/4290: AvgLoss: 2.95, S.Acc: 34.83%, E.Acc: 16.97%\n",
      "Epoch 0. Batch 360/4290: AvgLoss: 2.95, S.Acc: 34.82%, E.Acc: 16.91%\n",
      "Epoch 0. Batch 370/4290: AvgLoss: 2.95, S.Acc: 34.60%, E.Acc: 16.91%\n",
      "Epoch 0. Batch 380/4290: AvgLoss: 2.94, S.Acc: 34.81%, E.Acc: 16.83%\n",
      "Epoch 0. Batch 390/4290: AvgLoss: 2.94, S.Acc: 34.93%, E.Acc: 16.94%\n",
      "Epoch 0. Batch 400/4290: AvgLoss: 2.94, S.Acc: 34.99%, E.Acc: 17.07%\n",
      "Epoch 0. Batch 410/4290: AvgLoss: 2.94, S.Acc: 35.13%, E.Acc: 17.15%\n",
      "Epoch 0. Batch 420/4290: AvgLoss: 2.94, S.Acc: 35.04%, E.Acc: 17.28%\n",
      "Epoch 0. Batch 430/4290: AvgLoss: 2.93, S.Acc: 35.18%, E.Acc: 17.40%\n",
      "Epoch 0. Batch 440/4290: AvgLoss: 2.93, S.Acc: 35.20%, E.Acc: 17.43%\n",
      "Epoch 0. Batch 450/4290: AvgLoss: 2.93, S.Acc: 35.31%, E.Acc: 17.60%\n",
      "Epoch 0. Batch 460/4290: AvgLoss: 2.93, S.Acc: 35.52%, E.Acc: 17.71%\n",
      "Epoch 0. Batch 470/4290: AvgLoss: 2.93, S.Acc: 35.68%, E.Acc: 17.95%\n",
      "Epoch 0. Batch 480/4290: AvgLoss: 2.92, S.Acc: 35.72%, E.Acc: 18.17%\n",
      "Epoch 0. Batch 490/4290: AvgLoss: 2.92, S.Acc: 35.81%, E.Acc: 18.48%\n",
      "Epoch 0. Batch 500/4290: AvgLoss: 2.91, S.Acc: 35.84%, E.Acc: 18.91%\n",
      "Epoch 0. Batch 510/4290: AvgLoss: 2.91, S.Acc: 35.80%, E.Acc: 19.25%\n",
      "Epoch 0. Batch 520/4290: AvgLoss: 2.90, S.Acc: 35.81%, E.Acc: 19.59%\n",
      "Epoch 0. Batch 530/4290: AvgLoss: 2.89, S.Acc: 36.05%, E.Acc: 19.80%\n",
      "Epoch 0. Batch 540/4290: AvgLoss: 2.89, S.Acc: 36.19%, E.Acc: 20.12%\n",
      "Epoch 0. Batch 550/4290: AvgLoss: 2.88, S.Acc: 36.26%, E.Acc: 20.27%\n",
      "Epoch 0. Batch 560/4290: AvgLoss: 2.88, S.Acc: 36.34%, E.Acc: 20.38%\n",
      "Epoch 0. Batch 570/4290: AvgLoss: 2.87, S.Acc: 36.39%, E.Acc: 20.72%\n",
      "Epoch 0. Batch 580/4290: AvgLoss: 2.87, S.Acc: 36.41%, E.Acc: 20.99%\n",
      "Epoch 0. Batch 590/4290: AvgLoss: 2.86, S.Acc: 36.43%, E.Acc: 21.30%\n",
      "Epoch 0. Batch 600/4290: AvgLoss: 2.85, S.Acc: 36.58%, E.Acc: 21.63%\n",
      "Epoch 0. Batch 610/4290: AvgLoss: 2.85, S.Acc: 36.66%, E.Acc: 21.95%\n",
      "Epoch 0. Batch 620/4290: AvgLoss: 2.84, S.Acc: 36.69%, E.Acc: 22.17%\n",
      "Epoch 0. Batch 630/4290: AvgLoss: 2.83, S.Acc: 36.72%, E.Acc: 22.56%\n",
      "Epoch 0. Batch 640/4290: AvgLoss: 2.83, S.Acc: 36.71%, E.Acc: 22.77%\n",
      "Epoch 0. Batch 650/4290: AvgLoss: 2.82, S.Acc: 36.72%, E.Acc: 23.05%\n",
      "Epoch 0. Batch 660/4290: AvgLoss: 2.82, S.Acc: 36.72%, E.Acc: 23.32%\n",
      "Epoch 0. Batch 670/4290: AvgLoss: 2.81, S.Acc: 36.94%, E.Acc: 23.61%\n",
      "Epoch 0. Batch 680/4290: AvgLoss: 2.80, S.Acc: 37.03%, E.Acc: 23.93%\n",
      "Epoch 0. Batch 690/4290: AvgLoss: 2.80, S.Acc: 37.12%, E.Acc: 24.28%\n",
      "Epoch 0. Batch 700/4290: AvgLoss: 2.79, S.Acc: 37.32%, E.Acc: 24.56%\n",
      "Epoch 0. Batch 710/4290: AvgLoss: 2.78, S.Acc: 37.39%, E.Acc: 24.86%\n",
      "Epoch 0. Batch 720/4290: AvgLoss: 2.77, S.Acc: 37.55%, E.Acc: 25.23%\n",
      "Epoch 0. Batch 730/4290: AvgLoss: 2.77, S.Acc: 37.57%, E.Acc: 25.59%\n",
      "Epoch 0. Batch 740/4290: AvgLoss: 2.76, S.Acc: 37.67%, E.Acc: 26.00%\n",
      "Epoch 0. Batch 750/4290: AvgLoss: 2.75, S.Acc: 37.85%, E.Acc: 26.31%\n",
      "Epoch 0. Batch 760/4290: AvgLoss: 2.74, S.Acc: 37.98%, E.Acc: 26.64%\n",
      "Epoch 0. Batch 770/4290: AvgLoss: 2.73, S.Acc: 37.98%, E.Acc: 26.92%\n",
      "Epoch 0. Batch 780/4290: AvgLoss: 2.73, S.Acc: 38.06%, E.Acc: 27.22%\n",
      "Epoch 0. Batch 790/4290: AvgLoss: 2.72, S.Acc: 38.23%, E.Acc: 27.55%\n",
      "Epoch 0. Batch 800/4290: AvgLoss: 2.71, S.Acc: 38.37%, E.Acc: 28.00%\n",
      "Epoch 0. Batch 810/4290: AvgLoss: 2.70, S.Acc: 38.56%, E.Acc: 28.28%\n",
      "Epoch 0. Batch 820/4290: AvgLoss: 2.69, S.Acc: 38.70%, E.Acc: 28.67%\n",
      "Epoch 0. Batch 830/4290: AvgLoss: 2.68, S.Acc: 38.80%, E.Acc: 29.08%\n",
      "Epoch 0. Batch 840/4290: AvgLoss: 2.67, S.Acc: 38.85%, E.Acc: 29.45%\n",
      "Epoch 0. Batch 850/4290: AvgLoss: 2.66, S.Acc: 38.94%, E.Acc: 29.83%\n",
      "Epoch 0. Batch 860/4290: AvgLoss: 2.65, S.Acc: 39.02%, E.Acc: 30.23%\n",
      "Epoch 0. Batch 870/4290: AvgLoss: 2.65, S.Acc: 39.23%, E.Acc: 30.54%\n",
      "Epoch 0. Batch 880/4290: AvgLoss: 2.64, S.Acc: 39.32%, E.Acc: 30.88%\n",
      "Epoch 0. Batch 890/4290: AvgLoss: 2.63, S.Acc: 39.41%, E.Acc: 31.26%\n",
      "Epoch 0. Batch 900/4290: AvgLoss: 2.62, S.Acc: 39.53%, E.Acc: 31.61%\n",
      "Epoch 0. Batch 910/4290: AvgLoss: 2.61, S.Acc: 39.66%, E.Acc: 31.94%\n",
      "Epoch 0. Batch 920/4290: AvgLoss: 2.60, S.Acc: 39.85%, E.Acc: 32.32%\n",
      "Epoch 0. Batch 930/4290: AvgLoss: 2.59, S.Acc: 39.90%, E.Acc: 32.79%\n",
      "Epoch 0. Batch 940/4290: AvgLoss: 2.58, S.Acc: 40.10%, E.Acc: 33.24%\n",
      "Epoch 0. Batch 950/4290: AvgLoss: 2.58, S.Acc: 40.20%, E.Acc: 33.54%\n",
      "Epoch 0. Batch 960/4290: AvgLoss: 2.57, S.Acc: 40.32%, E.Acc: 33.95%\n",
      "Epoch 0. Batch 970/4290: AvgLoss: 2.56, S.Acc: 40.42%, E.Acc: 34.33%\n",
      "Epoch 0. Batch 980/4290: AvgLoss: 2.55, S.Acc: 40.46%, E.Acc: 34.70%\n",
      "Epoch 0. Batch 990/4290: AvgLoss: 2.54, S.Acc: 40.60%, E.Acc: 35.10%\n",
      "Epoch 0. Batch 1000/4290: AvgLoss: 2.53, S.Acc: 40.75%, E.Acc: 35.50%\n",
      "Epoch 0. Batch 1010/4290: AvgLoss: 2.52, S.Acc: 40.81%, E.Acc: 35.92%\n",
      "Epoch 0. Batch 1020/4290: AvgLoss: 2.52, S.Acc: 40.87%, E.Acc: 36.29%\n",
      "Epoch 0. Batch 1030/4290: AvgLoss: 2.51, S.Acc: 41.00%, E.Acc: 36.60%\n",
      "Epoch 0. Batch 1040/4290: AvgLoss: 2.50, S.Acc: 41.08%, E.Acc: 36.97%\n",
      "Epoch 0. Batch 1050/4290: AvgLoss: 2.49, S.Acc: 41.18%, E.Acc: 37.35%\n",
      "Epoch 0. Batch 1060/4290: AvgLoss: 2.48, S.Acc: 41.35%, E.Acc: 37.77%\n",
      "Epoch 0. Batch 1070/4290: AvgLoss: 2.47, S.Acc: 41.48%, E.Acc: 38.13%\n",
      "Epoch 0. Batch 1080/4290: AvgLoss: 2.47, S.Acc: 41.60%, E.Acc: 38.47%\n",
      "Epoch 0. Batch 1090/4290: AvgLoss: 2.46, S.Acc: 41.68%, E.Acc: 38.86%\n",
      "Epoch 0. Batch 1100/4290: AvgLoss: 2.45, S.Acc: 41.78%, E.Acc: 39.23%\n",
      "Epoch 0. Batch 1110/4290: AvgLoss: 2.44, S.Acc: 41.91%, E.Acc: 39.60%\n",
      "Epoch 0. Batch 1120/4290: AvgLoss: 2.43, S.Acc: 42.02%, E.Acc: 39.93%\n",
      "Epoch 0. Batch 1130/4290: AvgLoss: 2.43, S.Acc: 42.13%, E.Acc: 40.26%\n",
      "Epoch 0. Batch 1140/4290: AvgLoss: 2.42, S.Acc: 42.24%, E.Acc: 40.62%\n",
      "Epoch 0. Batch 1150/4290: AvgLoss: 2.41, S.Acc: 42.32%, E.Acc: 40.98%\n",
      "Epoch 0. Batch 1160/4290: AvgLoss: 2.40, S.Acc: 42.41%, E.Acc: 41.31%\n",
      "Epoch 0. Batch 1170/4290: AvgLoss: 2.39, S.Acc: 42.55%, E.Acc: 41.60%\n",
      "Epoch 0. Batch 1180/4290: AvgLoss: 2.39, S.Acc: 42.61%, E.Acc: 41.96%\n",
      "Epoch 0. Batch 1190/4290: AvgLoss: 2.38, S.Acc: 42.70%, E.Acc: 42.26%\n",
      "Epoch 0. Batch 1200/4290: AvgLoss: 2.37, S.Acc: 42.76%, E.Acc: 42.56%\n",
      "Epoch 0. Batch 1210/4290: AvgLoss: 2.36, S.Acc: 42.83%, E.Acc: 42.89%\n",
      "Epoch 0. Batch 1220/4290: AvgLoss: 2.36, S.Acc: 42.96%, E.Acc: 43.23%\n",
      "Epoch 0. Batch 1230/4290: AvgLoss: 2.35, S.Acc: 43.14%, E.Acc: 43.55%\n",
      "Epoch 0. Batch 1240/4290: AvgLoss: 2.34, S.Acc: 43.23%, E.Acc: 43.86%\n",
      "Epoch 0. Batch 1250/4290: AvgLoss: 2.33, S.Acc: 43.35%, E.Acc: 44.13%\n",
      "Epoch 0. Batch 1260/4290: AvgLoss: 2.33, S.Acc: 43.40%, E.Acc: 44.43%\n",
      "Epoch 0. Batch 1270/4290: AvgLoss: 2.32, S.Acc: 43.52%, E.Acc: 44.73%\n",
      "Epoch 0. Batch 1280/4290: AvgLoss: 2.31, S.Acc: 43.58%, E.Acc: 45.05%\n",
      "Epoch 0. Batch 1290/4290: AvgLoss: 2.31, S.Acc: 43.67%, E.Acc: 45.34%\n",
      "Epoch 0. Batch 1300/4290: AvgLoss: 2.30, S.Acc: 43.76%, E.Acc: 45.63%\n",
      "Epoch 0. Batch 1310/4290: AvgLoss: 2.29, S.Acc: 43.83%, E.Acc: 45.95%\n",
      "Epoch 0. Batch 1320/4290: AvgLoss: 2.28, S.Acc: 43.96%, E.Acc: 46.23%\n",
      "Epoch 0. Batch 1330/4290: AvgLoss: 2.28, S.Acc: 44.02%, E.Acc: 46.50%\n",
      "Epoch 0. Batch 1340/4290: AvgLoss: 2.27, S.Acc: 44.11%, E.Acc: 46.78%\n",
      "Epoch 0. Batch 1350/4290: AvgLoss: 2.26, S.Acc: 44.20%, E.Acc: 47.06%\n",
      "Epoch 0. Batch 1360/4290: AvgLoss: 2.26, S.Acc: 44.31%, E.Acc: 47.32%\n",
      "Epoch 0. Batch 1370/4290: AvgLoss: 2.25, S.Acc: 44.42%, E.Acc: 47.58%\n",
      "Epoch 0. Batch 1380/4290: AvgLoss: 2.24, S.Acc: 44.43%, E.Acc: 47.85%\n",
      "Epoch 0. Batch 1390/4290: AvgLoss: 2.24, S.Acc: 44.48%, E.Acc: 48.11%\n",
      "Epoch 0. Batch 1400/4290: AvgLoss: 2.23, S.Acc: 44.58%, E.Acc: 48.39%\n",
      "Epoch 0. Batch 1410/4290: AvgLoss: 2.23, S.Acc: 44.62%, E.Acc: 48.68%\n",
      "Epoch 0. Batch 1420/4290: AvgLoss: 2.22, S.Acc: 44.65%, E.Acc: 48.97%\n",
      "Epoch 0. Batch 1430/4290: AvgLoss: 2.21, S.Acc: 44.72%, E.Acc: 49.22%\n",
      "Epoch 0. Batch 1440/4290: AvgLoss: 2.21, S.Acc: 44.77%, E.Acc: 49.45%\n",
      "Epoch 0. Batch 1450/4290: AvgLoss: 2.20, S.Acc: 44.89%, E.Acc: 49.73%\n",
      "Epoch 0. Batch 1460/4290: AvgLoss: 2.19, S.Acc: 44.98%, E.Acc: 49.99%\n",
      "Epoch 0. Batch 1470/4290: AvgLoss: 2.19, S.Acc: 45.06%, E.Acc: 50.26%\n",
      "Epoch 0. Batch 1480/4290: AvgLoss: 2.18, S.Acc: 45.09%, E.Acc: 50.51%\n",
      "Epoch 0. Batch 1490/4290: AvgLoss: 2.18, S.Acc: 45.17%, E.Acc: 50.72%\n",
      "Epoch 0. Batch 1500/4290: AvgLoss: 2.17, S.Acc: 45.25%, E.Acc: 50.95%\n",
      "Epoch 0. Batch 1510/4290: AvgLoss: 2.16, S.Acc: 45.32%, E.Acc: 51.22%\n",
      "Epoch 0. Batch 1520/4290: AvgLoss: 2.16, S.Acc: 45.40%, E.Acc: 51.48%\n",
      "Epoch 0. Batch 1530/4290: AvgLoss: 2.15, S.Acc: 45.49%, E.Acc: 51.73%\n",
      "Epoch 0. Batch 1540/4290: AvgLoss: 2.14, S.Acc: 45.60%, E.Acc: 51.98%\n",
      "Epoch 0. Batch 1550/4290: AvgLoss: 2.14, S.Acc: 45.64%, E.Acc: 52.21%\n",
      "Epoch 0. Batch 1560/4290: AvgLoss: 2.13, S.Acc: 45.73%, E.Acc: 52.43%\n",
      "Epoch 0. Batch 1570/4290: AvgLoss: 2.13, S.Acc: 45.80%, E.Acc: 52.68%\n",
      "Epoch 0. Batch 1580/4290: AvgLoss: 2.12, S.Acc: 45.86%, E.Acc: 52.89%\n",
      "Epoch 0. Batch 1590/4290: AvgLoss: 2.12, S.Acc: 45.93%, E.Acc: 53.12%\n",
      "Epoch 0. Batch 1600/4290: AvgLoss: 2.11, S.Acc: 45.93%, E.Acc: 53.33%\n",
      "Epoch 0. Batch 1610/4290: AvgLoss: 2.11, S.Acc: 45.97%, E.Acc: 53.57%\n",
      "Epoch 0. Batch 1620/4290: AvgLoss: 2.10, S.Acc: 46.04%, E.Acc: 53.79%\n",
      "Epoch 0. Batch 1630/4290: AvgLoss: 2.09, S.Acc: 46.13%, E.Acc: 54.00%\n",
      "Epoch 0. Batch 1640/4290: AvgLoss: 2.09, S.Acc: 46.20%, E.Acc: 54.21%\n",
      "Epoch 0. Batch 1650/4290: AvgLoss: 2.08, S.Acc: 46.29%, E.Acc: 54.41%\n",
      "Epoch 0. Batch 1660/4290: AvgLoss: 2.08, S.Acc: 46.34%, E.Acc: 54.60%\n",
      "Epoch 0. Batch 1670/4290: AvgLoss: 2.07, S.Acc: 46.42%, E.Acc: 54.81%\n",
      "Epoch 0. Batch 1680/4290: AvgLoss: 2.07, S.Acc: 46.53%, E.Acc: 55.00%\n",
      "Epoch 0. Batch 1690/4290: AvgLoss: 2.06, S.Acc: 46.58%, E.Acc: 55.17%\n",
      "Epoch 0. Batch 1700/4290: AvgLoss: 2.06, S.Acc: 46.66%, E.Acc: 55.38%\n",
      "Epoch 0. Batch 1710/4290: AvgLoss: 2.05, S.Acc: 46.76%, E.Acc: 55.57%\n",
      "Epoch 0. Batch 1720/4290: AvgLoss: 2.05, S.Acc: 46.84%, E.Acc: 55.76%\n",
      "Epoch 0. Batch 1730/4290: AvgLoss: 2.04, S.Acc: 46.90%, E.Acc: 55.96%\n",
      "Epoch 0. Batch 1740/4290: AvgLoss: 2.04, S.Acc: 46.97%, E.Acc: 56.15%\n",
      "Epoch 0. Batch 1750/4290: AvgLoss: 2.04, S.Acc: 46.98%, E.Acc: 56.31%\n",
      "Epoch 0. Batch 1760/4290: AvgLoss: 2.03, S.Acc: 47.04%, E.Acc: 56.51%\n",
      "Epoch 0. Batch 1770/4290: AvgLoss: 2.03, S.Acc: 47.05%, E.Acc: 56.72%\n",
      "Epoch 0. Batch 1780/4290: AvgLoss: 2.02, S.Acc: 47.09%, E.Acc: 56.90%\n",
      "Epoch 0. Batch 1790/4290: AvgLoss: 2.02, S.Acc: 47.15%, E.Acc: 57.08%\n",
      "Epoch 0. Batch 1800/4290: AvgLoss: 2.01, S.Acc: 47.18%, E.Acc: 57.26%\n",
      "Epoch 0. Batch 1810/4290: AvgLoss: 2.01, S.Acc: 47.24%, E.Acc: 57.44%\n",
      "Epoch 0. Batch 1820/4290: AvgLoss: 2.00, S.Acc: 47.32%, E.Acc: 57.59%\n",
      "Epoch 0. Batch 1830/4290: AvgLoss: 2.00, S.Acc: 47.34%, E.Acc: 57.78%\n",
      "Epoch 0. Batch 1840/4290: AvgLoss: 2.00, S.Acc: 47.41%, E.Acc: 57.94%\n",
      "Epoch 0. Batch 1850/4290: AvgLoss: 1.99, S.Acc: 47.43%, E.Acc: 58.10%\n",
      "Epoch 0. Batch 1860/4290: AvgLoss: 1.99, S.Acc: 47.49%, E.Acc: 58.28%\n",
      "Epoch 0. Batch 1870/4290: AvgLoss: 1.98, S.Acc: 47.55%, E.Acc: 58.43%\n",
      "Epoch 0. Batch 1880/4290: AvgLoss: 1.98, S.Acc: 47.58%, E.Acc: 58.60%\n",
      "Epoch 0. Batch 1890/4290: AvgLoss: 1.98, S.Acc: 47.66%, E.Acc: 58.74%\n",
      "Epoch 0. Batch 1900/4290: AvgLoss: 1.97, S.Acc: 47.74%, E.Acc: 58.92%\n",
      "Epoch 0. Batch 1910/4290: AvgLoss: 1.97, S.Acc: 47.78%, E.Acc: 59.09%\n",
      "Epoch 0. Batch 1920/4290: AvgLoss: 1.96, S.Acc: 47.81%, E.Acc: 59.23%\n",
      "Epoch 0. Batch 1930/4290: AvgLoss: 1.96, S.Acc: 47.84%, E.Acc: 59.37%\n",
      "Epoch 0. Batch 1940/4290: AvgLoss: 1.96, S.Acc: 47.90%, E.Acc: 59.52%\n",
      "Epoch 0. Batch 1950/4290: AvgLoss: 1.95, S.Acc: 47.95%, E.Acc: 59.68%\n",
      "Epoch 0. Batch 1960/4290: AvgLoss: 1.95, S.Acc: 47.98%, E.Acc: 59.84%\n",
      "Epoch 0. Batch 1970/4290: AvgLoss: 1.94, S.Acc: 48.02%, E.Acc: 59.97%\n",
      "Epoch 0. Batch 1980/4290: AvgLoss: 1.94, S.Acc: 48.07%, E.Acc: 60.13%\n",
      "Epoch 0. Batch 1990/4290: AvgLoss: 1.94, S.Acc: 48.15%, E.Acc: 60.29%\n",
      "Epoch 0. Batch 2000/4290: AvgLoss: 1.93, S.Acc: 48.21%, E.Acc: 60.46%\n",
      "Epoch 0. Batch 2010/4290: AvgLoss: 1.93, S.Acc: 48.26%, E.Acc: 60.61%\n",
      "Epoch 0. Batch 2020/4290: AvgLoss: 1.92, S.Acc: 48.29%, E.Acc: 60.76%\n",
      "Epoch 0. Batch 2030/4290: AvgLoss: 1.92, S.Acc: 48.33%, E.Acc: 60.91%\n",
      "Epoch 0. Batch 2040/4290: AvgLoss: 1.91, S.Acc: 48.38%, E.Acc: 61.07%\n",
      "Epoch 0. Batch 2050/4290: AvgLoss: 1.91, S.Acc: 48.44%, E.Acc: 61.20%\n",
      "Epoch 0. Batch 2060/4290: AvgLoss: 1.91, S.Acc: 48.44%, E.Acc: 61.35%\n",
      "Epoch 0. Batch 2070/4290: AvgLoss: 1.90, S.Acc: 48.50%, E.Acc: 61.49%\n",
      "Epoch 0. Batch 2080/4290: AvgLoss: 1.90, S.Acc: 48.52%, E.Acc: 61.63%\n",
      "Epoch 0. Batch 2090/4290: AvgLoss: 1.90, S.Acc: 48.56%, E.Acc: 61.79%\n",
      "Epoch 0. Batch 2100/4290: AvgLoss: 1.89, S.Acc: 48.60%, E.Acc: 61.95%\n",
      "Epoch 0. Batch 2110/4290: AvgLoss: 1.89, S.Acc: 48.66%, E.Acc: 62.06%\n",
      "Epoch 0. Batch 2120/4290: AvgLoss: 1.88, S.Acc: 48.69%, E.Acc: 62.19%\n",
      "Epoch 0. Batch 2130/4290: AvgLoss: 1.88, S.Acc: 48.74%, E.Acc: 62.34%\n",
      "Epoch 0. Batch 2140/4290: AvgLoss: 1.88, S.Acc: 48.78%, E.Acc: 62.48%\n",
      "Epoch 0. Batch 2150/4290: AvgLoss: 1.87, S.Acc: 48.84%, E.Acc: 62.64%\n",
      "Epoch 0. Batch 2160/4290: AvgLoss: 1.87, S.Acc: 48.87%, E.Acc: 62.76%\n",
      "Epoch 0. Batch 2170/4290: AvgLoss: 1.87, S.Acc: 48.90%, E.Acc: 62.88%\n",
      "Epoch 0. Batch 2180/4290: AvgLoss: 1.86, S.Acc: 48.96%, E.Acc: 63.02%\n",
      "Epoch 0. Batch 2190/4290: AvgLoss: 1.86, S.Acc: 48.99%, E.Acc: 63.14%\n",
      "Epoch 0. Batch 2200/4290: AvgLoss: 1.86, S.Acc: 49.01%, E.Acc: 63.28%\n",
      "Epoch 0. Batch 2210/4290: AvgLoss: 1.85, S.Acc: 49.04%, E.Acc: 63.41%\n",
      "Epoch 0. Batch 2220/4290: AvgLoss: 1.85, S.Acc: 49.09%, E.Acc: 63.54%\n",
      "Epoch 0. Batch 2230/4290: AvgLoss: 1.85, S.Acc: 49.13%, E.Acc: 63.66%\n",
      "Epoch 0. Batch 2240/4290: AvgLoss: 1.84, S.Acc: 49.18%, E.Acc: 63.79%\n",
      "Epoch 0. Batch 2250/4290: AvgLoss: 1.84, S.Acc: 49.23%, E.Acc: 63.90%\n",
      "Epoch 0. Batch 2260/4290: AvgLoss: 1.84, S.Acc: 49.26%, E.Acc: 64.01%\n",
      "Epoch 0. Batch 2270/4290: AvgLoss: 1.83, S.Acc: 49.29%, E.Acc: 64.13%\n",
      "Epoch 0. Batch 2280/4290: AvgLoss: 1.83, S.Acc: 49.34%, E.Acc: 64.26%\n",
      "Epoch 0. Batch 2290/4290: AvgLoss: 1.83, S.Acc: 49.36%, E.Acc: 64.37%\n",
      "Epoch 0. Batch 2300/4290: AvgLoss: 1.82, S.Acc: 49.41%, E.Acc: 64.50%\n",
      "Epoch 0. Batch 2310/4290: AvgLoss: 1.82, S.Acc: 49.45%, E.Acc: 64.63%\n",
      "Epoch 0. Batch 2320/4290: AvgLoss: 1.82, S.Acc: 49.47%, E.Acc: 64.76%\n",
      "Epoch 0. Batch 2330/4290: AvgLoss: 1.81, S.Acc: 49.51%, E.Acc: 64.87%\n",
      "Epoch 0. Batch 2340/4290: AvgLoss: 1.81, S.Acc: 49.55%, E.Acc: 64.99%\n",
      "Epoch 0. Batch 2350/4290: AvgLoss: 1.81, S.Acc: 49.61%, E.Acc: 65.09%\n",
      "Epoch 0. Batch 2360/4290: AvgLoss: 1.81, S.Acc: 49.67%, E.Acc: 65.21%\n",
      "Epoch 0. Batch 2370/4290: AvgLoss: 1.80, S.Acc: 49.68%, E.Acc: 65.32%\n",
      "Epoch 0. Batch 2380/4290: AvgLoss: 1.80, S.Acc: 49.74%, E.Acc: 65.45%\n",
      "Epoch 0. Batch 2390/4290: AvgLoss: 1.80, S.Acc: 49.78%, E.Acc: 65.56%\n",
      "Epoch 0. Batch 2400/4290: AvgLoss: 1.79, S.Acc: 49.81%, E.Acc: 65.67%\n",
      "Epoch 0. Batch 2410/4290: AvgLoss: 1.79, S.Acc: 49.83%, E.Acc: 65.78%\n",
      "Epoch 0. Batch 2420/4290: AvgLoss: 1.79, S.Acc: 49.85%, E.Acc: 65.89%\n",
      "Epoch 0. Batch 2430/4290: AvgLoss: 1.78, S.Acc: 49.86%, E.Acc: 66.00%\n",
      "Epoch 0. Batch 2440/4290: AvgLoss: 1.78, S.Acc: 49.88%, E.Acc: 66.11%\n",
      "Epoch 0. Batch 2450/4290: AvgLoss: 1.78, S.Acc: 49.91%, E.Acc: 66.21%\n",
      "Epoch 0. Batch 2460/4290: AvgLoss: 1.78, S.Acc: 49.96%, E.Acc: 66.31%\n",
      "Epoch 0. Batch 2470/4290: AvgLoss: 1.77, S.Acc: 49.98%, E.Acc: 66.42%\n",
      "Epoch 0. Batch 2480/4290: AvgLoss: 1.77, S.Acc: 49.98%, E.Acc: 66.50%\n",
      "Epoch 0. Batch 2490/4290: AvgLoss: 1.77, S.Acc: 50.02%, E.Acc: 66.59%\n",
      "Epoch 0. Batch 2500/4290: AvgLoss: 1.77, S.Acc: 50.05%, E.Acc: 66.69%\n",
      "Epoch 0. Batch 2510/4290: AvgLoss: 1.76, S.Acc: 50.07%, E.Acc: 66.78%\n",
      "Epoch 0. Batch 2520/4290: AvgLoss: 1.76, S.Acc: 50.07%, E.Acc: 66.89%\n",
      "Epoch 0. Batch 2530/4290: AvgLoss: 1.76, S.Acc: 50.12%, E.Acc: 66.99%\n",
      "Epoch 0. Batch 2540/4290: AvgLoss: 1.76, S.Acc: 50.16%, E.Acc: 67.08%\n",
      "Epoch 0. Batch 2550/4290: AvgLoss: 1.75, S.Acc: 50.21%, E.Acc: 67.16%\n",
      "Epoch 0. Batch 2560/4290: AvgLoss: 1.75, S.Acc: 50.21%, E.Acc: 67.26%\n",
      "Epoch 0. Batch 2570/4290: AvgLoss: 1.75, S.Acc: 50.26%, E.Acc: 67.35%\n",
      "Epoch 0. Batch 2580/4290: AvgLoss: 1.75, S.Acc: 50.26%, E.Acc: 67.44%\n",
      "Epoch 0. Batch 2590/4290: AvgLoss: 1.74, S.Acc: 50.30%, E.Acc: 67.53%\n",
      "Epoch 0. Batch 2600/4290: AvgLoss: 1.74, S.Acc: 50.35%, E.Acc: 67.61%\n",
      "Epoch 0. Batch 2610/4290: AvgLoss: 1.74, S.Acc: 50.38%, E.Acc: 67.71%\n",
      "Epoch 0. Batch 2620/4290: AvgLoss: 1.73, S.Acc: 50.45%, E.Acc: 67.81%\n",
      "Epoch 0. Batch 2630/4290: AvgLoss: 1.73, S.Acc: 50.49%, E.Acc: 67.91%\n",
      "Epoch 0. Batch 2640/4290: AvgLoss: 1.73, S.Acc: 50.53%, E.Acc: 68.01%\n",
      "Epoch 0. Batch 2650/4290: AvgLoss: 1.73, S.Acc: 50.55%, E.Acc: 68.10%\n",
      "Epoch 0. Batch 2660/4290: AvgLoss: 1.73, S.Acc: 50.56%, E.Acc: 68.20%\n",
      "Epoch 0. Batch 2670/4290: AvgLoss: 1.72, S.Acc: 50.61%, E.Acc: 68.27%\n",
      "Epoch 0. Batch 2680/4290: AvgLoss: 1.72, S.Acc: 50.63%, E.Acc: 68.38%\n",
      "Epoch 0. Batch 2690/4290: AvgLoss: 1.72, S.Acc: 50.65%, E.Acc: 68.46%\n",
      "Epoch 0. Batch 2700/4290: AvgLoss: 1.72, S.Acc: 50.70%, E.Acc: 68.53%\n",
      "Epoch 0. Batch 2710/4290: AvgLoss: 1.71, S.Acc: 50.74%, E.Acc: 68.63%\n",
      "Epoch 0. Batch 2720/4290: AvgLoss: 1.71, S.Acc: 50.79%, E.Acc: 68.72%\n",
      "Epoch 0. Batch 2730/4290: AvgLoss: 1.71, S.Acc: 50.82%, E.Acc: 68.80%\n",
      "Epoch 0. Batch 2740/4290: AvgLoss: 1.71, S.Acc: 50.84%, E.Acc: 68.89%\n",
      "Epoch 0. Batch 2750/4290: AvgLoss: 1.70, S.Acc: 50.86%, E.Acc: 68.97%\n",
      "Epoch 0. Batch 2760/4290: AvgLoss: 1.70, S.Acc: 50.89%, E.Acc: 69.07%\n",
      "Epoch 0. Batch 2770/4290: AvgLoss: 1.70, S.Acc: 50.93%, E.Acc: 69.15%\n",
      "Epoch 0. Batch 2780/4290: AvgLoss: 1.70, S.Acc: 50.97%, E.Acc: 69.24%\n",
      "Epoch 0. Batch 2790/4290: AvgLoss: 1.69, S.Acc: 51.01%, E.Acc: 69.32%\n",
      "Epoch 0. Batch 2800/4290: AvgLoss: 1.69, S.Acc: 51.03%, E.Acc: 69.39%\n",
      "Epoch 0. Batch 2810/4290: AvgLoss: 1.69, S.Acc: 51.07%, E.Acc: 69.48%\n",
      "Epoch 0. Batch 2820/4290: AvgLoss: 1.69, S.Acc: 51.08%, E.Acc: 69.55%\n",
      "Epoch 0. Batch 2830/4290: AvgLoss: 1.69, S.Acc: 51.11%, E.Acc: 69.63%\n",
      "Epoch 0. Batch 2840/4290: AvgLoss: 1.68, S.Acc: 51.13%, E.Acc: 69.71%\n",
      "Epoch 0. Batch 2850/4290: AvgLoss: 1.68, S.Acc: 51.16%, E.Acc: 69.80%\n",
      "Epoch 0. Batch 2860/4290: AvgLoss: 1.68, S.Acc: 51.17%, E.Acc: 69.88%\n",
      "Epoch 0. Batch 2870/4290: AvgLoss: 1.68, S.Acc: 51.23%, E.Acc: 69.96%\n",
      "Epoch 0. Batch 2880/4290: AvgLoss: 1.67, S.Acc: 51.26%, E.Acc: 70.04%\n",
      "Epoch 0. Batch 2890/4290: AvgLoss: 1.67, S.Acc: 51.29%, E.Acc: 70.12%\n",
      "Epoch 0. Batch 2900/4290: AvgLoss: 1.67, S.Acc: 51.33%, E.Acc: 70.20%\n",
      "Epoch 0. Batch 2910/4290: AvgLoss: 1.67, S.Acc: 51.36%, E.Acc: 70.27%\n",
      "Epoch 0. Batch 2920/4290: AvgLoss: 1.67, S.Acc: 51.38%, E.Acc: 70.36%\n",
      "Epoch 0. Batch 2930/4290: AvgLoss: 1.66, S.Acc: 51.41%, E.Acc: 70.43%\n",
      "Epoch 0. Batch 2940/4290: AvgLoss: 1.66, S.Acc: 51.46%, E.Acc: 70.51%\n",
      "Epoch 0. Batch 2950/4290: AvgLoss: 1.66, S.Acc: 51.47%, E.Acc: 70.58%\n",
      "Epoch 0. Batch 2960/4290: AvgLoss: 1.66, S.Acc: 51.52%, E.Acc: 70.65%\n",
      "Epoch 0. Batch 2970/4290: AvgLoss: 1.66, S.Acc: 51.55%, E.Acc: 70.72%\n",
      "Epoch 0. Batch 2980/4290: AvgLoss: 1.65, S.Acc: 51.61%, E.Acc: 70.80%\n",
      "Epoch 0. Batch 2990/4290: AvgLoss: 1.65, S.Acc: 51.64%, E.Acc: 70.88%\n",
      "Epoch 0. Batch 3000/4290: AvgLoss: 1.65, S.Acc: 51.66%, E.Acc: 70.96%\n",
      "Epoch 0. Batch 3010/4290: AvgLoss: 1.65, S.Acc: 51.70%, E.Acc: 71.04%\n",
      "Epoch 0. Batch 3020/4290: AvgLoss: 1.64, S.Acc: 51.70%, E.Acc: 71.10%\n",
      "Epoch 0. Batch 3030/4290: AvgLoss: 1.64, S.Acc: 51.73%, E.Acc: 71.17%\n",
      "Epoch 0. Batch 3040/4290: AvgLoss: 1.64, S.Acc: 51.76%, E.Acc: 71.23%\n",
      "Epoch 0. Batch 3050/4290: AvgLoss: 1.64, S.Acc: 51.79%, E.Acc: 71.29%\n",
      "Epoch 0. Batch 3060/4290: AvgLoss: 1.64, S.Acc: 51.82%, E.Acc: 71.36%\n",
      "Epoch 0. Batch 3070/4290: AvgLoss: 1.64, S.Acc: 51.82%, E.Acc: 71.42%\n",
      "Epoch 0. Batch 3080/4290: AvgLoss: 1.63, S.Acc: 51.84%, E.Acc: 71.50%\n",
      "Epoch 0. Batch 3090/4290: AvgLoss: 1.63, S.Acc: 51.85%, E.Acc: 71.57%\n",
      "Epoch 0. Batch 3100/4290: AvgLoss: 1.63, S.Acc: 51.88%, E.Acc: 71.63%\n",
      "Epoch 0. Batch 3110/4290: AvgLoss: 1.63, S.Acc: 51.91%, E.Acc: 71.70%\n",
      "Epoch 0. Batch 3120/4290: AvgLoss: 1.63, S.Acc: 51.92%, E.Acc: 71.77%\n",
      "Epoch 0. Batch 3130/4290: AvgLoss: 1.62, S.Acc: 51.95%, E.Acc: 71.84%\n",
      "Epoch 0. Batch 3140/4290: AvgLoss: 1.62, S.Acc: 51.97%, E.Acc: 71.90%\n",
      "Epoch 0. Batch 3150/4290: AvgLoss: 1.62, S.Acc: 52.00%, E.Acc: 71.97%\n",
      "Epoch 0. Batch 3160/4290: AvgLoss: 1.62, S.Acc: 52.02%, E.Acc: 72.02%\n",
      "Epoch 0. Batch 3170/4290: AvgLoss: 1.62, S.Acc: 52.03%, E.Acc: 72.10%\n",
      "Epoch 0. Batch 3180/4290: AvgLoss: 1.61, S.Acc: 52.06%, E.Acc: 72.16%\n",
      "Epoch 0. Batch 3190/4290: AvgLoss: 1.61, S.Acc: 52.08%, E.Acc: 72.23%\n",
      "Epoch 0. Batch 3200/4290: AvgLoss: 1.61, S.Acc: 52.10%, E.Acc: 72.29%\n",
      "Epoch 0. Batch 3210/4290: AvgLoss: 1.61, S.Acc: 52.12%, E.Acc: 72.35%\n",
      "Epoch 0. Batch 3220/4290: AvgLoss: 1.61, S.Acc: 52.14%, E.Acc: 72.42%\n",
      "Epoch 0. Batch 3230/4290: AvgLoss: 1.60, S.Acc: 52.17%, E.Acc: 72.48%\n",
      "Epoch 0. Batch 3240/4290: AvgLoss: 1.60, S.Acc: 52.20%, E.Acc: 72.55%\n",
      "Epoch 0. Batch 3250/4290: AvgLoss: 1.60, S.Acc: 52.23%, E.Acc: 72.61%\n",
      "Epoch 0. Batch 3260/4290: AvgLoss: 1.60, S.Acc: 52.28%, E.Acc: 72.68%\n",
      "Epoch 0. Batch 3270/4290: AvgLoss: 1.60, S.Acc: 52.30%, E.Acc: 72.74%\n",
      "Epoch 0. Batch 3280/4290: AvgLoss: 1.60, S.Acc: 52.34%, E.Acc: 72.81%\n",
      "Epoch 0. Batch 3290/4290: AvgLoss: 1.59, S.Acc: 52.38%, E.Acc: 72.87%\n",
      "Epoch 0. Batch 3300/4290: AvgLoss: 1.59, S.Acc: 52.41%, E.Acc: 72.92%\n",
      "Epoch 0. Batch 3310/4290: AvgLoss: 1.59, S.Acc: 52.41%, E.Acc: 72.97%\n",
      "Epoch 0. Batch 3320/4290: AvgLoss: 1.59, S.Acc: 52.43%, E.Acc: 73.04%\n",
      "Epoch 0. Batch 3330/4290: AvgLoss: 1.59, S.Acc: 52.45%, E.Acc: 73.10%\n",
      "Epoch 0. Batch 3340/4290: AvgLoss: 1.59, S.Acc: 52.48%, E.Acc: 73.15%\n",
      "Epoch 0. Batch 3350/4290: AvgLoss: 1.58, S.Acc: 52.50%, E.Acc: 73.21%\n",
      "Epoch 0. Batch 3360/4290: AvgLoss: 1.58, S.Acc: 52.50%, E.Acc: 73.28%\n",
      "Epoch 0. Batch 3370/4290: AvgLoss: 1.58, S.Acc: 52.53%, E.Acc: 73.33%\n",
      "Epoch 0. Batch 3380/4290: AvgLoss: 1.58, S.Acc: 52.54%, E.Acc: 73.40%\n",
      "Epoch 0. Batch 3390/4290: AvgLoss: 1.58, S.Acc: 52.56%, E.Acc: 73.46%\n",
      "Epoch 0. Batch 3400/4290: AvgLoss: 1.58, S.Acc: 52.58%, E.Acc: 73.52%\n",
      "Epoch 0. Batch 3410/4290: AvgLoss: 1.57, S.Acc: 52.60%, E.Acc: 73.57%\n",
      "Epoch 0. Batch 3420/4290: AvgLoss: 1.57, S.Acc: 52.61%, E.Acc: 73.63%\n",
      "Epoch 0. Batch 3430/4290: AvgLoss: 1.57, S.Acc: 52.64%, E.Acc: 73.69%\n",
      "Epoch 0. Batch 3440/4290: AvgLoss: 1.57, S.Acc: 52.68%, E.Acc: 73.74%\n",
      "Epoch 0. Batch 3450/4290: AvgLoss: 1.57, S.Acc: 52.68%, E.Acc: 73.80%\n",
      "Epoch 0. Batch 3460/4290: AvgLoss: 1.57, S.Acc: 52.73%, E.Acc: 73.85%\n",
      "Epoch 0. Batch 3470/4290: AvgLoss: 1.56, S.Acc: 52.75%, E.Acc: 73.90%\n",
      "Epoch 0. Batch 3480/4290: AvgLoss: 1.56, S.Acc: 52.77%, E.Acc: 73.95%\n",
      "Epoch 0. Batch 3490/4290: AvgLoss: 1.56, S.Acc: 52.80%, E.Acc: 74.01%\n",
      "Epoch 0. Batch 3500/4290: AvgLoss: 1.56, S.Acc: 52.82%, E.Acc: 74.07%\n",
      "Epoch 0. Batch 3510/4290: AvgLoss: 1.56, S.Acc: 52.85%, E.Acc: 74.13%\n",
      "Epoch 0. Batch 3520/4290: AvgLoss: 1.56, S.Acc: 52.86%, E.Acc: 74.18%\n",
      "Epoch 0. Batch 3530/4290: AvgLoss: 1.55, S.Acc: 52.89%, E.Acc: 74.25%\n",
      "Epoch 0. Batch 3540/4290: AvgLoss: 1.55, S.Acc: 52.91%, E.Acc: 74.31%\n",
      "Epoch 0. Batch 3550/4290: AvgLoss: 1.55, S.Acc: 52.95%, E.Acc: 74.37%\n",
      "Epoch 0. Batch 3560/4290: AvgLoss: 1.55, S.Acc: 52.98%, E.Acc: 74.43%\n",
      "Epoch 0. Batch 3570/4290: AvgLoss: 1.55, S.Acc: 53.01%, E.Acc: 74.47%\n",
      "Epoch 0. Batch 3580/4290: AvgLoss: 1.55, S.Acc: 53.06%, E.Acc: 74.52%\n",
      "Epoch 0. Batch 3590/4290: AvgLoss: 1.54, S.Acc: 53.09%, E.Acc: 74.58%\n",
      "Epoch 0. Batch 3600/4290: AvgLoss: 1.54, S.Acc: 53.12%, E.Acc: 74.64%\n",
      "Epoch 0. Batch 3610/4290: AvgLoss: 1.54, S.Acc: 53.14%, E.Acc: 74.68%\n",
      "Epoch 0. Batch 3620/4290: AvgLoss: 1.54, S.Acc: 53.18%, E.Acc: 74.74%\n",
      "Epoch 0. Batch 3630/4290: AvgLoss: 1.54, S.Acc: 53.20%, E.Acc: 74.79%\n",
      "Epoch 0. Batch 3640/4290: AvgLoss: 1.54, S.Acc: 53.21%, E.Acc: 74.83%\n",
      "Epoch 0. Batch 3650/4290: AvgLoss: 1.54, S.Acc: 53.23%, E.Acc: 74.87%\n",
      "Epoch 0. Batch 3660/4290: AvgLoss: 1.53, S.Acc: 53.25%, E.Acc: 74.92%\n",
      "Epoch 0. Batch 3670/4290: AvgLoss: 1.53, S.Acc: 53.27%, E.Acc: 74.97%\n",
      "Epoch 0. Batch 3680/4290: AvgLoss: 1.53, S.Acc: 53.29%, E.Acc: 75.02%\n",
      "Epoch 0. Batch 3690/4290: AvgLoss: 1.53, S.Acc: 53.30%, E.Acc: 75.07%\n",
      "Epoch 0. Batch 3700/4290: AvgLoss: 1.53, S.Acc: 53.31%, E.Acc: 75.12%\n",
      "Epoch 0. Batch 3710/4290: AvgLoss: 1.53, S.Acc: 53.31%, E.Acc: 75.17%\n",
      "Epoch 0. Batch 3720/4290: AvgLoss: 1.53, S.Acc: 53.33%, E.Acc: 75.22%\n",
      "Epoch 0. Batch 3730/4290: AvgLoss: 1.52, S.Acc: 53.35%, E.Acc: 75.27%\n",
      "Epoch 0. Batch 3740/4290: AvgLoss: 1.52, S.Acc: 53.37%, E.Acc: 75.31%\n",
      "Epoch 0. Batch 3750/4290: AvgLoss: 1.52, S.Acc: 53.40%, E.Acc: 75.37%\n",
      "Epoch 0. Batch 3760/4290: AvgLoss: 1.52, S.Acc: 53.43%, E.Acc: 75.43%\n",
      "Epoch 0. Batch 3770/4290: AvgLoss: 1.52, S.Acc: 53.45%, E.Acc: 75.48%\n",
      "Epoch 0. Batch 3780/4290: AvgLoss: 1.52, S.Acc: 53.46%, E.Acc: 75.54%\n",
      "Epoch 0. Batch 3790/4290: AvgLoss: 1.51, S.Acc: 53.47%, E.Acc: 75.59%\n",
      "Epoch 0. Batch 3800/4290: AvgLoss: 1.51, S.Acc: 53.49%, E.Acc: 75.64%\n",
      "Epoch 0. Batch 3810/4290: AvgLoss: 1.51, S.Acc: 53.50%, E.Acc: 75.69%\n",
      "Epoch 0. Batch 3820/4290: AvgLoss: 1.51, S.Acc: 53.52%, E.Acc: 75.73%\n",
      "Epoch 0. Batch 3830/4290: AvgLoss: 1.51, S.Acc: 53.54%, E.Acc: 75.78%\n",
      "Epoch 0. Batch 3840/4290: AvgLoss: 1.51, S.Acc: 53.55%, E.Acc: 75.82%\n",
      "Epoch 0. Batch 3850/4290: AvgLoss: 1.51, S.Acc: 53.57%, E.Acc: 75.86%\n",
      "Epoch 0. Batch 3860/4290: AvgLoss: 1.51, S.Acc: 53.60%, E.Acc: 75.90%\n",
      "Epoch 0. Batch 3870/4290: AvgLoss: 1.50, S.Acc: 53.62%, E.Acc: 75.94%\n",
      "Epoch 0. Batch 3880/4290: AvgLoss: 1.50, S.Acc: 53.63%, E.Acc: 75.99%\n",
      "Epoch 0. Batch 3890/4290: AvgLoss: 1.50, S.Acc: 53.65%, E.Acc: 76.04%\n",
      "Epoch 0. Batch 3900/4290: AvgLoss: 1.50, S.Acc: 53.67%, E.Acc: 76.09%\n",
      "Epoch 0. Batch 3910/4290: AvgLoss: 1.50, S.Acc: 53.69%, E.Acc: 76.12%\n",
      "Epoch 0. Batch 3920/4290: AvgLoss: 1.50, S.Acc: 53.71%, E.Acc: 76.16%\n",
      "Epoch 0. Batch 3930/4290: AvgLoss: 1.50, S.Acc: 53.75%, E.Acc: 76.19%\n",
      "Epoch 0. Batch 3940/4290: AvgLoss: 1.50, S.Acc: 53.76%, E.Acc: 76.24%\n",
      "Epoch 0. Batch 3950/4290: AvgLoss: 1.49, S.Acc: 53.78%, E.Acc: 76.26%\n",
      "Epoch 0. Batch 3960/4290: AvgLoss: 1.49, S.Acc: 53.79%, E.Acc: 76.30%\n",
      "Epoch 0. Batch 3970/4290: AvgLoss: 1.49, S.Acc: 53.81%, E.Acc: 76.35%\n",
      "Epoch 0. Batch 3980/4290: AvgLoss: 1.49, S.Acc: 53.82%, E.Acc: 76.39%\n",
      "Epoch 0. Batch 3990/4290: AvgLoss: 1.49, S.Acc: 53.82%, E.Acc: 76.44%\n",
      "Epoch 0. Batch 4000/4290: AvgLoss: 1.49, S.Acc: 53.83%, E.Acc: 76.47%\n",
      "Epoch 0. Batch 4010/4290: AvgLoss: 1.49, S.Acc: 53.83%, E.Acc: 76.51%\n",
      "Epoch 0. Batch 4020/4290: AvgLoss: 1.49, S.Acc: 53.86%, E.Acc: 76.55%\n",
      "Epoch 0. Batch 4030/4290: AvgLoss: 1.48, S.Acc: 53.87%, E.Acc: 76.58%\n",
      "Epoch 0. Batch 4040/4290: AvgLoss: 1.48, S.Acc: 53.87%, E.Acc: 76.63%\n",
      "Epoch 0. Batch 4050/4290: AvgLoss: 1.48, S.Acc: 53.89%, E.Acc: 76.68%\n",
      "Epoch 0. Batch 4060/4290: AvgLoss: 1.48, S.Acc: 53.92%, E.Acc: 76.71%\n",
      "Epoch 0. Batch 4070/4290: AvgLoss: 1.48, S.Acc: 53.95%, E.Acc: 76.75%\n",
      "Epoch 0. Batch 4080/4290: AvgLoss: 1.48, S.Acc: 53.95%, E.Acc: 76.79%\n",
      "Epoch 0. Batch 4090/4290: AvgLoss: 1.48, S.Acc: 53.97%, E.Acc: 76.82%\n",
      "Epoch 0. Batch 4100/4290: AvgLoss: 1.48, S.Acc: 53.99%, E.Acc: 76.86%\n",
      "Epoch 0. Batch 4110/4290: AvgLoss: 1.48, S.Acc: 54.02%, E.Acc: 76.89%\n",
      "Epoch 0. Batch 4120/4290: AvgLoss: 1.47, S.Acc: 54.02%, E.Acc: 76.93%\n",
      "Epoch 0. Batch 4130/4290: AvgLoss: 1.47, S.Acc: 54.07%, E.Acc: 76.97%\n",
      "Epoch 0. Batch 4140/4290: AvgLoss: 1.47, S.Acc: 54.07%, E.Acc: 77.00%\n",
      "Epoch 0. Batch 4150/4290: AvgLoss: 1.47, S.Acc: 54.09%, E.Acc: 77.03%\n",
      "Epoch 0. Batch 4160/4290: AvgLoss: 1.47, S.Acc: 54.12%, E.Acc: 77.07%\n",
      "Epoch 0. Batch 4170/4290: AvgLoss: 1.47, S.Acc: 54.14%, E.Acc: 77.11%\n",
      "Epoch 0. Batch 4180/4290: AvgLoss: 1.47, S.Acc: 54.15%, E.Acc: 77.15%\n",
      "Epoch 0. Batch 4190/4290: AvgLoss: 1.47, S.Acc: 54.17%, E.Acc: 77.19%\n",
      "Epoch 0. Batch 4200/4290: AvgLoss: 1.47, S.Acc: 54.19%, E.Acc: 77.23%\n",
      "Epoch 0. Batch 4210/4290: AvgLoss: 1.46, S.Acc: 54.21%, E.Acc: 77.27%\n",
      "Epoch 0. Batch 4220/4290: AvgLoss: 1.46, S.Acc: 54.23%, E.Acc: 77.30%\n",
      "Epoch 0. Batch 4230/4290: AvgLoss: 1.46, S.Acc: 54.26%, E.Acc: 77.34%\n",
      "Epoch 0. Batch 4240/4290: AvgLoss: 1.46, S.Acc: 54.28%, E.Acc: 77.37%\n",
      "Epoch 0. Batch 4250/4290: AvgLoss: 1.46, S.Acc: 54.29%, E.Acc: 77.40%\n",
      "Epoch 0. Batch 4260/4290: AvgLoss: 1.46, S.Acc: 54.30%, E.Acc: 77.44%\n",
      "Epoch 0. Batch 4270/4290: AvgLoss: 1.46, S.Acc: 54.33%, E.Acc: 77.47%\n",
      "Epoch 0. Batch 4280/4290: AvgLoss: 1.46, S.Acc: 54.35%, E.Acc: 77.50%\n",
      "Train Loss: 1.4549, Train Sentiment Accuracy: 54.38%, Train Emotion Accuracy: 77.53%\n",
      "Validation Sentiment Accuracy: 63.41%, Validation Emotion Accuracy: 94.12%\n",
      "Epoch 1. Batch 0/4290: AvgLoss: 0.82, S.Acc: 43.75%, E.Acc: 100.00%\n",
      "Epoch 1. Batch 10/4290: AvgLoss: 0.84, S.Acc: 68.75%, E.Acc: 92.61%\n",
      "Epoch 1. Batch 20/4290: AvgLoss: 0.82, S.Acc: 72.02%, E.Acc: 92.86%\n",
      "Epoch 1. Batch 30/4290: AvgLoss: 0.82, S.Acc: 70.56%, E.Acc: 92.94%\n",
      "Epoch 1. Batch 40/4290: AvgLoss: 0.81, S.Acc: 70.73%, E.Acc: 93.45%\n",
      "Epoch 1. Batch 50/4290: AvgLoss: 0.84, S.Acc: 68.38%, E.Acc: 94.36%\n",
      "Epoch 1. Batch 60/4290: AvgLoss: 0.84, S.Acc: 67.62%, E.Acc: 95.08%\n",
      "Epoch 1. Batch 70/4290: AvgLoss: 0.85, S.Acc: 66.81%, E.Acc: 95.25%\n",
      "Epoch 1. Batch 80/4290: AvgLoss: 0.84, S.Acc: 67.67%, E.Acc: 95.14%\n",
      "Epoch 1. Batch 90/4290: AvgLoss: 0.84, S.Acc: 67.86%, E.Acc: 95.05%\n",
      "Epoch 1. Batch 100/4290: AvgLoss: 0.85, S.Acc: 67.57%, E.Acc: 94.93%\n",
      "Epoch 1. Batch 110/4290: AvgLoss: 0.86, S.Acc: 67.79%, E.Acc: 94.99%\n",
      "Epoch 1. Batch 120/4290: AvgLoss: 0.85, S.Acc: 68.08%, E.Acc: 95.04%\n",
      "Epoch 1. Batch 130/4290: AvgLoss: 0.85, S.Acc: 67.41%, E.Acc: 95.13%\n",
      "Epoch 1. Batch 140/4290: AvgLoss: 0.85, S.Acc: 67.55%, E.Acc: 94.99%\n",
      "Epoch 1. Batch 150/4290: AvgLoss: 0.85, S.Acc: 67.88%, E.Acc: 94.66%\n",
      "Epoch 1. Batch 160/4290: AvgLoss: 0.84, S.Acc: 67.90%, E.Acc: 94.84%\n",
      "Epoch 1. Batch 170/4290: AvgLoss: 0.85, S.Acc: 67.84%, E.Acc: 94.63%\n",
      "Epoch 1. Batch 180/4290: AvgLoss: 0.86, S.Acc: 67.82%, E.Acc: 94.34%\n",
      "Epoch 1. Batch 190/4290: AvgLoss: 0.86, S.Acc: 67.83%, E.Acc: 94.14%\n",
      "Epoch 1. Batch 200/4290: AvgLoss: 0.86, S.Acc: 67.60%, E.Acc: 94.25%\n",
      "Epoch 1. Batch 210/4290: AvgLoss: 0.87, S.Acc: 67.09%, E.Acc: 94.19%\n",
      "Epoch 1. Batch 220/4290: AvgLoss: 0.87, S.Acc: 66.94%, E.Acc: 93.92%\n",
      "Epoch 1. Batch 230/4290: AvgLoss: 0.87, S.Acc: 67.29%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 240/4290: AvgLoss: 0.87, S.Acc: 67.84%, E.Acc: 93.59%\n",
      "Epoch 1. Batch 250/4290: AvgLoss: 0.87, S.Acc: 67.95%, E.Acc: 93.48%\n",
      "Epoch 1. Batch 260/4290: AvgLoss: 0.87, S.Acc: 68.15%, E.Acc: 93.37%\n",
      "Epoch 1. Batch 270/4290: AvgLoss: 0.87, S.Acc: 68.27%, E.Acc: 93.27%\n",
      "Epoch 1. Batch 280/4290: AvgLoss: 0.87, S.Acc: 68.31%, E.Acc: 93.35%\n",
      "Epoch 1. Batch 290/4290: AvgLoss: 0.87, S.Acc: 68.19%, E.Acc: 93.34%\n",
      "Epoch 1. Batch 300/4290: AvgLoss: 0.87, S.Acc: 68.40%, E.Acc: 93.38%\n",
      "Epoch 1. Batch 310/4290: AvgLoss: 0.86, S.Acc: 68.47%, E.Acc: 93.49%\n",
      "Epoch 1. Batch 320/4290: AvgLoss: 0.86, S.Acc: 68.42%, E.Acc: 93.57%\n",
      "Epoch 1. Batch 330/4290: AvgLoss: 0.86, S.Acc: 68.56%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 340/4290: AvgLoss: 0.86, S.Acc: 68.53%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 350/4290: AvgLoss: 0.86, S.Acc: 68.54%, E.Acc: 93.64%\n",
      "Epoch 1. Batch 360/4290: AvgLoss: 0.86, S.Acc: 68.39%, E.Acc: 93.61%\n",
      "Epoch 1. Batch 370/4290: AvgLoss: 0.86, S.Acc: 68.19%, E.Acc: 93.62%\n",
      "Epoch 1. Batch 380/4290: AvgLoss: 0.86, S.Acc: 68.09%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 390/4290: AvgLoss: 0.86, S.Acc: 68.05%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 400/4290: AvgLoss: 0.86, S.Acc: 67.97%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 410/4290: AvgLoss: 0.86, S.Acc: 67.91%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 420/4290: AvgLoss: 0.86, S.Acc: 67.86%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 430/4290: AvgLoss: 0.86, S.Acc: 67.85%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 440/4290: AvgLoss: 0.86, S.Acc: 67.89%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 450/4290: AvgLoss: 0.87, S.Acc: 67.90%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 460/4290: AvgLoss: 0.87, S.Acc: 67.79%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 470/4290: AvgLoss: 0.87, S.Acc: 67.73%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 480/4290: AvgLoss: 0.87, S.Acc: 67.79%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 490/4290: AvgLoss: 0.87, S.Acc: 67.88%, E.Acc: 93.61%\n",
      "Epoch 1. Batch 500/4290: AvgLoss: 0.87, S.Acc: 67.79%, E.Acc: 93.56%\n",
      "Epoch 1. Batch 510/4290: AvgLoss: 0.87, S.Acc: 67.69%, E.Acc: 93.47%\n",
      "Epoch 1. Batch 520/4290: AvgLoss: 0.87, S.Acc: 67.71%, E.Acc: 93.43%\n",
      "Epoch 1. Batch 530/4290: AvgLoss: 0.87, S.Acc: 67.80%, E.Acc: 93.44%\n",
      "Epoch 1. Batch 540/4290: AvgLoss: 0.87, S.Acc: 67.72%, E.Acc: 93.47%\n",
      "Epoch 1. Batch 550/4290: AvgLoss: 0.87, S.Acc: 67.64%, E.Acc: 93.49%\n",
      "Epoch 1. Batch 560/4290: AvgLoss: 0.87, S.Acc: 67.71%, E.Acc: 93.48%\n",
      "Epoch 1. Batch 570/4290: AvgLoss: 0.87, S.Acc: 67.67%, E.Acc: 93.49%\n",
      "Epoch 1. Batch 580/4290: AvgLoss: 0.87, S.Acc: 67.71%, E.Acc: 93.48%\n",
      "Epoch 1. Batch 590/4290: AvgLoss: 0.88, S.Acc: 67.65%, E.Acc: 93.43%\n",
      "Epoch 1. Batch 600/4290: AvgLoss: 0.88, S.Acc: 67.71%, E.Acc: 93.42%\n",
      "Epoch 1. Batch 610/4290: AvgLoss: 0.88, S.Acc: 67.78%, E.Acc: 93.37%\n",
      "Epoch 1. Batch 620/4290: AvgLoss: 0.88, S.Acc: 67.73%, E.Acc: 93.41%\n",
      "Epoch 1. Batch 630/4290: AvgLoss: 0.88, S.Acc: 67.71%, E.Acc: 93.41%\n",
      "Epoch 1. Batch 640/4290: AvgLoss: 0.87, S.Acc: 67.77%, E.Acc: 93.41%\n",
      "Epoch 1. Batch 650/4290: AvgLoss: 0.87, S.Acc: 67.76%, E.Acc: 93.44%\n",
      "Epoch 1. Batch 660/4290: AvgLoss: 0.87, S.Acc: 67.80%, E.Acc: 93.47%\n",
      "Epoch 1. Batch 670/4290: AvgLoss: 0.87, S.Acc: 67.80%, E.Acc: 93.49%\n",
      "Epoch 1. Batch 680/4290: AvgLoss: 0.87, S.Acc: 67.84%, E.Acc: 93.54%\n",
      "Epoch 1. Batch 690/4290: AvgLoss: 0.87, S.Acc: 67.83%, E.Acc: 93.59%\n",
      "Epoch 1. Batch 700/4290: AvgLoss: 0.87, S.Acc: 67.82%, E.Acc: 93.57%\n",
      "Epoch 1. Batch 710/4290: AvgLoss: 0.87, S.Acc: 67.83%, E.Acc: 93.62%\n",
      "Epoch 1. Batch 720/4290: AvgLoss: 0.87, S.Acc: 67.81%, E.Acc: 93.59%\n",
      "Epoch 1. Batch 730/4290: AvgLoss: 0.87, S.Acc: 67.85%, E.Acc: 93.62%\n",
      "Epoch 1. Batch 740/4290: AvgLoss: 0.87, S.Acc: 67.81%, E.Acc: 93.62%\n",
      "Epoch 1. Batch 750/4290: AvgLoss: 0.87, S.Acc: 67.77%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 760/4290: AvgLoss: 0.87, S.Acc: 67.73%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 770/4290: AvgLoss: 0.87, S.Acc: 67.70%, E.Acc: 93.61%\n",
      "Epoch 1. Batch 780/4290: AvgLoss: 0.87, S.Acc: 67.65%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 790/4290: AvgLoss: 0.87, S.Acc: 67.58%, E.Acc: 93.62%\n",
      "Epoch 1. Batch 800/4290: AvgLoss: 0.87, S.Acc: 67.49%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 810/4290: AvgLoss: 0.87, S.Acc: 67.49%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 820/4290: AvgLoss: 0.87, S.Acc: 67.55%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 830/4290: AvgLoss: 0.87, S.Acc: 67.52%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 840/4290: AvgLoss: 0.87, S.Acc: 67.50%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 850/4290: AvgLoss: 0.87, S.Acc: 67.48%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 860/4290: AvgLoss: 0.87, S.Acc: 67.41%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 870/4290: AvgLoss: 0.87, S.Acc: 67.46%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 880/4290: AvgLoss: 0.87, S.Acc: 67.40%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 890/4290: AvgLoss: 0.87, S.Acc: 67.33%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 900/4290: AvgLoss: 0.87, S.Acc: 67.33%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 910/4290: AvgLoss: 0.87, S.Acc: 67.34%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 920/4290: AvgLoss: 0.87, S.Acc: 67.29%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 930/4290: AvgLoss: 0.87, S.Acc: 67.34%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 940/4290: AvgLoss: 0.87, S.Acc: 67.25%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 950/4290: AvgLoss: 0.87, S.Acc: 67.23%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 960/4290: AvgLoss: 0.87, S.Acc: 67.20%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 970/4290: AvgLoss: 0.87, S.Acc: 67.26%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 980/4290: AvgLoss: 0.88, S.Acc: 67.30%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 990/4290: AvgLoss: 0.88, S.Acc: 67.35%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1000/4290: AvgLoss: 0.87, S.Acc: 67.35%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1010/4290: AvgLoss: 0.88, S.Acc: 67.37%, E.Acc: 93.64%\n",
      "Epoch 1. Batch 1020/4290: AvgLoss: 0.88, S.Acc: 67.34%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 1030/4290: AvgLoss: 0.88, S.Acc: 67.33%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 1040/4290: AvgLoss: 0.88, S.Acc: 67.29%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1050/4290: AvgLoss: 0.87, S.Acc: 67.29%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1060/4290: AvgLoss: 0.87, S.Acc: 67.31%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1070/4290: AvgLoss: 0.87, S.Acc: 67.30%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1080/4290: AvgLoss: 0.87, S.Acc: 67.27%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1090/4290: AvgLoss: 0.87, S.Acc: 67.23%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1100/4290: AvgLoss: 0.88, S.Acc: 67.18%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1110/4290: AvgLoss: 0.88, S.Acc: 67.19%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 1120/4290: AvgLoss: 0.88, S.Acc: 67.18%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1130/4290: AvgLoss: 0.88, S.Acc: 67.22%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1140/4290: AvgLoss: 0.88, S.Acc: 67.21%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 1150/4290: AvgLoss: 0.88, S.Acc: 67.16%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1160/4290: AvgLoss: 0.88, S.Acc: 67.11%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1170/4290: AvgLoss: 0.87, S.Acc: 67.11%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1180/4290: AvgLoss: 0.87, S.Acc: 67.10%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 1190/4290: AvgLoss: 0.87, S.Acc: 67.09%, E.Acc: 93.79%\n",
      "Epoch 1. Batch 1200/4290: AvgLoss: 0.87, S.Acc: 67.07%, E.Acc: 93.79%\n",
      "Epoch 1. Batch 1210/4290: AvgLoss: 0.87, S.Acc: 67.09%, E.Acc: 93.80%\n",
      "Epoch 1. Batch 1220/4290: AvgLoss: 0.87, S.Acc: 67.08%, E.Acc: 93.80%\n",
      "Epoch 1. Batch 1230/4290: AvgLoss: 0.87, S.Acc: 67.07%, E.Acc: 93.80%\n",
      "Epoch 1. Batch 1240/4290: AvgLoss: 0.87, S.Acc: 67.06%, E.Acc: 93.81%\n",
      "Epoch 1. Batch 1250/4290: AvgLoss: 0.87, S.Acc: 67.06%, E.Acc: 93.79%\n",
      "Epoch 1. Batch 1260/4290: AvgLoss: 0.87, S.Acc: 67.01%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 1270/4290: AvgLoss: 0.87, S.Acc: 66.97%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1280/4290: AvgLoss: 0.88, S.Acc: 66.93%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 1290/4290: AvgLoss: 0.87, S.Acc: 66.94%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 1300/4290: AvgLoss: 0.87, S.Acc: 66.98%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1310/4290: AvgLoss: 0.88, S.Acc: 67.01%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1320/4290: AvgLoss: 0.88, S.Acc: 66.97%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1330/4290: AvgLoss: 0.88, S.Acc: 66.90%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1340/4290: AvgLoss: 0.88, S.Acc: 66.91%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1350/4290: AvgLoss: 0.88, S.Acc: 66.90%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1360/4290: AvgLoss: 0.88, S.Acc: 66.91%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1370/4290: AvgLoss: 0.88, S.Acc: 66.92%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1380/4290: AvgLoss: 0.88, S.Acc: 66.91%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1390/4290: AvgLoss: 0.88, S.Acc: 66.89%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1400/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1410/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1420/4290: AvgLoss: 0.88, S.Acc: 66.85%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 1430/4290: AvgLoss: 0.88, S.Acc: 66.84%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1440/4290: AvgLoss: 0.88, S.Acc: 66.82%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1450/4290: AvgLoss: 0.88, S.Acc: 66.77%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1460/4290: AvgLoss: 0.88, S.Acc: 66.77%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1470/4290: AvgLoss: 0.88, S.Acc: 66.77%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1480/4290: AvgLoss: 0.88, S.Acc: 66.76%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1490/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1500/4290: AvgLoss: 0.88, S.Acc: 66.76%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1510/4290: AvgLoss: 0.88, S.Acc: 66.75%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 1520/4290: AvgLoss: 0.88, S.Acc: 66.73%, E.Acc: 93.63%\n",
      "Epoch 1. Batch 1530/4290: AvgLoss: 0.88, S.Acc: 66.75%, E.Acc: 93.64%\n",
      "Epoch 1. Batch 1540/4290: AvgLoss: 0.88, S.Acc: 66.72%, E.Acc: 93.65%\n",
      "Epoch 1. Batch 1550/4290: AvgLoss: 0.88, S.Acc: 66.74%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1560/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.66%\n",
      "Epoch 1. Batch 1570/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1580/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1590/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 1600/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 1610/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 1620/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1630/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 1640/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 1650/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1660/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1670/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1680/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1690/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1700/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1710/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1720/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1730/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1740/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1750/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1760/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1770/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1780/4290: AvgLoss: 0.87, S.Acc: 66.93%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1790/4290: AvgLoss: 0.87, S.Acc: 66.94%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 1800/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1810/4290: AvgLoss: 0.87, S.Acc: 66.95%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1820/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1830/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1840/4290: AvgLoss: 0.87, S.Acc: 66.89%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1850/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1860/4290: AvgLoss: 0.87, S.Acc: 66.89%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1870/4290: AvgLoss: 0.87, S.Acc: 66.89%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1880/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1890/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1900/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 1910/4290: AvgLoss: 0.87, S.Acc: 66.87%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 1920/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1930/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1940/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1950/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 1960/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1970/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 1980/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 1990/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2000/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2010/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 2020/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2030/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2040/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2050/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2060/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2070/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2080/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2090/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2100/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2110/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2120/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2130/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2140/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2150/4290: AvgLoss: 0.87, S.Acc: 66.87%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2160/4290: AvgLoss: 0.87, S.Acc: 66.87%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2170/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2180/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2190/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2200/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2210/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2220/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2230/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2240/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 2250/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.79%\n",
      "Epoch 1. Batch 2260/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.79%\n",
      "Epoch 1. Batch 2270/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2280/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2290/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2300/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2310/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2320/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2330/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2340/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2350/4290: AvgLoss: 0.87, S.Acc: 66.71%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2360/4290: AvgLoss: 0.87, S.Acc: 66.71%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2370/4290: AvgLoss: 0.87, S.Acc: 66.72%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2380/4290: AvgLoss: 0.87, S.Acc: 66.73%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2390/4290: AvgLoss: 0.87, S.Acc: 66.73%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2400/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2410/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2420/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 2430/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 2440/4290: AvgLoss: 0.87, S.Acc: 66.72%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2450/4290: AvgLoss: 0.87, S.Acc: 66.70%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2460/4290: AvgLoss: 0.87, S.Acc: 66.72%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2470/4290: AvgLoss: 0.87, S.Acc: 66.71%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2480/4290: AvgLoss: 0.87, S.Acc: 66.70%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2490/4290: AvgLoss: 0.87, S.Acc: 66.68%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2500/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2510/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2520/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2530/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2540/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2550/4290: AvgLoss: 0.87, S.Acc: 66.64%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2560/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2570/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2580/4290: AvgLoss: 0.87, S.Acc: 66.68%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2590/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2600/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2610/4290: AvgLoss: 0.87, S.Acc: 66.69%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2620/4290: AvgLoss: 0.87, S.Acc: 66.69%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2630/4290: AvgLoss: 0.87, S.Acc: 66.70%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2640/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2650/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2660/4290: AvgLoss: 0.87, S.Acc: 66.68%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2670/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2680/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2690/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2700/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2710/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2720/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2730/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2740/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 2750/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2760/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2770/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2780/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 2790/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 2800/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 2810/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 2820/4290: AvgLoss: 0.87, S.Acc: 66.64%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 2830/4290: AvgLoss: 0.87, S.Acc: 66.65%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 2840/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2850/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2860/4290: AvgLoss: 0.87, S.Acc: 66.64%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2870/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2880/4290: AvgLoss: 0.87, S.Acc: 66.67%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2890/4290: AvgLoss: 0.87, S.Acc: 66.64%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2900/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2910/4290: AvgLoss: 0.87, S.Acc: 66.66%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2920/4290: AvgLoss: 0.87, S.Acc: 66.69%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2930/4290: AvgLoss: 0.87, S.Acc: 66.70%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2940/4290: AvgLoss: 0.88, S.Acc: 66.70%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2950/4290: AvgLoss: 0.88, S.Acc: 66.69%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2960/4290: AvgLoss: 0.88, S.Acc: 66.68%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2970/4290: AvgLoss: 0.88, S.Acc: 66.69%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 2980/4290: AvgLoss: 0.88, S.Acc: 66.69%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 2990/4290: AvgLoss: 0.87, S.Acc: 66.69%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3000/4290: AvgLoss: 0.87, S.Acc: 66.70%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3010/4290: AvgLoss: 0.87, S.Acc: 66.71%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3020/4290: AvgLoss: 0.87, S.Acc: 66.71%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3030/4290: AvgLoss: 0.88, S.Acc: 66.71%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3040/4290: AvgLoss: 0.87, S.Acc: 66.73%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3050/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3060/4290: AvgLoss: 0.87, S.Acc: 66.72%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3070/4290: AvgLoss: 0.87, S.Acc: 66.73%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3080/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3090/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3100/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3110/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3120/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3130/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3140/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 3150/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 3160/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.67%\n",
      "Epoch 1. Batch 3170/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 3180/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 3190/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.68%\n",
      "Epoch 1. Batch 3200/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.69%\n",
      "Epoch 1. Batch 3210/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.70%\n",
      "Epoch 1. Batch 3220/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3230/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3240/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3250/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3260/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3270/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3280/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3290/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3300/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3310/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3320/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3330/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3340/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3350/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3360/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3370/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3380/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3390/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3400/4290: AvgLoss: 0.87, S.Acc: 66.73%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3410/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3420/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3430/4290: AvgLoss: 0.87, S.Acc: 66.74%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3440/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3450/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3460/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3470/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3480/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3490/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3500/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3510/4290: AvgLoss: 0.87, S.Acc: 66.75%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3520/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3530/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3540/4290: AvgLoss: 0.87, S.Acc: 66.77%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3550/4290: AvgLoss: 0.87, S.Acc: 66.76%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3560/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3570/4290: AvgLoss: 0.87, S.Acc: 66.78%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3580/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3590/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3600/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3610/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3620/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.71%\n",
      "Epoch 1. Batch 3630/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3640/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.72%\n",
      "Epoch 1. Batch 3650/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3660/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3670/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3680/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3690/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3700/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3710/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3720/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3730/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3740/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.73%\n",
      "Epoch 1. Batch 3750/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3760/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3770/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3780/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3790/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3800/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3810/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.74%\n",
      "Epoch 1. Batch 3820/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3830/4290: AvgLoss: 0.87, S.Acc: 66.79%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3840/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3850/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3860/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3870/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3880/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3890/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 3900/4290: AvgLoss: 0.87, S.Acc: 66.82%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3910/4290: AvgLoss: 0.87, S.Acc: 66.80%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3920/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3930/4290: AvgLoss: 0.87, S.Acc: 66.81%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3940/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3950/4290: AvgLoss: 0.87, S.Acc: 66.83%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3960/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3970/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 3980/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 3990/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4000/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4010/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4020/4290: AvgLoss: 0.87, S.Acc: 66.85%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4030/4290: AvgLoss: 0.87, S.Acc: 66.84%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4040/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4050/4290: AvgLoss: 0.87, S.Acc: 66.86%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4060/4290: AvgLoss: 0.87, S.Acc: 66.87%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4070/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4080/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4090/4290: AvgLoss: 0.87, S.Acc: 66.89%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4100/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4110/4290: AvgLoss: 0.87, S.Acc: 66.89%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4120/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4130/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4140/4290: AvgLoss: 0.87, S.Acc: 66.88%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4150/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4160/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4170/4290: AvgLoss: 0.87, S.Acc: 66.90%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 4180/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4190/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 4200/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 4210/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.75%\n",
      "Epoch 1. Batch 4220/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.76%\n",
      "Epoch 1. Batch 4230/4290: AvgLoss: 0.87, S.Acc: 66.93%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4240/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4250/4290: AvgLoss: 0.87, S.Acc: 66.91%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4260/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.78%\n",
      "Epoch 1. Batch 4270/4290: AvgLoss: 0.87, S.Acc: 66.93%, E.Acc: 93.77%\n",
      "Epoch 1. Batch 4280/4290: AvgLoss: 0.87, S.Acc: 66.92%, E.Acc: 93.77%\n",
      "Train Loss: 0.8698, Train Sentiment Accuracy: 66.92%, Train Emotion Accuracy: 93.77%\n",
      "Validation Sentiment Accuracy: 64.98%, Validation Emotion Accuracy: 94.36%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_acc_sent, train_acc_emot = train_epoch(\n",
    "        model, train_loader, loss_fn_sentiment, loss_fn_emotion, optimizer, device, epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Sentiment Accuracy: {train_acc_sent:.2f}%, \"\n",
    "          f\"Train Emotion Accuracy: {train_acc_emot:.2f}%\")\n",
    "    val_acc_sent, val_acc_emot = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation Sentiment Accuracy: {val_acc_sent:.2f}%, Validation Emotion Accuracy: {val_acc_emot:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentiment Accuracy: 65.46%\n",
      "Test Emotion Accuracy: 94.15%\n"
     ]
    }
   ],
   "source": [
    "test_acc_sent, test_acc_emot = eval_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Test Sentiment Accuracy: {test_acc_sent:.2f}%\")\n",
    "print(f\"Test Emotion Accuracy: {test_acc_emot:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy Sentiment | Validation Accuracy Sentiment | Train Accuracy Emotion | Validation Accuracy Emotion |\n",
    "|--------------|--------------------------|-------------------------------|------------------------|-----------------------------|\n",
    "| **Epoch 1**  | 54.38%                   | 63.41%                        | 77.53%                 | 94.12%                      |\n",
    "| **Epoch 2**  | 66.92%                   | 64.98%                        | 93.77%                 | 94.36%                      |\n",
    "\n",
    "\n",
    "### Observation\n",
    "* The training accuracy for both sentiment and emotion classification steadily improves over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './GRU_multitask_model/gru_multitask_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
