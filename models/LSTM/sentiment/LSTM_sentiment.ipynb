{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: LSTM (sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a5a3df2e3ce62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        pooled = torch.mean(lstm_out, dim=1)\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(self.relu(dropped))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4317 - Avg Loss: 1.1358 - Accuracy: 6.25%\n",
      "Epoch: 0. Batch 10/4317 - Avg Loss: 1.1217 - Accuracy: 36.93%\n",
      "Epoch: 0. Batch 20/4317 - Avg Loss: 1.1139 - Accuracy: 39.88%\n",
      "Epoch: 0. Batch 30/4317 - Avg Loss: 1.1159 - Accuracy: 36.09%\n",
      "Epoch: 0. Batch 40/4317 - Avg Loss: 1.1124 - Accuracy: 34.15%\n",
      "Epoch: 0. Batch 50/4317 - Avg Loss: 1.1172 - Accuracy: 32.11%\n",
      "Epoch: 0. Batch 60/4317 - Avg Loss: 1.1155 - Accuracy: 32.68%\n",
      "Epoch: 0. Batch 70/4317 - Avg Loss: 1.1135 - Accuracy: 33.63%\n",
      "Epoch: 0. Batch 80/4317 - Avg Loss: 1.1120 - Accuracy: 34.41%\n",
      "Epoch: 0. Batch 90/4317 - Avg Loss: 1.1112 - Accuracy: 34.96%\n",
      "Epoch: 0. Batch 100/4317 - Avg Loss: 1.1091 - Accuracy: 35.40%\n",
      "Epoch: 0. Batch 110/4317 - Avg Loss: 1.1071 - Accuracy: 35.47%\n",
      "Epoch: 0. Batch 120/4317 - Avg Loss: 1.1094 - Accuracy: 34.92%\n",
      "Epoch: 0. Batch 130/4317 - Avg Loss: 1.1099 - Accuracy: 34.21%\n",
      "Epoch: 0. Batch 140/4317 - Avg Loss: 1.1091 - Accuracy: 34.44%\n",
      "Epoch: 0. Batch 150/4317 - Avg Loss: 1.1091 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 160/4317 - Avg Loss: 1.1088 - Accuracy: 34.43%\n",
      "Epoch: 0. Batch 170/4317 - Avg Loss: 1.1083 - Accuracy: 34.83%\n",
      "Epoch: 0. Batch 180/4317 - Avg Loss: 1.1085 - Accuracy: 35.22%\n",
      "Epoch: 0. Batch 190/4317 - Avg Loss: 1.1083 - Accuracy: 35.80%\n",
      "Epoch: 0. Batch 200/4317 - Avg Loss: 1.1077 - Accuracy: 36.07%\n",
      "Epoch: 0. Batch 210/4317 - Avg Loss: 1.1084 - Accuracy: 36.02%\n",
      "Epoch: 0. Batch 220/4317 - Avg Loss: 1.1082 - Accuracy: 35.52%\n",
      "Epoch: 0. Batch 230/4317 - Avg Loss: 1.1077 - Accuracy: 35.28%\n",
      "Epoch: 0. Batch 240/4317 - Avg Loss: 1.1075 - Accuracy: 34.88%\n",
      "Epoch: 0. Batch 250/4317 - Avg Loss: 1.1074 - Accuracy: 34.64%\n",
      "Epoch: 0. Batch 260/4317 - Avg Loss: 1.1070 - Accuracy: 34.77%\n",
      "Epoch: 0. Batch 270/4317 - Avg Loss: 1.1071 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 280/4317 - Avg Loss: 1.1071 - Accuracy: 34.43%\n",
      "Epoch: 0. Batch 290/4317 - Avg Loss: 1.1069 - Accuracy: 34.21%\n",
      "Epoch: 0. Batch 300/4317 - Avg Loss: 1.1064 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 310/4317 - Avg Loss: 1.1061 - Accuracy: 34.18%\n",
      "Epoch: 0. Batch 320/4317 - Avg Loss: 1.1058 - Accuracy: 34.15%\n",
      "Epoch: 0. Batch 330/4317 - Avg Loss: 1.1055 - Accuracy: 34.04%\n",
      "Epoch: 0. Batch 340/4317 - Avg Loss: 1.1054 - Accuracy: 34.05%\n",
      "Epoch: 0. Batch 350/4317 - Avg Loss: 1.1053 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 360/4317 - Avg Loss: 1.1053 - Accuracy: 34.26%\n",
      "Epoch: 0. Batch 370/4317 - Avg Loss: 1.1050 - Accuracy: 34.28%\n",
      "Epoch: 0. Batch 380/4317 - Avg Loss: 1.1043 - Accuracy: 34.40%\n",
      "Epoch: 0. Batch 390/4317 - Avg Loss: 1.1044 - Accuracy: 34.29%\n",
      "Epoch: 0. Batch 400/4317 - Avg Loss: 1.1043 - Accuracy: 34.09%\n",
      "Epoch: 0. Batch 410/4317 - Avg Loss: 1.1041 - Accuracy: 34.11%\n",
      "Epoch: 0. Batch 420/4317 - Avg Loss: 1.1038 - Accuracy: 34.23%\n",
      "Epoch: 0. Batch 430/4317 - Avg Loss: 1.1037 - Accuracy: 34.47%\n",
      "Epoch: 0. Batch 440/4317 - Avg Loss: 1.1035 - Accuracy: 34.59%\n",
      "Epoch: 0. Batch 450/4317 - Avg Loss: 1.1029 - Accuracy: 34.69%\n",
      "Epoch: 0. Batch 460/4317 - Avg Loss: 1.1027 - Accuracy: 34.68%\n",
      "Epoch: 0. Batch 470/4317 - Avg Loss: 1.1024 - Accuracy: 34.86%\n",
      "Epoch: 0. Batch 480/4317 - Avg Loss: 1.1015 - Accuracy: 35.06%\n",
      "Epoch: 0. Batch 490/4317 - Avg Loss: 1.1013 - Accuracy: 35.13%\n",
      "Epoch: 0. Batch 500/4317 - Avg Loss: 1.1009 - Accuracy: 35.25%\n",
      "Epoch: 0. Batch 510/4317 - Avg Loss: 1.1008 - Accuracy: 35.25%\n",
      "Epoch: 0. Batch 520/4317 - Avg Loss: 1.1008 - Accuracy: 35.21%\n",
      "Epoch: 0. Batch 530/4317 - Avg Loss: 1.0997 - Accuracy: 35.36%\n",
      "Epoch: 0. Batch 540/4317 - Avg Loss: 1.0988 - Accuracy: 35.49%\n",
      "Epoch: 0. Batch 550/4317 - Avg Loss: 1.0988 - Accuracy: 35.55%\n",
      "Epoch: 0. Batch 560/4317 - Avg Loss: 1.0984 - Accuracy: 35.70%\n",
      "Epoch: 0. Batch 570/4317 - Avg Loss: 1.0978 - Accuracy: 35.89%\n",
      "Epoch: 0. Batch 580/4317 - Avg Loss: 1.0976 - Accuracy: 36.03%\n",
      "Epoch: 0. Batch 590/4317 - Avg Loss: 1.0957 - Accuracy: 36.28%\n",
      "Epoch: 0. Batch 600/4317 - Avg Loss: 1.0949 - Accuracy: 36.40%\n",
      "Epoch: 0. Batch 610/4317 - Avg Loss: 1.0942 - Accuracy: 36.57%\n",
      "Epoch: 0. Batch 620/4317 - Avg Loss: 1.0934 - Accuracy: 36.67%\n",
      "Epoch: 0. Batch 630/4317 - Avg Loss: 1.0925 - Accuracy: 36.80%\n",
      "Epoch: 0. Batch 640/4317 - Avg Loss: 1.0918 - Accuracy: 36.85%\n",
      "Epoch: 0. Batch 650/4317 - Avg Loss: 1.0903 - Accuracy: 37.06%\n",
      "Epoch: 0. Batch 660/4317 - Avg Loss: 1.0890 - Accuracy: 37.21%\n",
      "Epoch: 0. Batch 670/4317 - Avg Loss: 1.0883 - Accuracy: 37.31%\n",
      "Epoch: 0. Batch 680/4317 - Avg Loss: 1.0876 - Accuracy: 37.48%\n",
      "Epoch: 0. Batch 690/4317 - Avg Loss: 1.0868 - Accuracy: 37.55%\n",
      "Epoch: 0. Batch 700/4317 - Avg Loss: 1.0860 - Accuracy: 37.65%\n",
      "Epoch: 0. Batch 710/4317 - Avg Loss: 1.0859 - Accuracy: 37.75%\n",
      "Epoch: 0. Batch 720/4317 - Avg Loss: 1.0846 - Accuracy: 37.86%\n",
      "Epoch: 0. Batch 730/4317 - Avg Loss: 1.0833 - Accuracy: 38.02%\n",
      "Epoch: 0. Batch 740/4317 - Avg Loss: 1.0829 - Accuracy: 37.95%\n",
      "Epoch: 0. Batch 750/4317 - Avg Loss: 1.0825 - Accuracy: 38.02%\n",
      "Epoch: 0. Batch 760/4317 - Avg Loss: 1.0818 - Accuracy: 38.09%\n",
      "Epoch: 0. Batch 770/4317 - Avg Loss: 1.0804 - Accuracy: 38.28%\n",
      "Epoch: 0. Batch 780/4317 - Avg Loss: 1.0792 - Accuracy: 38.42%\n",
      "Epoch: 0. Batch 790/4317 - Avg Loss: 1.0778 - Accuracy: 38.59%\n",
      "Epoch: 0. Batch 800/4317 - Avg Loss: 1.0760 - Accuracy: 38.76%\n",
      "Epoch: 0. Batch 810/4317 - Avg Loss: 1.0754 - Accuracy: 38.86%\n",
      "Epoch: 0. Batch 820/4317 - Avg Loss: 1.0738 - Accuracy: 39.08%\n",
      "Epoch: 0. Batch 830/4317 - Avg Loss: 1.0730 - Accuracy: 39.17%\n",
      "Epoch: 0. Batch 840/4317 - Avg Loss: 1.0716 - Accuracy: 39.25%\n",
      "Epoch: 0. Batch 850/4317 - Avg Loss: 1.0704 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 860/4317 - Avg Loss: 1.0693 - Accuracy: 39.49%\n",
      "Epoch: 0. Batch 870/4317 - Avg Loss: 1.0683 - Accuracy: 39.67%\n",
      "Epoch: 0. Batch 880/4317 - Avg Loss: 1.0668 - Accuracy: 39.91%\n",
      "Epoch: 0. Batch 890/4317 - Avg Loss: 1.0660 - Accuracy: 39.98%\n",
      "Epoch: 0. Batch 900/4317 - Avg Loss: 1.0662 - Accuracy: 40.05%\n",
      "Epoch: 0. Batch 910/4317 - Avg Loss: 1.0654 - Accuracy: 40.13%\n",
      "Epoch: 0. Batch 920/4317 - Avg Loss: 1.0642 - Accuracy: 40.30%\n",
      "Epoch: 0. Batch 930/4317 - Avg Loss: 1.0641 - Accuracy: 40.33%\n",
      "Epoch: 0. Batch 940/4317 - Avg Loss: 1.0635 - Accuracy: 40.39%\n",
      "Epoch: 0. Batch 950/4317 - Avg Loss: 1.0620 - Accuracy: 40.58%\n",
      "Epoch: 0. Batch 960/4317 - Avg Loss: 1.0613 - Accuracy: 40.68%\n",
      "Epoch: 0. Batch 970/4317 - Avg Loss: 1.0602 - Accuracy: 40.78%\n",
      "Epoch: 0. Batch 980/4317 - Avg Loss: 1.0602 - Accuracy: 40.80%\n",
      "Epoch: 0. Batch 990/4317 - Avg Loss: 1.0591 - Accuracy: 40.92%\n",
      "Epoch: 0. Batch 1000/4317 - Avg Loss: 1.0575 - Accuracy: 41.09%\n",
      "Epoch: 0. Batch 1010/4317 - Avg Loss: 1.0565 - Accuracy: 41.23%\n",
      "Epoch: 0. Batch 1020/4317 - Avg Loss: 1.0555 - Accuracy: 41.28%\n",
      "Epoch: 0. Batch 1030/4317 - Avg Loss: 1.0546 - Accuracy: 41.40%\n",
      "Epoch: 0. Batch 1040/4317 - Avg Loss: 1.0530 - Accuracy: 41.55%\n",
      "Epoch: 0. Batch 1050/4317 - Avg Loss: 1.0523 - Accuracy: 41.65%\n",
      "Epoch: 0. Batch 1060/4317 - Avg Loss: 1.0520 - Accuracy: 41.71%\n",
      "Epoch: 0. Batch 1070/4317 - Avg Loss: 1.0510 - Accuracy: 41.82%\n",
      "Epoch: 0. Batch 1080/4317 - Avg Loss: 1.0509 - Accuracy: 41.89%\n",
      "Epoch: 0. Batch 1090/4317 - Avg Loss: 1.0500 - Accuracy: 42.01%\n",
      "Epoch: 0. Batch 1100/4317 - Avg Loss: 1.0490 - Accuracy: 42.12%\n",
      "Epoch: 0. Batch 1110/4317 - Avg Loss: 1.0483 - Accuracy: 42.18%\n",
      "Epoch: 0. Batch 1120/4317 - Avg Loss: 1.0471 - Accuracy: 42.31%\n",
      "Epoch: 0. Batch 1130/4317 - Avg Loss: 1.0457 - Accuracy: 42.44%\n",
      "Epoch: 0. Batch 1140/4317 - Avg Loss: 1.0450 - Accuracy: 42.47%\n",
      "Epoch: 0. Batch 1150/4317 - Avg Loss: 1.0437 - Accuracy: 42.62%\n",
      "Epoch: 0. Batch 1160/4317 - Avg Loss: 1.0427 - Accuracy: 42.69%\n",
      "Epoch: 0. Batch 1170/4317 - Avg Loss: 1.0420 - Accuracy: 42.77%\n",
      "Epoch: 0. Batch 1180/4317 - Avg Loss: 1.0412 - Accuracy: 42.86%\n",
      "Epoch: 0. Batch 1190/4317 - Avg Loss: 1.0399 - Accuracy: 42.96%\n",
      "Epoch: 0. Batch 1200/4317 - Avg Loss: 1.0391 - Accuracy: 43.05%\n",
      "Epoch: 0. Batch 1210/4317 - Avg Loss: 1.0387 - Accuracy: 43.14%\n",
      "Epoch: 0. Batch 1220/4317 - Avg Loss: 1.0382 - Accuracy: 43.22%\n",
      "Epoch: 0. Batch 1230/4317 - Avg Loss: 1.0371 - Accuracy: 43.33%\n",
      "Epoch: 0. Batch 1240/4317 - Avg Loss: 1.0361 - Accuracy: 43.40%\n",
      "Epoch: 0. Batch 1250/4317 - Avg Loss: 1.0344 - Accuracy: 43.49%\n",
      "Epoch: 0. Batch 1260/4317 - Avg Loss: 1.0334 - Accuracy: 43.56%\n",
      "Epoch: 0. Batch 1270/4317 - Avg Loss: 1.0329 - Accuracy: 43.61%\n",
      "Epoch: 0. Batch 1280/4317 - Avg Loss: 1.0324 - Accuracy: 43.70%\n",
      "Epoch: 0. Batch 1290/4317 - Avg Loss: 1.0313 - Accuracy: 43.82%\n",
      "Epoch: 0. Batch 1300/4317 - Avg Loss: 1.0309 - Accuracy: 43.86%\n",
      "Epoch: 0. Batch 1310/4317 - Avg Loss: 1.0299 - Accuracy: 43.95%\n",
      "Epoch: 0. Batch 1320/4317 - Avg Loss: 1.0290 - Accuracy: 44.05%\n",
      "Epoch: 0. Batch 1330/4317 - Avg Loss: 1.0276 - Accuracy: 44.20%\n",
      "Epoch: 0. Batch 1340/4317 - Avg Loss: 1.0268 - Accuracy: 44.27%\n",
      "Epoch: 0. Batch 1350/4317 - Avg Loss: 1.0264 - Accuracy: 44.35%\n",
      "Epoch: 0. Batch 1360/4317 - Avg Loss: 1.0257 - Accuracy: 44.41%\n",
      "Epoch: 0. Batch 1370/4317 - Avg Loss: 1.0249 - Accuracy: 44.47%\n",
      "Epoch: 0. Batch 1380/4317 - Avg Loss: 1.0243 - Accuracy: 44.53%\n",
      "Epoch: 0. Batch 1390/4317 - Avg Loss: 1.0235 - Accuracy: 44.59%\n",
      "Epoch: 0. Batch 1400/4317 - Avg Loss: 1.0229 - Accuracy: 44.68%\n",
      "Epoch: 0. Batch 1410/4317 - Avg Loss: 1.0224 - Accuracy: 44.74%\n",
      "Epoch: 0. Batch 1420/4317 - Avg Loss: 1.0214 - Accuracy: 44.83%\n",
      "Epoch: 0. Batch 1430/4317 - Avg Loss: 1.0206 - Accuracy: 44.86%\n",
      "Epoch: 0. Batch 1440/4317 - Avg Loss: 1.0198 - Accuracy: 44.93%\n",
      "Epoch: 0. Batch 1450/4317 - Avg Loss: 1.0189 - Accuracy: 45.00%\n",
      "Epoch: 0. Batch 1460/4317 - Avg Loss: 1.0182 - Accuracy: 45.07%\n",
      "Epoch: 0. Batch 1470/4317 - Avg Loss: 1.0178 - Accuracy: 45.14%\n",
      "Epoch: 0. Batch 1480/4317 - Avg Loss: 1.0165 - Accuracy: 45.22%\n",
      "Epoch: 0. Batch 1490/4317 - Avg Loss: 1.0162 - Accuracy: 45.28%\n",
      "Epoch: 0. Batch 1500/4317 - Avg Loss: 1.0154 - Accuracy: 45.32%\n",
      "Epoch: 0. Batch 1510/4317 - Avg Loss: 1.0147 - Accuracy: 45.38%\n",
      "Epoch: 0. Batch 1520/4317 - Avg Loss: 1.0136 - Accuracy: 45.46%\n",
      "Epoch: 0. Batch 1530/4317 - Avg Loss: 1.0124 - Accuracy: 45.56%\n",
      "Epoch: 0. Batch 1540/4317 - Avg Loss: 1.0119 - Accuracy: 45.64%\n",
      "Epoch: 0. Batch 1550/4317 - Avg Loss: 1.0114 - Accuracy: 45.73%\n",
      "Epoch: 0. Batch 1560/4317 - Avg Loss: 1.0103 - Accuracy: 45.81%\n",
      "Epoch: 0. Batch 1570/4317 - Avg Loss: 1.0096 - Accuracy: 45.89%\n",
      "Epoch: 0. Batch 1580/4317 - Avg Loss: 1.0090 - Accuracy: 45.95%\n",
      "Epoch: 0. Batch 1590/4317 - Avg Loss: 1.0083 - Accuracy: 46.03%\n",
      "Epoch: 0. Batch 1600/4317 - Avg Loss: 1.0077 - Accuracy: 46.09%\n",
      "Epoch: 0. Batch 1610/4317 - Avg Loss: 1.0070 - Accuracy: 46.14%\n",
      "Epoch: 0. Batch 1620/4317 - Avg Loss: 1.0061 - Accuracy: 46.21%\n",
      "Epoch: 0. Batch 1630/4317 - Avg Loss: 1.0056 - Accuracy: 46.27%\n",
      "Epoch: 0. Batch 1640/4317 - Avg Loss: 1.0052 - Accuracy: 46.31%\n",
      "Epoch: 0. Batch 1650/4317 - Avg Loss: 1.0042 - Accuracy: 46.40%\n",
      "Epoch: 0. Batch 1660/4317 - Avg Loss: 1.0033 - Accuracy: 46.51%\n",
      "Epoch: 0. Batch 1670/4317 - Avg Loss: 1.0020 - Accuracy: 46.59%\n",
      "Epoch: 0. Batch 1680/4317 - Avg Loss: 1.0009 - Accuracy: 46.66%\n",
      "Epoch: 0. Batch 1690/4317 - Avg Loss: 1.0002 - Accuracy: 46.70%\n",
      "Epoch: 0. Batch 1700/4317 - Avg Loss: 0.9992 - Accuracy: 46.77%\n",
      "Epoch: 0. Batch 1710/4317 - Avg Loss: 0.9988 - Accuracy: 46.82%\n",
      "Epoch: 0. Batch 1720/4317 - Avg Loss: 0.9976 - Accuracy: 46.91%\n",
      "Epoch: 0. Batch 1730/4317 - Avg Loss: 0.9966 - Accuracy: 47.03%\n",
      "Epoch: 0. Batch 1740/4317 - Avg Loss: 0.9961 - Accuracy: 47.05%\n",
      "Epoch: 0. Batch 1750/4317 - Avg Loss: 0.9967 - Accuracy: 47.04%\n",
      "Epoch: 0. Batch 1760/4317 - Avg Loss: 0.9959 - Accuracy: 47.12%\n",
      "Epoch: 0. Batch 1770/4317 - Avg Loss: 0.9952 - Accuracy: 47.20%\n",
      "Epoch: 0. Batch 1780/4317 - Avg Loss: 0.9949 - Accuracy: 47.22%\n",
      "Epoch: 0. Batch 1790/4317 - Avg Loss: 0.9942 - Accuracy: 47.33%\n",
      "Epoch: 0. Batch 1800/4317 - Avg Loss: 0.9936 - Accuracy: 47.37%\n",
      "Epoch: 0. Batch 1810/4317 - Avg Loss: 0.9930 - Accuracy: 47.45%\n",
      "Epoch: 0. Batch 1820/4317 - Avg Loss: 0.9920 - Accuracy: 47.50%\n",
      "Epoch: 0. Batch 1830/4317 - Avg Loss: 0.9915 - Accuracy: 47.56%\n",
      "Epoch: 0. Batch 1840/4317 - Avg Loss: 0.9906 - Accuracy: 47.62%\n",
      "Epoch: 0. Batch 1850/4317 - Avg Loss: 0.9901 - Accuracy: 47.69%\n",
      "Epoch: 0. Batch 1860/4317 - Avg Loss: 0.9896 - Accuracy: 47.74%\n",
      "Epoch: 0. Batch 1870/4317 - Avg Loss: 0.9887 - Accuracy: 47.81%\n",
      "Epoch: 0. Batch 1880/4317 - Avg Loss: 0.9882 - Accuracy: 47.88%\n",
      "Epoch: 0. Batch 1890/4317 - Avg Loss: 0.9876 - Accuracy: 47.93%\n",
      "Epoch: 0. Batch 1900/4317 - Avg Loss: 0.9868 - Accuracy: 47.98%\n",
      "Epoch: 0. Batch 1910/4317 - Avg Loss: 0.9865 - Accuracy: 48.00%\n",
      "Epoch: 0. Batch 1920/4317 - Avg Loss: 0.9865 - Accuracy: 48.02%\n",
      "Epoch: 0. Batch 1930/4317 - Avg Loss: 0.9856 - Accuracy: 48.08%\n",
      "Epoch: 0. Batch 1940/4317 - Avg Loss: 0.9847 - Accuracy: 48.15%\n",
      "Epoch: 0. Batch 1950/4317 - Avg Loss: 0.9841 - Accuracy: 48.22%\n",
      "Epoch: 0. Batch 1960/4317 - Avg Loss: 0.9832 - Accuracy: 48.29%\n",
      "Epoch: 0. Batch 1970/4317 - Avg Loss: 0.9829 - Accuracy: 48.33%\n",
      "Epoch: 0. Batch 1980/4317 - Avg Loss: 0.9825 - Accuracy: 48.39%\n",
      "Epoch: 0. Batch 1990/4317 - Avg Loss: 0.9820 - Accuracy: 48.45%\n",
      "Epoch: 0. Batch 2000/4317 - Avg Loss: 0.9814 - Accuracy: 48.48%\n",
      "Epoch: 0. Batch 2010/4317 - Avg Loss: 0.9808 - Accuracy: 48.54%\n",
      "Epoch: 0. Batch 2020/4317 - Avg Loss: 0.9800 - Accuracy: 48.59%\n",
      "Epoch: 0. Batch 2030/4317 - Avg Loss: 0.9794 - Accuracy: 48.63%\n",
      "Epoch: 0. Batch 2040/4317 - Avg Loss: 0.9785 - Accuracy: 48.68%\n",
      "Epoch: 0. Batch 2050/4317 - Avg Loss: 0.9781 - Accuracy: 48.71%\n",
      "Epoch: 0. Batch 2060/4317 - Avg Loss: 0.9775 - Accuracy: 48.77%\n",
      "Epoch: 0. Batch 2070/4317 - Avg Loss: 0.9770 - Accuracy: 48.83%\n",
      "Epoch: 0. Batch 2080/4317 - Avg Loss: 0.9767 - Accuracy: 48.88%\n",
      "Epoch: 0. Batch 2090/4317 - Avg Loss: 0.9761 - Accuracy: 48.92%\n",
      "Epoch: 0. Batch 2100/4317 - Avg Loss: 0.9753 - Accuracy: 48.96%\n",
      "Epoch: 0. Batch 2110/4317 - Avg Loss: 0.9749 - Accuracy: 49.00%\n",
      "Epoch: 0. Batch 2120/4317 - Avg Loss: 0.9745 - Accuracy: 49.00%\n",
      "Epoch: 0. Batch 2130/4317 - Avg Loss: 0.9741 - Accuracy: 49.02%\n",
      "Epoch: 0. Batch 2140/4317 - Avg Loss: 0.9734 - Accuracy: 49.09%\n",
      "Epoch: 0. Batch 2150/4317 - Avg Loss: 0.9732 - Accuracy: 49.13%\n",
      "Epoch: 0. Batch 2160/4317 - Avg Loss: 0.9729 - Accuracy: 49.21%\n",
      "Epoch: 0. Batch 2170/4317 - Avg Loss: 0.9728 - Accuracy: 49.22%\n",
      "Epoch: 0. Batch 2180/4317 - Avg Loss: 0.9722 - Accuracy: 49.29%\n",
      "Epoch: 0. Batch 2190/4317 - Avg Loss: 0.9715 - Accuracy: 49.32%\n",
      "Epoch: 0. Batch 2200/4317 - Avg Loss: 0.9711 - Accuracy: 49.34%\n",
      "Epoch: 0. Batch 2210/4317 - Avg Loss: 0.9708 - Accuracy: 49.39%\n",
      "Epoch: 0. Batch 2220/4317 - Avg Loss: 0.9703 - Accuracy: 49.44%\n",
      "Epoch: 0. Batch 2230/4317 - Avg Loss: 0.9697 - Accuracy: 49.47%\n",
      "Epoch: 0. Batch 2240/4317 - Avg Loss: 0.9689 - Accuracy: 49.51%\n",
      "Epoch: 0. Batch 2250/4317 - Avg Loss: 0.9682 - Accuracy: 49.56%\n",
      "Epoch: 0. Batch 2260/4317 - Avg Loss: 0.9679 - Accuracy: 49.59%\n",
      "Epoch: 0. Batch 2270/4317 - Avg Loss: 0.9673 - Accuracy: 49.66%\n",
      "Epoch: 0. Batch 2280/4317 - Avg Loss: 0.9666 - Accuracy: 49.73%\n",
      "Epoch: 0. Batch 2290/4317 - Avg Loss: 0.9662 - Accuracy: 49.74%\n",
      "Epoch: 0. Batch 2300/4317 - Avg Loss: 0.9655 - Accuracy: 49.79%\n",
      "Epoch: 0. Batch 2310/4317 - Avg Loss: 0.9651 - Accuracy: 49.81%\n",
      "Epoch: 0. Batch 2320/4317 - Avg Loss: 0.9646 - Accuracy: 49.83%\n",
      "Epoch: 0. Batch 2330/4317 - Avg Loss: 0.9640 - Accuracy: 49.92%\n",
      "Epoch: 0. Batch 2340/4317 - Avg Loss: 0.9641 - Accuracy: 49.94%\n",
      "Epoch: 0. Batch 2350/4317 - Avg Loss: 0.9631 - Accuracy: 50.00%\n",
      "Epoch: 0. Batch 2360/4317 - Avg Loss: 0.9628 - Accuracy: 50.04%\n",
      "Epoch: 0. Batch 2370/4317 - Avg Loss: 0.9623 - Accuracy: 50.08%\n",
      "Epoch: 0. Batch 2380/4317 - Avg Loss: 0.9622 - Accuracy: 50.12%\n",
      "Epoch: 0. Batch 2390/4317 - Avg Loss: 0.9617 - Accuracy: 50.18%\n",
      "Epoch: 0. Batch 2400/4317 - Avg Loss: 0.9611 - Accuracy: 50.24%\n",
      "Epoch: 0. Batch 2410/4317 - Avg Loss: 0.9605 - Accuracy: 50.29%\n",
      "Epoch: 0. Batch 2420/4317 - Avg Loss: 0.9598 - Accuracy: 50.31%\n",
      "Epoch: 0. Batch 2430/4317 - Avg Loss: 0.9594 - Accuracy: 50.33%\n",
      "Epoch: 0. Batch 2440/4317 - Avg Loss: 0.9586 - Accuracy: 50.38%\n",
      "Epoch: 0. Batch 2450/4317 - Avg Loss: 0.9579 - Accuracy: 50.43%\n",
      "Epoch: 0. Batch 2460/4317 - Avg Loss: 0.9573 - Accuracy: 50.47%\n",
      "Epoch: 0. Batch 2470/4317 - Avg Loss: 0.9571 - Accuracy: 50.50%\n",
      "Epoch: 0. Batch 2480/4317 - Avg Loss: 0.9562 - Accuracy: 50.58%\n",
      "Epoch: 0. Batch 2490/4317 - Avg Loss: 0.9556 - Accuracy: 50.60%\n",
      "Epoch: 0. Batch 2500/4317 - Avg Loss: 0.9549 - Accuracy: 50.66%\n",
      "Epoch: 0. Batch 2510/4317 - Avg Loss: 0.9543 - Accuracy: 50.71%\n",
      "Epoch: 0. Batch 2520/4317 - Avg Loss: 0.9539 - Accuracy: 50.74%\n",
      "Epoch: 0. Batch 2530/4317 - Avg Loss: 0.9532 - Accuracy: 50.80%\n",
      "Epoch: 0. Batch 2540/4317 - Avg Loss: 0.9530 - Accuracy: 50.82%\n",
      "Epoch: 0. Batch 2550/4317 - Avg Loss: 0.9532 - Accuracy: 50.84%\n",
      "Epoch: 0. Batch 2560/4317 - Avg Loss: 0.9532 - Accuracy: 50.84%\n",
      "Epoch: 0. Batch 2570/4317 - Avg Loss: 0.9532 - Accuracy: 50.86%\n",
      "Epoch: 0. Batch 2580/4317 - Avg Loss: 0.9529 - Accuracy: 50.88%\n",
      "Epoch: 0. Batch 2590/4317 - Avg Loss: 0.9526 - Accuracy: 50.91%\n",
      "Epoch: 0. Batch 2600/4317 - Avg Loss: 0.9522 - Accuracy: 50.94%\n",
      "Epoch: 0. Batch 2610/4317 - Avg Loss: 0.9517 - Accuracy: 50.97%\n",
      "Epoch: 0. Batch 2620/4317 - Avg Loss: 0.9514 - Accuracy: 50.99%\n",
      "Epoch: 0. Batch 2630/4317 - Avg Loss: 0.9510 - Accuracy: 51.03%\n",
      "Epoch: 0. Batch 2640/4317 - Avg Loss: 0.9504 - Accuracy: 51.06%\n",
      "Epoch: 0. Batch 2650/4317 - Avg Loss: 0.9501 - Accuracy: 51.06%\n",
      "Epoch: 0. Batch 2660/4317 - Avg Loss: 0.9495 - Accuracy: 51.10%\n",
      "Epoch: 0. Batch 2670/4317 - Avg Loss: 0.9489 - Accuracy: 51.15%\n",
      "Epoch: 0. Batch 2680/4317 - Avg Loss: 0.9485 - Accuracy: 51.19%\n",
      "Epoch: 0. Batch 2690/4317 - Avg Loss: 0.9482 - Accuracy: 51.24%\n",
      "Epoch: 0. Batch 2700/4317 - Avg Loss: 0.9477 - Accuracy: 51.29%\n",
      "Epoch: 0. Batch 2710/4317 - Avg Loss: 0.9470 - Accuracy: 51.32%\n",
      "Epoch: 0. Batch 2720/4317 - Avg Loss: 0.9462 - Accuracy: 51.36%\n",
      "Epoch: 0. Batch 2730/4317 - Avg Loss: 0.9458 - Accuracy: 51.41%\n",
      "Epoch: 0. Batch 2740/4317 - Avg Loss: 0.9455 - Accuracy: 51.45%\n",
      "Epoch: 0. Batch 2750/4317 - Avg Loss: 0.9448 - Accuracy: 51.49%\n",
      "Epoch: 0. Batch 2760/4317 - Avg Loss: 0.9447 - Accuracy: 51.52%\n",
      "Epoch: 0. Batch 2770/4317 - Avg Loss: 0.9445 - Accuracy: 51.53%\n",
      "Epoch: 0. Batch 2780/4317 - Avg Loss: 0.9439 - Accuracy: 51.58%\n",
      "Epoch: 0. Batch 2790/4317 - Avg Loss: 0.9434 - Accuracy: 51.63%\n",
      "Epoch: 0. Batch 2800/4317 - Avg Loss: 0.9427 - Accuracy: 51.68%\n",
      "Epoch: 0. Batch 2810/4317 - Avg Loss: 0.9426 - Accuracy: 51.71%\n",
      "Epoch: 0. Batch 2820/4317 - Avg Loss: 0.9427 - Accuracy: 51.72%\n",
      "Epoch: 0. Batch 2830/4317 - Avg Loss: 0.9423 - Accuracy: 51.76%\n",
      "Epoch: 0. Batch 2840/4317 - Avg Loss: 0.9419 - Accuracy: 51.80%\n",
      "Epoch: 0. Batch 2850/4317 - Avg Loss: 0.9413 - Accuracy: 51.84%\n",
      "Epoch: 0. Batch 2860/4317 - Avg Loss: 0.9410 - Accuracy: 51.87%\n",
      "Epoch: 0. Batch 2870/4317 - Avg Loss: 0.9403 - Accuracy: 51.90%\n",
      "Epoch: 0. Batch 2880/4317 - Avg Loss: 0.9398 - Accuracy: 51.93%\n",
      "Epoch: 0. Batch 2890/4317 - Avg Loss: 0.9397 - Accuracy: 51.94%\n",
      "Epoch: 0. Batch 2900/4317 - Avg Loss: 0.9393 - Accuracy: 51.98%\n",
      "Epoch: 0. Batch 2910/4317 - Avg Loss: 0.9387 - Accuracy: 52.03%\n",
      "Epoch: 0. Batch 2920/4317 - Avg Loss: 0.9383 - Accuracy: 52.05%\n",
      "Epoch: 0. Batch 2930/4317 - Avg Loss: 0.9377 - Accuracy: 52.10%\n",
      "Epoch: 0. Batch 2940/4317 - Avg Loss: 0.9376 - Accuracy: 52.10%\n",
      "Epoch: 0. Batch 2950/4317 - Avg Loss: 0.9370 - Accuracy: 52.15%\n",
      "Epoch: 0. Batch 2960/4317 - Avg Loss: 0.9369 - Accuracy: 52.17%\n",
      "Epoch: 0. Batch 2970/4317 - Avg Loss: 0.9365 - Accuracy: 52.20%\n",
      "Epoch: 0. Batch 2980/4317 - Avg Loss: 0.9362 - Accuracy: 52.22%\n",
      "Epoch: 0. Batch 2990/4317 - Avg Loss: 0.9359 - Accuracy: 52.26%\n",
      "Epoch: 0. Batch 3000/4317 - Avg Loss: 0.9358 - Accuracy: 52.26%\n",
      "Epoch: 0. Batch 3010/4317 - Avg Loss: 0.9352 - Accuracy: 52.30%\n",
      "Epoch: 0. Batch 3020/4317 - Avg Loss: 0.9350 - Accuracy: 52.33%\n",
      "Epoch: 0. Batch 3030/4317 - Avg Loss: 0.9347 - Accuracy: 52.35%\n",
      "Epoch: 0. Batch 3040/4317 - Avg Loss: 0.9346 - Accuracy: 52.34%\n",
      "Epoch: 0. Batch 3050/4317 - Avg Loss: 0.9342 - Accuracy: 52.37%\n",
      "Epoch: 0. Batch 3060/4317 - Avg Loss: 0.9338 - Accuracy: 52.38%\n",
      "Epoch: 0. Batch 3070/4317 - Avg Loss: 0.9331 - Accuracy: 52.42%\n",
      "Epoch: 0. Batch 3080/4317 - Avg Loss: 0.9329 - Accuracy: 52.46%\n",
      "Epoch: 0. Batch 3090/4317 - Avg Loss: 0.9326 - Accuracy: 52.50%\n",
      "Epoch: 0. Batch 3100/4317 - Avg Loss: 0.9323 - Accuracy: 52.54%\n",
      "Epoch: 0. Batch 3110/4317 - Avg Loss: 0.9317 - Accuracy: 52.58%\n",
      "Epoch: 0. Batch 3120/4317 - Avg Loss: 0.9313 - Accuracy: 52.61%\n",
      "Epoch: 0. Batch 3130/4317 - Avg Loss: 0.9306 - Accuracy: 52.65%\n",
      "Epoch: 0. Batch 3140/4317 - Avg Loss: 0.9304 - Accuracy: 52.69%\n",
      "Epoch: 0. Batch 3150/4317 - Avg Loss: 0.9302 - Accuracy: 52.71%\n",
      "Epoch: 0. Batch 3160/4317 - Avg Loss: 0.9300 - Accuracy: 52.72%\n",
      "Epoch: 0. Batch 3170/4317 - Avg Loss: 0.9302 - Accuracy: 52.74%\n",
      "Epoch: 0. Batch 3180/4317 - Avg Loss: 0.9295 - Accuracy: 52.80%\n",
      "Epoch: 0. Batch 3190/4317 - Avg Loss: 0.9291 - Accuracy: 52.82%\n",
      "Epoch: 0. Batch 3200/4317 - Avg Loss: 0.9287 - Accuracy: 52.85%\n",
      "Epoch: 0. Batch 3210/4317 - Avg Loss: 0.9281 - Accuracy: 52.87%\n",
      "Epoch: 0. Batch 3220/4317 - Avg Loss: 0.9279 - Accuracy: 52.90%\n",
      "Epoch: 0. Batch 3230/4317 - Avg Loss: 0.9279 - Accuracy: 52.91%\n",
      "Epoch: 0. Batch 3240/4317 - Avg Loss: 0.9278 - Accuracy: 52.93%\n",
      "Epoch: 0. Batch 3250/4317 - Avg Loss: 0.9274 - Accuracy: 52.97%\n",
      "Epoch: 0. Batch 3260/4317 - Avg Loss: 0.9271 - Accuracy: 53.01%\n",
      "Epoch: 0. Batch 3270/4317 - Avg Loss: 0.9265 - Accuracy: 53.06%\n",
      "Epoch: 0. Batch 3280/4317 - Avg Loss: 0.9263 - Accuracy: 53.08%\n",
      "Epoch: 0. Batch 3290/4317 - Avg Loss: 0.9258 - Accuracy: 53.11%\n",
      "Epoch: 0. Batch 3300/4317 - Avg Loss: 0.9258 - Accuracy: 53.12%\n",
      "Epoch: 0. Batch 3310/4317 - Avg Loss: 0.9257 - Accuracy: 53.13%\n",
      "Epoch: 0. Batch 3320/4317 - Avg Loss: 0.9253 - Accuracy: 53.16%\n",
      "Epoch: 0. Batch 3330/4317 - Avg Loss: 0.9248 - Accuracy: 53.20%\n",
      "Epoch: 0. Batch 3340/4317 - Avg Loss: 0.9244 - Accuracy: 53.23%\n",
      "Epoch: 0. Batch 3350/4317 - Avg Loss: 0.9244 - Accuracy: 53.22%\n",
      "Epoch: 0. Batch 3360/4317 - Avg Loss: 0.9237 - Accuracy: 53.26%\n",
      "Epoch: 0. Batch 3370/4317 - Avg Loss: 0.9234 - Accuracy: 53.29%\n",
      "Epoch: 0. Batch 3380/4317 - Avg Loss: 0.9232 - Accuracy: 53.31%\n",
      "Epoch: 0. Batch 3390/4317 - Avg Loss: 0.9230 - Accuracy: 53.34%\n",
      "Epoch: 0. Batch 3400/4317 - Avg Loss: 0.9226 - Accuracy: 53.36%\n",
      "Epoch: 0. Batch 3410/4317 - Avg Loss: 0.9222 - Accuracy: 53.39%\n",
      "Epoch: 0. Batch 3420/4317 - Avg Loss: 0.9217 - Accuracy: 53.44%\n",
      "Epoch: 0. Batch 3430/4317 - Avg Loss: 0.9214 - Accuracy: 53.47%\n",
      "Epoch: 0. Batch 3440/4317 - Avg Loss: 0.9212 - Accuracy: 53.49%\n",
      "Epoch: 0. Batch 3450/4317 - Avg Loss: 0.9208 - Accuracy: 53.52%\n",
      "Epoch: 0. Batch 3460/4317 - Avg Loss: 0.9207 - Accuracy: 53.54%\n",
      "Epoch: 0. Batch 3470/4317 - Avg Loss: 0.9205 - Accuracy: 53.55%\n",
      "Epoch: 0. Batch 3480/4317 - Avg Loss: 0.9201 - Accuracy: 53.58%\n",
      "Epoch: 0. Batch 3490/4317 - Avg Loss: 0.9201 - Accuracy: 53.58%\n",
      "Epoch: 0. Batch 3500/4317 - Avg Loss: 0.9199 - Accuracy: 53.59%\n",
      "Epoch: 0. Batch 3510/4317 - Avg Loss: 0.9195 - Accuracy: 53.61%\n",
      "Epoch: 0. Batch 3520/4317 - Avg Loss: 0.9191 - Accuracy: 53.63%\n",
      "Epoch: 0. Batch 3530/4317 - Avg Loss: 0.9187 - Accuracy: 53.66%\n",
      "Epoch: 0. Batch 3540/4317 - Avg Loss: 0.9183 - Accuracy: 53.69%\n",
      "Epoch: 0. Batch 3550/4317 - Avg Loss: 0.9182 - Accuracy: 53.70%\n",
      "Epoch: 0. Batch 3560/4317 - Avg Loss: 0.9180 - Accuracy: 53.74%\n",
      "Epoch: 0. Batch 3570/4317 - Avg Loss: 0.9176 - Accuracy: 53.77%\n",
      "Epoch: 0. Batch 3580/4317 - Avg Loss: 0.9174 - Accuracy: 53.78%\n",
      "Epoch: 0. Batch 3590/4317 - Avg Loss: 0.9172 - Accuracy: 53.81%\n",
      "Epoch: 0. Batch 3600/4317 - Avg Loss: 0.9168 - Accuracy: 53.83%\n",
      "Epoch: 0. Batch 3610/4317 - Avg Loss: 0.9162 - Accuracy: 53.86%\n",
      "Epoch: 0. Batch 3620/4317 - Avg Loss: 0.9158 - Accuracy: 53.89%\n",
      "Epoch: 0. Batch 3630/4317 - Avg Loss: 0.9155 - Accuracy: 53.92%\n",
      "Epoch: 0. Batch 3640/4317 - Avg Loss: 0.9150 - Accuracy: 53.95%\n",
      "Epoch: 0. Batch 3650/4317 - Avg Loss: 0.9144 - Accuracy: 53.99%\n",
      "Epoch: 0. Batch 3660/4317 - Avg Loss: 0.9139 - Accuracy: 54.02%\n",
      "Epoch: 0. Batch 3670/4317 - Avg Loss: 0.9137 - Accuracy: 54.03%\n",
      "Epoch: 0. Batch 3680/4317 - Avg Loss: 0.9135 - Accuracy: 54.04%\n",
      "Epoch: 0. Batch 3690/4317 - Avg Loss: 0.9131 - Accuracy: 54.08%\n",
      "Epoch: 0. Batch 3700/4317 - Avg Loss: 0.9129 - Accuracy: 54.11%\n",
      "Epoch: 0. Batch 3710/4317 - Avg Loss: 0.9127 - Accuracy: 54.13%\n",
      "Epoch: 0. Batch 3720/4317 - Avg Loss: 0.9125 - Accuracy: 54.15%\n",
      "Epoch: 0. Batch 3730/4317 - Avg Loss: 0.9121 - Accuracy: 54.18%\n",
      "Epoch: 0. Batch 3740/4317 - Avg Loss: 0.9119 - Accuracy: 54.19%\n",
      "Epoch: 0. Batch 3750/4317 - Avg Loss: 0.9118 - Accuracy: 54.21%\n",
      "Epoch: 0. Batch 3760/4317 - Avg Loss: 0.9115 - Accuracy: 54.23%\n",
      "Epoch: 0. Batch 3770/4317 - Avg Loss: 0.9112 - Accuracy: 54.27%\n",
      "Epoch: 0. Batch 3780/4317 - Avg Loss: 0.9110 - Accuracy: 54.28%\n",
      "Epoch: 0. Batch 3790/4317 - Avg Loss: 0.9106 - Accuracy: 54.31%\n",
      "Epoch: 0. Batch 3800/4317 - Avg Loss: 0.9102 - Accuracy: 54.35%\n",
      "Epoch: 0. Batch 3810/4317 - Avg Loss: 0.9096 - Accuracy: 54.37%\n",
      "Epoch: 0. Batch 3820/4317 - Avg Loss: 0.9091 - Accuracy: 54.40%\n",
      "Epoch: 0. Batch 3830/4317 - Avg Loss: 0.9089 - Accuracy: 54.41%\n",
      "Epoch: 0. Batch 3840/4317 - Avg Loss: 0.9084 - Accuracy: 54.43%\n",
      "Epoch: 0. Batch 3850/4317 - Avg Loss: 0.9080 - Accuracy: 54.46%\n",
      "Epoch: 0. Batch 3860/4317 - Avg Loss: 0.9078 - Accuracy: 54.47%\n",
      "Epoch: 0. Batch 3870/4317 - Avg Loss: 0.9075 - Accuracy: 54.50%\n",
      "Epoch: 0. Batch 3880/4317 - Avg Loss: 0.9074 - Accuracy: 54.51%\n",
      "Epoch: 0. Batch 3890/4317 - Avg Loss: 0.9070 - Accuracy: 54.54%\n",
      "Epoch: 0. Batch 3900/4317 - Avg Loss: 0.9066 - Accuracy: 54.56%\n",
      "Epoch: 0. Batch 3910/4317 - Avg Loss: 0.9063 - Accuracy: 54.58%\n",
      "Epoch: 0. Batch 3920/4317 - Avg Loss: 0.9058 - Accuracy: 54.60%\n",
      "Epoch: 0. Batch 3930/4317 - Avg Loss: 0.9058 - Accuracy: 54.60%\n",
      "Epoch: 0. Batch 3940/4317 - Avg Loss: 0.9053 - Accuracy: 54.62%\n",
      "Epoch: 0. Batch 3950/4317 - Avg Loss: 0.9053 - Accuracy: 54.63%\n",
      "Epoch: 0. Batch 3960/4317 - Avg Loss: 0.9050 - Accuracy: 54.66%\n",
      "Epoch: 0. Batch 3970/4317 - Avg Loss: 0.9050 - Accuracy: 54.67%\n",
      "Epoch: 0. Batch 3980/4317 - Avg Loss: 0.9049 - Accuracy: 54.67%\n",
      "Epoch: 0. Batch 3990/4317 - Avg Loss: 0.9046 - Accuracy: 54.70%\n",
      "Epoch: 0. Batch 4000/4317 - Avg Loss: 0.9044 - Accuracy: 54.70%\n",
      "Epoch: 0. Batch 4010/4317 - Avg Loss: 0.9040 - Accuracy: 54.72%\n",
      "Epoch: 0. Batch 4020/4317 - Avg Loss: 0.9036 - Accuracy: 54.74%\n",
      "Epoch: 0. Batch 4030/4317 - Avg Loss: 0.9034 - Accuracy: 54.76%\n",
      "Epoch: 0. Batch 4040/4317 - Avg Loss: 0.9031 - Accuracy: 54.78%\n",
      "Epoch: 0. Batch 4050/4317 - Avg Loss: 0.9026 - Accuracy: 54.80%\n",
      "Epoch: 0. Batch 4060/4317 - Avg Loss: 0.9021 - Accuracy: 54.83%\n",
      "Epoch: 0. Batch 4070/4317 - Avg Loss: 0.9018 - Accuracy: 54.85%\n",
      "Epoch: 0. Batch 4080/4317 - Avg Loss: 0.9017 - Accuracy: 54.88%\n",
      "Epoch: 0. Batch 4090/4317 - Avg Loss: 0.9015 - Accuracy: 54.90%\n",
      "Epoch: 0. Batch 4100/4317 - Avg Loss: 0.9011 - Accuracy: 54.94%\n",
      "Epoch: 0. Batch 4110/4317 - Avg Loss: 0.9007 - Accuracy: 54.96%\n",
      "Epoch: 0. Batch 4120/4317 - Avg Loss: 0.9004 - Accuracy: 54.99%\n",
      "Epoch: 0. Batch 4130/4317 - Avg Loss: 0.9002 - Accuracy: 55.01%\n",
      "Epoch: 0. Batch 4140/4317 - Avg Loss: 0.9001 - Accuracy: 55.02%\n",
      "Epoch: 0. Batch 4150/4317 - Avg Loss: 0.9000 - Accuracy: 55.04%\n",
      "Epoch: 0. Batch 4160/4317 - Avg Loss: 0.8997 - Accuracy: 55.05%\n",
      "Epoch: 0. Batch 4170/4317 - Avg Loss: 0.8993 - Accuracy: 55.06%\n",
      "Epoch: 0. Batch 4180/4317 - Avg Loss: 0.8991 - Accuracy: 55.08%\n",
      "Epoch: 0. Batch 4190/4317 - Avg Loss: 0.8988 - Accuracy: 55.09%\n",
      "Epoch: 0. Batch 4200/4317 - Avg Loss: 0.8985 - Accuracy: 55.11%\n",
      "Epoch: 0. Batch 4210/4317 - Avg Loss: 0.8983 - Accuracy: 55.13%\n",
      "Epoch: 0. Batch 4220/4317 - Avg Loss: 0.8981 - Accuracy: 55.15%\n",
      "Epoch: 0. Batch 4230/4317 - Avg Loss: 0.8979 - Accuracy: 55.17%\n",
      "Epoch: 0. Batch 4240/4317 - Avg Loss: 0.8977 - Accuracy: 55.18%\n",
      "Epoch: 0. Batch 4250/4317 - Avg Loss: 0.8975 - Accuracy: 55.19%\n",
      "Epoch: 0. Batch 4260/4317 - Avg Loss: 0.8972 - Accuracy: 55.20%\n",
      "Epoch: 0. Batch 4270/4317 - Avg Loss: 0.8970 - Accuracy: 55.22%\n",
      "Epoch: 0. Batch 4280/4317 - Avg Loss: 0.8967 - Accuracy: 55.24%\n",
      "Epoch: 0. Batch 4290/4317 - Avg Loss: 0.8963 - Accuracy: 55.24%\n",
      "Epoch: 0. Batch 4300/4317 - Avg Loss: 0.8962 - Accuracy: 55.25%\n",
      "Epoch: 0. Batch 4310/4317 - Avg Loss: 0.8959 - Accuracy: 55.27%\n",
      "Train loss: 0.8959 - Train accuracy: 55.27%\n",
      "Validation accuracy: 63.0380\n",
      "Epoch: 1. Batch 0/4317 - Avg Loss: 0.7710 - Accuracy: 62.50%\n",
      "Epoch: 1. Batch 10/4317 - Avg Loss: 0.7270 - Accuracy: 67.61%\n",
      "Epoch: 1. Batch 20/4317 - Avg Loss: 0.7768 - Accuracy: 63.39%\n",
      "Epoch: 1. Batch 30/4317 - Avg Loss: 0.7555 - Accuracy: 63.91%\n",
      "Epoch: 1. Batch 40/4317 - Avg Loss: 0.7457 - Accuracy: 64.79%\n",
      "Epoch: 1. Batch 50/4317 - Avg Loss: 0.7324 - Accuracy: 66.30%\n",
      "Epoch: 1. Batch 60/4317 - Avg Loss: 0.7154 - Accuracy: 67.11%\n",
      "Epoch: 1. Batch 70/4317 - Avg Loss: 0.7067 - Accuracy: 67.69%\n",
      "Epoch: 1. Batch 80/4317 - Avg Loss: 0.7060 - Accuracy: 67.44%\n",
      "Epoch: 1. Batch 90/4317 - Avg Loss: 0.6988 - Accuracy: 67.31%\n",
      "Epoch: 1. Batch 100/4317 - Avg Loss: 0.7002 - Accuracy: 67.02%\n",
      "Epoch: 1. Batch 110/4317 - Avg Loss: 0.7067 - Accuracy: 67.17%\n",
      "Epoch: 1. Batch 120/4317 - Avg Loss: 0.7030 - Accuracy: 67.30%\n",
      "Epoch: 1. Batch 130/4317 - Avg Loss: 0.6994 - Accuracy: 67.56%\n",
      "Epoch: 1. Batch 140/4317 - Avg Loss: 0.6957 - Accuracy: 67.73%\n",
      "Epoch: 1. Batch 150/4317 - Avg Loss: 0.6904 - Accuracy: 67.88%\n",
      "Epoch: 1. Batch 160/4317 - Avg Loss: 0.6933 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 170/4317 - Avg Loss: 0.6952 - Accuracy: 67.87%\n",
      "Epoch: 1. Batch 180/4317 - Avg Loss: 0.6923 - Accuracy: 67.65%\n",
      "Epoch: 1. Batch 190/4317 - Avg Loss: 0.6909 - Accuracy: 67.87%\n",
      "Epoch: 1. Batch 200/4317 - Avg Loss: 0.6917 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 210/4317 - Avg Loss: 0.6913 - Accuracy: 67.83%\n",
      "Epoch: 1. Batch 220/4317 - Avg Loss: 0.6924 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 230/4317 - Avg Loss: 0.6935 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 240/4317 - Avg Loss: 0.6974 - Accuracy: 67.89%\n",
      "Epoch: 1. Batch 250/4317 - Avg Loss: 0.6993 - Accuracy: 67.80%\n",
      "Epoch: 1. Batch 260/4317 - Avg Loss: 0.7013 - Accuracy: 67.86%\n",
      "Epoch: 1. Batch 270/4317 - Avg Loss: 0.6997 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 280/4317 - Avg Loss: 0.7019 - Accuracy: 67.77%\n",
      "Epoch: 1. Batch 290/4317 - Avg Loss: 0.7033 - Accuracy: 67.59%\n",
      "Epoch: 1. Batch 300/4317 - Avg Loss: 0.7000 - Accuracy: 67.88%\n",
      "Epoch: 1. Batch 310/4317 - Avg Loss: 0.7022 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 320/4317 - Avg Loss: 0.7046 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 330/4317 - Avg Loss: 0.7042 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 340/4317 - Avg Loss: 0.7049 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 350/4317 - Avg Loss: 0.7043 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 360/4317 - Avg Loss: 0.7051 - Accuracy: 67.87%\n",
      "Epoch: 1. Batch 370/4317 - Avg Loss: 0.7047 - Accuracy: 67.76%\n",
      "Epoch: 1. Batch 380/4317 - Avg Loss: 0.7037 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 390/4317 - Avg Loss: 0.7050 - Accuracy: 67.82%\n",
      "Epoch: 1. Batch 400/4317 - Avg Loss: 0.7026 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 410/4317 - Avg Loss: 0.7014 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 420/4317 - Avg Loss: 0.7013 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 430/4317 - Avg Loss: 0.7007 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 440/4317 - Avg Loss: 0.7014 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 450/4317 - Avg Loss: 0.7036 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 460/4317 - Avg Loss: 0.7054 - Accuracy: 67.83%\n",
      "Epoch: 1. Batch 470/4317 - Avg Loss: 0.7032 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 480/4317 - Avg Loss: 0.7016 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 490/4317 - Avg Loss: 0.7019 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 500/4317 - Avg Loss: 0.7019 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 510/4317 - Avg Loss: 0.7023 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 520/4317 - Avg Loss: 0.7024 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 530/4317 - Avg Loss: 0.7021 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 540/4317 - Avg Loss: 0.7053 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 550/4317 - Avg Loss: 0.7068 - Accuracy: 67.85%\n",
      "Epoch: 1. Batch 560/4317 - Avg Loss: 0.7073 - Accuracy: 67.89%\n",
      "Epoch: 1. Batch 570/4317 - Avg Loss: 0.7096 - Accuracy: 67.78%\n",
      "Epoch: 1. Batch 580/4317 - Avg Loss: 0.7106 - Accuracy: 67.75%\n",
      "Epoch: 1. Batch 590/4317 - Avg Loss: 0.7083 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 600/4317 - Avg Loss: 0.7079 - Accuracy: 67.88%\n",
      "Epoch: 1. Batch 610/4317 - Avg Loss: 0.7080 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 620/4317 - Avg Loss: 0.7076 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 630/4317 - Avg Loss: 0.7072 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 640/4317 - Avg Loss: 0.7062 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 650/4317 - Avg Loss: 0.7045 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 660/4317 - Avg Loss: 0.7045 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 670/4317 - Avg Loss: 0.7038 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 680/4317 - Avg Loss: 0.7034 - Accuracy: 68.22%\n",
      "Epoch: 1. Batch 690/4317 - Avg Loss: 0.7023 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 700/4317 - Avg Loss: 0.7041 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 710/4317 - Avg Loss: 0.7049 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 720/4317 - Avg Loss: 0.7060 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 730/4317 - Avg Loss: 0.7054 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 740/4317 - Avg Loss: 0.7049 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 750/4317 - Avg Loss: 0.7056 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 760/4317 - Avg Loss: 0.7060 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 770/4317 - Avg Loss: 0.7045 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 780/4317 - Avg Loss: 0.7039 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 790/4317 - Avg Loss: 0.7028 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 800/4317 - Avg Loss: 0.7019 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 810/4317 - Avg Loss: 0.7014 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 820/4317 - Avg Loss: 0.7012 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 830/4317 - Avg Loss: 0.7004 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 840/4317 - Avg Loss: 0.7005 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 850/4317 - Avg Loss: 0.7008 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 860/4317 - Avg Loss: 0.7012 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 870/4317 - Avg Loss: 0.7008 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 880/4317 - Avg Loss: 0.7009 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 890/4317 - Avg Loss: 0.7008 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 900/4317 - Avg Loss: 0.6998 - Accuracy: 68.23%\n",
      "Epoch: 1. Batch 910/4317 - Avg Loss: 0.7004 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 920/4317 - Avg Loss: 0.7007 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 930/4317 - Avg Loss: 0.6999 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 940/4317 - Avg Loss: 0.6998 - Accuracy: 68.23%\n",
      "Epoch: 1. Batch 950/4317 - Avg Loss: 0.7008 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 960/4317 - Avg Loss: 0.7005 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 970/4317 - Avg Loss: 0.7010 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 980/4317 - Avg Loss: 0.7004 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 990/4317 - Avg Loss: 0.7000 - Accuracy: 68.26%\n",
      "Epoch: 1. Batch 1000/4317 - Avg Loss: 0.6996 - Accuracy: 68.25%\n",
      "Epoch: 1. Batch 1010/4317 - Avg Loss: 0.6991 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 1020/4317 - Avg Loss: 0.6988 - Accuracy: 68.26%\n",
      "Epoch: 1. Batch 1030/4317 - Avg Loss: 0.6993 - Accuracy: 68.27%\n",
      "Epoch: 1. Batch 1040/4317 - Avg Loss: 0.6989 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1050/4317 - Avg Loss: 0.6988 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1060/4317 - Avg Loss: 0.6989 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1070/4317 - Avg Loss: 0.6992 - Accuracy: 68.29%\n",
      "Epoch: 1. Batch 1080/4317 - Avg Loss: 0.6992 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1090/4317 - Avg Loss: 0.6990 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1100/4317 - Avg Loss: 0.6996 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1110/4317 - Avg Loss: 0.6998 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1120/4317 - Avg Loss: 0.6989 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1130/4317 - Avg Loss: 0.6982 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1140/4317 - Avg Loss: 0.6986 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1150/4317 - Avg Loss: 0.6982 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1160/4317 - Avg Loss: 0.6983 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1170/4317 - Avg Loss: 0.6985 - Accuracy: 68.38%\n",
      "Epoch: 1. Batch 1180/4317 - Avg Loss: 0.6985 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1190/4317 - Avg Loss: 0.6992 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1200/4317 - Avg Loss: 0.6992 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1210/4317 - Avg Loss: 0.6994 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1220/4317 - Avg Loss: 0.6992 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1230/4317 - Avg Loss: 0.6998 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1240/4317 - Avg Loss: 0.7008 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1250/4317 - Avg Loss: 0.7010 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1260/4317 - Avg Loss: 0.7007 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1270/4317 - Avg Loss: 0.7005 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1280/4317 - Avg Loss: 0.7009 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1290/4317 - Avg Loss: 0.7010 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1300/4317 - Avg Loss: 0.7007 - Accuracy: 68.37%\n",
      "Epoch: 1. Batch 1310/4317 - Avg Loss: 0.7009 - Accuracy: 68.36%\n",
      "Epoch: 1. Batch 1320/4317 - Avg Loss: 0.7011 - Accuracy: 68.33%\n",
      "Epoch: 1. Batch 1330/4317 - Avg Loss: 0.7009 - Accuracy: 68.34%\n",
      "Epoch: 1. Batch 1340/4317 - Avg Loss: 0.7004 - Accuracy: 68.35%\n",
      "Epoch: 1. Batch 1350/4317 - Avg Loss: 0.7008 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1360/4317 - Avg Loss: 0.7004 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1370/4317 - Avg Loss: 0.7005 - Accuracy: 68.32%\n",
      "Epoch: 1. Batch 1380/4317 - Avg Loss: 0.7011 - Accuracy: 68.30%\n",
      "Epoch: 1. Batch 1390/4317 - Avg Loss: 0.7012 - Accuracy: 68.31%\n",
      "Epoch: 1. Batch 1400/4317 - Avg Loss: 0.7017 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1410/4317 - Avg Loss: 0.7021 - Accuracy: 68.26%\n",
      "Epoch: 1. Batch 1420/4317 - Avg Loss: 0.7015 - Accuracy: 68.31%\n",
      "Epoch: 1. Batch 1430/4317 - Avg Loss: 0.7019 - Accuracy: 68.28%\n",
      "Epoch: 1. Batch 1440/4317 - Avg Loss: 0.7028 - Accuracy: 68.23%\n",
      "Epoch: 1. Batch 1450/4317 - Avg Loss: 0.7026 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 1460/4317 - Avg Loss: 0.7025 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 1470/4317 - Avg Loss: 0.7027 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 1480/4317 - Avg Loss: 0.7031 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 1490/4317 - Avg Loss: 0.7026 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 1500/4317 - Avg Loss: 0.7027 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 1510/4317 - Avg Loss: 0.7028 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 1520/4317 - Avg Loss: 0.7024 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 1530/4317 - Avg Loss: 0.7026 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 1540/4317 - Avg Loss: 0.7025 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 1550/4317 - Avg Loss: 0.7028 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 1560/4317 - Avg Loss: 0.7030 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 1570/4317 - Avg Loss: 0.7032 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 1580/4317 - Avg Loss: 0.7038 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 1590/4317 - Avg Loss: 0.7032 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 1600/4317 - Avg Loss: 0.7034 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 1610/4317 - Avg Loss: 0.7038 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1620/4317 - Avg Loss: 0.7037 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1630/4317 - Avg Loss: 0.7041 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 1640/4317 - Avg Loss: 0.7037 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1650/4317 - Avg Loss: 0.7040 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1660/4317 - Avg Loss: 0.7043 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 1670/4317 - Avg Loss: 0.7041 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 1680/4317 - Avg Loss: 0.7041 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 1690/4317 - Avg Loss: 0.7036 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 1700/4317 - Avg Loss: 0.7039 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 1710/4317 - Avg Loss: 0.7035 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 1720/4317 - Avg Loss: 0.7032 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 1730/4317 - Avg Loss: 0.7031 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1740/4317 - Avg Loss: 0.7031 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 1750/4317 - Avg Loss: 0.7030 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1760/4317 - Avg Loss: 0.7035 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1770/4317 - Avg Loss: 0.7037 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 1780/4317 - Avg Loss: 0.7038 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 1790/4317 - Avg Loss: 0.7036 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 1800/4317 - Avg Loss: 0.7033 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1810/4317 - Avg Loss: 0.7036 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1820/4317 - Avg Loss: 0.7032 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1830/4317 - Avg Loss: 0.7034 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 1840/4317 - Avg Loss: 0.7035 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 1850/4317 - Avg Loss: 0.7033 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 1860/4317 - Avg Loss: 0.7034 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 1870/4317 - Avg Loss: 0.7037 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 1880/4317 - Avg Loss: 0.7035 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 1890/4317 - Avg Loss: 0.7030 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1900/4317 - Avg Loss: 0.7031 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 1910/4317 - Avg Loss: 0.7030 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 1920/4317 - Avg Loss: 0.7030 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 1930/4317 - Avg Loss: 0.7025 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1940/4317 - Avg Loss: 0.7021 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 1950/4317 - Avg Loss: 0.7021 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 1960/4317 - Avg Loss: 0.7023 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 1970/4317 - Avg Loss: 0.7020 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 1980/4317 - Avg Loss: 0.7023 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 1990/4317 - Avg Loss: 0.7024 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 2000/4317 - Avg Loss: 0.7021 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 2010/4317 - Avg Loss: 0.7019 - Accuracy: 68.18%\n",
      "Epoch: 1. Batch 2020/4317 - Avg Loss: 0.7017 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2030/4317 - Avg Loss: 0.7015 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 2040/4317 - Avg Loss: 0.7019 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2050/4317 - Avg Loss: 0.7018 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2060/4317 - Avg Loss: 0.7018 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 2070/4317 - Avg Loss: 0.7017 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2080/4317 - Avg Loss: 0.7016 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2090/4317 - Avg Loss: 0.7017 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2100/4317 - Avg Loss: 0.7017 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2110/4317 - Avg Loss: 0.7015 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2120/4317 - Avg Loss: 0.7016 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2130/4317 - Avg Loss: 0.7012 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 2140/4317 - Avg Loss: 0.7014 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 2150/4317 - Avg Loss: 0.7013 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2160/4317 - Avg Loss: 0.7016 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2170/4317 - Avg Loss: 0.7015 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 2180/4317 - Avg Loss: 0.7015 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 2190/4317 - Avg Loss: 0.7020 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2200/4317 - Avg Loss: 0.7022 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2210/4317 - Avg Loss: 0.7023 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2220/4317 - Avg Loss: 0.7020 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2230/4317 - Avg Loss: 0.7018 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2240/4317 - Avg Loss: 0.7016 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2250/4317 - Avg Loss: 0.7014 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 2260/4317 - Avg Loss: 0.7007 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2270/4317 - Avg Loss: 0.7003 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 2280/4317 - Avg Loss: 0.7005 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 2290/4317 - Avg Loss: 0.7006 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 2300/4317 - Avg Loss: 0.7015 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2310/4317 - Avg Loss: 0.7016 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2320/4317 - Avg Loss: 0.7018 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2330/4317 - Avg Loss: 0.7014 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2340/4317 - Avg Loss: 0.7010 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 2350/4317 - Avg Loss: 0.7009 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2360/4317 - Avg Loss: 0.7005 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 2370/4317 - Avg Loss: 0.7004 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 2380/4317 - Avg Loss: 0.7002 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2390/4317 - Avg Loss: 0.7001 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 2400/4317 - Avg Loss: 0.7007 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 2410/4317 - Avg Loss: 0.7009 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2420/4317 - Avg Loss: 0.7011 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 2430/4317 - Avg Loss: 0.7018 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2440/4317 - Avg Loss: 0.7018 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2450/4317 - Avg Loss: 0.7021 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2460/4317 - Avg Loss: 0.7026 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2470/4317 - Avg Loss: 0.7025 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2480/4317 - Avg Loss: 0.7027 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2490/4317 - Avg Loss: 0.7028 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2500/4317 - Avg Loss: 0.7027 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2510/4317 - Avg Loss: 0.7023 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2520/4317 - Avg Loss: 0.7021 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2530/4317 - Avg Loss: 0.7018 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2540/4317 - Avg Loss: 0.7016 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2550/4317 - Avg Loss: 0.7020 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2560/4317 - Avg Loss: 0.7020 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2570/4317 - Avg Loss: 0.7022 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2580/4317 - Avg Loss: 0.7025 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2590/4317 - Avg Loss: 0.7024 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2600/4317 - Avg Loss: 0.7027 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2610/4317 - Avg Loss: 0.7026 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2620/4317 - Avg Loss: 0.7022 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2630/4317 - Avg Loss: 0.7025 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2640/4317 - Avg Loss: 0.7021 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2650/4317 - Avg Loss: 0.7020 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2660/4317 - Avg Loss: 0.7019 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 2670/4317 - Avg Loss: 0.7020 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 2680/4317 - Avg Loss: 0.7019 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2690/4317 - Avg Loss: 0.7021 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2700/4317 - Avg Loss: 0.7023 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2710/4317 - Avg Loss: 0.7026 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2720/4317 - Avg Loss: 0.7024 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2730/4317 - Avg Loss: 0.7020 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2740/4317 - Avg Loss: 0.7021 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2750/4317 - Avg Loss: 0.7025 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2760/4317 - Avg Loss: 0.7026 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2770/4317 - Avg Loss: 0.7024 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2780/4317 - Avg Loss: 0.7025 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2790/4317 - Avg Loss: 0.7023 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2800/4317 - Avg Loss: 0.7021 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 2810/4317 - Avg Loss: 0.7017 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2820/4317 - Avg Loss: 0.7015 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 2830/4317 - Avg Loss: 0.7019 - Accuracy: 68.02%\n",
      "Epoch: 1. Batch 2840/4317 - Avg Loss: 0.7021 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2850/4317 - Avg Loss: 0.7023 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2860/4317 - Avg Loss: 0.7024 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2870/4317 - Avg Loss: 0.7023 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2880/4317 - Avg Loss: 0.7023 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2890/4317 - Avg Loss: 0.7020 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2900/4317 - Avg Loss: 0.7021 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 2910/4317 - Avg Loss: 0.7020 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2920/4317 - Avg Loss: 0.7018 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2930/4317 - Avg Loss: 0.7018 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2940/4317 - Avg Loss: 0.7016 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2950/4317 - Avg Loss: 0.7021 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2960/4317 - Avg Loss: 0.7023 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 2970/4317 - Avg Loss: 0.7022 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 2980/4317 - Avg Loss: 0.7023 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 2990/4317 - Avg Loss: 0.7020 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3000/4317 - Avg Loss: 0.7021 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3010/4317 - Avg Loss: 0.7019 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3020/4317 - Avg Loss: 0.7021 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3030/4317 - Avg Loss: 0.7022 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3040/4317 - Avg Loss: 0.7023 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3050/4317 - Avg Loss: 0.7026 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3060/4317 - Avg Loss: 0.7028 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3070/4317 - Avg Loss: 0.7028 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3080/4317 - Avg Loss: 0.7026 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3090/4317 - Avg Loss: 0.7027 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3100/4317 - Avg Loss: 0.7029 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3110/4317 - Avg Loss: 0.7030 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3120/4317 - Avg Loss: 0.7033 - Accuracy: 67.93%\n",
      "Epoch: 1. Batch 3130/4317 - Avg Loss: 0.7029 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3140/4317 - Avg Loss: 0.7028 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3150/4317 - Avg Loss: 0.7030 - Accuracy: 67.93%\n",
      "Epoch: 1. Batch 3160/4317 - Avg Loss: 0.7029 - Accuracy: 67.93%\n",
      "Epoch: 1. Batch 3170/4317 - Avg Loss: 0.7031 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 3180/4317 - Avg Loss: 0.7030 - Accuracy: 67.93%\n",
      "Epoch: 1. Batch 3190/4317 - Avg Loss: 0.7029 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3200/4317 - Avg Loss: 0.7027 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3210/4317 - Avg Loss: 0.7027 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3220/4317 - Avg Loss: 0.7026 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3230/4317 - Avg Loss: 0.7025 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3240/4317 - Avg Loss: 0.7028 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3250/4317 - Avg Loss: 0.7030 - Accuracy: 67.93%\n",
      "Epoch: 1. Batch 3260/4317 - Avg Loss: 0.7029 - Accuracy: 67.94%\n",
      "Epoch: 1. Batch 3270/4317 - Avg Loss: 0.7030 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 3280/4317 - Avg Loss: 0.7032 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 3290/4317 - Avg Loss: 0.7034 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3300/4317 - Avg Loss: 0.7034 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3310/4317 - Avg Loss: 0.7034 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3320/4317 - Avg Loss: 0.7035 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 3330/4317 - Avg Loss: 0.7035 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 3340/4317 - Avg Loss: 0.7038 - Accuracy: 67.88%\n",
      "Epoch: 1. Batch 3350/4317 - Avg Loss: 0.7036 - Accuracy: 67.89%\n",
      "Epoch: 1. Batch 3360/4317 - Avg Loss: 0.7037 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 3370/4317 - Avg Loss: 0.7036 - Accuracy: 67.89%\n",
      "Epoch: 1. Batch 3380/4317 - Avg Loss: 0.7036 - Accuracy: 67.88%\n",
      "Epoch: 1. Batch 3390/4317 - Avg Loss: 0.7034 - Accuracy: 67.89%\n",
      "Epoch: 1. Batch 3400/4317 - Avg Loss: 0.7032 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 3410/4317 - Avg Loss: 0.7031 - Accuracy: 67.90%\n",
      "Epoch: 1. Batch 3420/4317 - Avg Loss: 0.7030 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3430/4317 - Avg Loss: 0.7030 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3440/4317 - Avg Loss: 0.7032 - Accuracy: 67.91%\n",
      "Epoch: 1. Batch 3450/4317 - Avg Loss: 0.7029 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 3460/4317 - Avg Loss: 0.7030 - Accuracy: 67.92%\n",
      "Epoch: 1. Batch 3470/4317 - Avg Loss: 0.7028 - Accuracy: 67.95%\n",
      "Epoch: 1. Batch 3480/4317 - Avg Loss: 0.7025 - Accuracy: 67.96%\n",
      "Epoch: 1. Batch 3490/4317 - Avg Loss: 0.7023 - Accuracy: 67.97%\n",
      "Epoch: 1. Batch 3500/4317 - Avg Loss: 0.7020 - Accuracy: 67.98%\n",
      "Epoch: 1. Batch 3510/4317 - Avg Loss: 0.7019 - Accuracy: 67.99%\n",
      "Epoch: 1. Batch 3520/4317 - Avg Loss: 0.7017 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3530/4317 - Avg Loss: 0.7016 - Accuracy: 68.00%\n",
      "Epoch: 1. Batch 3540/4317 - Avg Loss: 0.7016 - Accuracy: 68.01%\n",
      "Epoch: 1. Batch 3550/4317 - Avg Loss: 0.7014 - Accuracy: 68.03%\n",
      "Epoch: 1. Batch 3560/4317 - Avg Loss: 0.7012 - Accuracy: 68.04%\n",
      "Epoch: 1. Batch 3570/4317 - Avg Loss: 0.7009 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3580/4317 - Avg Loss: 0.7008 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3590/4317 - Avg Loss: 0.7007 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3600/4317 - Avg Loss: 0.7005 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 3610/4317 - Avg Loss: 0.7006 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 3620/4317 - Avg Loss: 0.7004 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 3630/4317 - Avg Loss: 0.6999 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 3640/4317 - Avg Loss: 0.7001 - Accuracy: 68.10%\n",
      "Epoch: 1. Batch 3650/4317 - Avg Loss: 0.7005 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 3660/4317 - Avg Loss: 0.7005 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 3670/4317 - Avg Loss: 0.7005 - Accuracy: 68.08%\n",
      "Epoch: 1. Batch 3680/4317 - Avg Loss: 0.7006 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3690/4317 - Avg Loss: 0.7007 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 3700/4317 - Avg Loss: 0.7009 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3710/4317 - Avg Loss: 0.7010 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3720/4317 - Avg Loss: 0.7009 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3730/4317 - Avg Loss: 0.7009 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3740/4317 - Avg Loss: 0.7009 - Accuracy: 68.05%\n",
      "Epoch: 1. Batch 3750/4317 - Avg Loss: 0.7009 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3760/4317 - Avg Loss: 0.7007 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 3770/4317 - Avg Loss: 0.7007 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3780/4317 - Avg Loss: 0.7006 - Accuracy: 68.06%\n",
      "Epoch: 1. Batch 3790/4317 - Avg Loss: 0.7004 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 3800/4317 - Avg Loss: 0.7006 - Accuracy: 68.07%\n",
      "Epoch: 1. Batch 3810/4317 - Avg Loss: 0.7003 - Accuracy: 68.09%\n",
      "Epoch: 1. Batch 3820/4317 - Avg Loss: 0.7003 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3830/4317 - Avg Loss: 0.7003 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3840/4317 - Avg Loss: 0.7001 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3850/4317 - Avg Loss: 0.7001 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3860/4317 - Avg Loss: 0.7000 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3870/4317 - Avg Loss: 0.7000 - Accuracy: 68.11%\n",
      "Epoch: 1. Batch 3880/4317 - Avg Loss: 0.6997 - Accuracy: 68.12%\n",
      "Epoch: 1. Batch 3890/4317 - Avg Loss: 0.6996 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3900/4317 - Avg Loss: 0.6997 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3910/4317 - Avg Loss: 0.6998 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3920/4317 - Avg Loss: 0.6998 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3930/4317 - Avg Loss: 0.7000 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3940/4317 - Avg Loss: 0.7000 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 3950/4317 - Avg Loss: 0.6999 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 3960/4317 - Avg Loss: 0.6998 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 3970/4317 - Avg Loss: 0.6998 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 3980/4317 - Avg Loss: 0.6998 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 3990/4317 - Avg Loss: 0.6999 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 4000/4317 - Avg Loss: 0.6998 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 4010/4317 - Avg Loss: 0.7000 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 4020/4317 - Avg Loss: 0.7001 - Accuracy: 68.14%\n",
      "Epoch: 1. Batch 4030/4317 - Avg Loss: 0.7000 - Accuracy: 68.13%\n",
      "Epoch: 1. Batch 4040/4317 - Avg Loss: 0.6999 - Accuracy: 68.15%\n",
      "Epoch: 1. Batch 4050/4317 - Avg Loss: 0.6999 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 4060/4317 - Avg Loss: 0.6997 - Accuracy: 68.16%\n",
      "Epoch: 1. Batch 4070/4317 - Avg Loss: 0.6997 - Accuracy: 68.17%\n",
      "Epoch: 1. Batch 4080/4317 - Avg Loss: 0.6996 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4090/4317 - Avg Loss: 0.6996 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4100/4317 - Avg Loss: 0.6994 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 4110/4317 - Avg Loss: 0.6995 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 4120/4317 - Avg Loss: 0.6996 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4130/4317 - Avg Loss: 0.6995 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 4140/4317 - Avg Loss: 0.6996 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4150/4317 - Avg Loss: 0.6997 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4160/4317 - Avg Loss: 0.6996 - Accuracy: 68.19%\n",
      "Epoch: 1. Batch 4170/4317 - Avg Loss: 0.6995 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 4180/4317 - Avg Loss: 0.6994 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4190/4317 - Avg Loss: 0.6995 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4200/4317 - Avg Loss: 0.6995 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4210/4317 - Avg Loss: 0.6996 - Accuracy: 68.20%\n",
      "Epoch: 1. Batch 4220/4317 - Avg Loss: 0.6994 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4230/4317 - Avg Loss: 0.6994 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4240/4317 - Avg Loss: 0.6994 - Accuracy: 68.22%\n",
      "Epoch: 1. Batch 4250/4317 - Avg Loss: 0.6993 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4260/4317 - Avg Loss: 0.6993 - Accuracy: 68.21%\n",
      "Epoch: 1. Batch 4270/4317 - Avg Loss: 0.6993 - Accuracy: 68.22%\n",
      "Epoch: 1. Batch 4280/4317 - Avg Loss: 0.6993 - Accuracy: 68.22%\n",
      "Epoch: 1. Batch 4290/4317 - Avg Loss: 0.6990 - Accuracy: 68.24%\n",
      "Epoch: 1. Batch 4300/4317 - Avg Loss: 0.6990 - Accuracy: 68.23%\n",
      "Epoch: 1. Batch 4310/4317 - Avg Loss: 0.6989 - Accuracy: 68.23%\n",
      "Train loss: 0.6988 - Train accuracy: 68.24%\n",
      "Validation accuracy: 66.6802\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.0405%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 55.27%        | 63.04%              |\n",
    "| **Epoch 2** | 68.24%        | 66.68%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './LSTM_sentiment_model/lstm_sentiment_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
