{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: LSTM (sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a5a3df2e3ce62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4317 - Avg Loss: 1.0994 - Accuracy: 31.25%\n",
      "Epoch: 0. Batch 10/4317 - Avg Loss: 1.1423 - Accuracy: 31.25%\n",
      "Epoch: 0. Batch 20/4317 - Avg Loss: 1.1507 - Accuracy: 35.71%\n",
      "Epoch: 0. Batch 30/4317 - Avg Loss: 1.1344 - Accuracy: 37.30%\n",
      "Epoch: 0. Batch 40/4317 - Avg Loss: 1.1275 - Accuracy: 35.82%\n",
      "Epoch: 0. Batch 50/4317 - Avg Loss: 1.1217 - Accuracy: 36.03%\n",
      "Epoch: 0. Batch 60/4317 - Avg Loss: 1.1177 - Accuracy: 37.19%\n",
      "Epoch: 0. Batch 70/4317 - Avg Loss: 1.1159 - Accuracy: 37.94%\n",
      "Epoch: 0. Batch 80/4317 - Avg Loss: 1.1141 - Accuracy: 36.11%\n",
      "Epoch: 0. Batch 90/4317 - Avg Loss: 1.1129 - Accuracy: 35.03%\n",
      "Epoch: 0. Batch 100/4317 - Avg Loss: 1.1118 - Accuracy: 33.79%\n",
      "Epoch: 0. Batch 110/4317 - Avg Loss: 1.1098 - Accuracy: 34.63%\n",
      "Epoch: 0. Batch 120/4317 - Avg Loss: 1.1101 - Accuracy: 34.35%\n",
      "Epoch: 0. Batch 130/4317 - Avg Loss: 1.1096 - Accuracy: 34.73%\n",
      "Epoch: 0. Batch 140/4317 - Avg Loss: 1.1089 - Accuracy: 35.28%\n",
      "Epoch: 0. Batch 150/4317 - Avg Loss: 1.1084 - Accuracy: 35.51%\n",
      "Epoch: 0. Batch 160/4317 - Avg Loss: 1.1077 - Accuracy: 35.02%\n",
      "Epoch: 0. Batch 170/4317 - Avg Loss: 1.1067 - Accuracy: 34.72%\n",
      "Epoch: 0. Batch 180/4317 - Avg Loss: 1.1069 - Accuracy: 33.94%\n",
      "Epoch: 0. Batch 190/4317 - Avg Loss: 1.1071 - Accuracy: 33.12%\n",
      "Epoch: 0. Batch 200/4317 - Avg Loss: 1.1068 - Accuracy: 33.64%\n",
      "Epoch: 0. Batch 210/4317 - Avg Loss: 1.1064 - Accuracy: 34.15%\n",
      "Epoch: 0. Batch 220/4317 - Avg Loss: 1.1061 - Accuracy: 34.67%\n",
      "Epoch: 0. Batch 230/4317 - Avg Loss: 1.1057 - Accuracy: 35.23%\n",
      "Epoch: 0. Batch 240/4317 - Avg Loss: 1.1058 - Accuracy: 35.24%\n",
      "Epoch: 0. Batch 250/4317 - Avg Loss: 1.1056 - Accuracy: 35.53%\n",
      "Epoch: 0. Batch 260/4317 - Avg Loss: 1.1053 - Accuracy: 35.42%\n",
      "Epoch: 0. Batch 270/4317 - Avg Loss: 1.1051 - Accuracy: 34.96%\n",
      "Epoch: 0. Batch 280/4317 - Avg Loss: 1.1049 - Accuracy: 34.59%\n",
      "Epoch: 0. Batch 290/4317 - Avg Loss: 1.1046 - Accuracy: 34.94%\n",
      "Epoch: 0. Batch 300/4317 - Avg Loss: 1.1044 - Accuracy: 35.30%\n",
      "Epoch: 0. Batch 310/4317 - Avg Loss: 1.1042 - Accuracy: 35.47%\n",
      "Epoch: 0. Batch 320/4317 - Avg Loss: 1.1041 - Accuracy: 35.65%\n",
      "Epoch: 0. Batch 330/4317 - Avg Loss: 1.1039 - Accuracy: 35.90%\n",
      "Epoch: 0. Batch 340/4317 - Avg Loss: 1.1038 - Accuracy: 36.18%\n",
      "Epoch: 0. Batch 350/4317 - Avg Loss: 1.1036 - Accuracy: 36.34%\n",
      "Epoch: 0. Batch 360/4317 - Avg Loss: 1.1035 - Accuracy: 36.60%\n",
      "Epoch: 0. Batch 370/4317 - Avg Loss: 1.1034 - Accuracy: 36.74%\n",
      "Epoch: 0. Batch 380/4317 - Avg Loss: 1.1031 - Accuracy: 36.81%\n",
      "Epoch: 0. Batch 390/4317 - Avg Loss: 1.1032 - Accuracy: 36.72%\n",
      "Epoch: 0. Batch 400/4317 - Avg Loss: 1.1031 - Accuracy: 36.67%\n",
      "Epoch: 0. Batch 410/4317 - Avg Loss: 1.1029 - Accuracy: 36.66%\n",
      "Epoch: 0. Batch 420/4317 - Avg Loss: 1.1028 - Accuracy: 36.58%\n",
      "Epoch: 0. Batch 430/4317 - Avg Loss: 1.1026 - Accuracy: 36.75%\n",
      "Epoch: 0. Batch 440/4317 - Avg Loss: 1.1025 - Accuracy: 36.62%\n",
      "Epoch: 0. Batch 450/4317 - Avg Loss: 1.1024 - Accuracy: 36.54%\n",
      "Epoch: 0. Batch 460/4317 - Avg Loss: 1.1024 - Accuracy: 36.67%\n",
      "Epoch: 0. Batch 470/4317 - Avg Loss: 1.1024 - Accuracy: 36.68%\n",
      "Epoch: 0. Batch 480/4317 - Avg Loss: 1.1023 - Accuracy: 36.93%\n",
      "Epoch: 0. Batch 490/4317 - Avg Loss: 1.1022 - Accuracy: 37.12%\n",
      "Epoch: 0. Batch 500/4317 - Avg Loss: 1.1023 - Accuracy: 37.09%\n",
      "Epoch: 0. Batch 510/4317 - Avg Loss: 1.1022 - Accuracy: 37.28%\n",
      "Epoch: 0. Batch 520/4317 - Avg Loss: 1.1021 - Accuracy: 37.50%\n",
      "Epoch: 0. Batch 530/4317 - Avg Loss: 1.1020 - Accuracy: 37.66%\n",
      "Epoch: 0. Batch 540/4317 - Avg Loss: 1.1019 - Accuracy: 37.81%\n",
      "Epoch: 0. Batch 550/4317 - Avg Loss: 1.1019 - Accuracy: 37.77%\n",
      "Epoch: 0. Batch 560/4317 - Avg Loss: 1.1018 - Accuracy: 37.87%\n",
      "Epoch: 0. Batch 570/4317 - Avg Loss: 1.1018 - Accuracy: 37.93%\n",
      "Epoch: 0. Batch 580/4317 - Avg Loss: 1.1017 - Accuracy: 37.97%\n",
      "Epoch: 0. Batch 590/4317 - Avg Loss: 1.1016 - Accuracy: 38.04%\n",
      "Epoch: 0. Batch 600/4317 - Avg Loss: 1.1016 - Accuracy: 38.07%\n",
      "Epoch: 0. Batch 610/4317 - Avg Loss: 1.1016 - Accuracy: 38.03%\n",
      "Epoch: 0. Batch 620/4317 - Avg Loss: 1.1016 - Accuracy: 38.04%\n",
      "Epoch: 0. Batch 630/4317 - Avg Loss: 1.1015 - Accuracy: 38.05%\n",
      "Epoch: 0. Batch 640/4317 - Avg Loss: 1.1015 - Accuracy: 38.09%\n",
      "Epoch: 0. Batch 650/4317 - Avg Loss: 1.1014 - Accuracy: 38.10%\n",
      "Epoch: 0. Batch 660/4317 - Avg Loss: 1.1014 - Accuracy: 38.03%\n",
      "Epoch: 0. Batch 670/4317 - Avg Loss: 1.1013 - Accuracy: 37.96%\n",
      "Epoch: 0. Batch 680/4317 - Avg Loss: 1.1013 - Accuracy: 37.95%\n",
      "Epoch: 0. Batch 690/4317 - Avg Loss: 1.1012 - Accuracy: 37.83%\n",
      "Epoch: 0. Batch 700/4317 - Avg Loss: 1.1012 - Accuracy: 37.84%\n",
      "Epoch: 0. Batch 710/4317 - Avg Loss: 1.1011 - Accuracy: 37.77%\n",
      "Epoch: 0. Batch 720/4317 - Avg Loss: 1.1011 - Accuracy: 37.73%\n",
      "Epoch: 0. Batch 730/4317 - Avg Loss: 1.1011 - Accuracy: 37.71%\n",
      "Epoch: 0. Batch 740/4317 - Avg Loss: 1.1010 - Accuracy: 37.61%\n",
      "Epoch: 0. Batch 750/4317 - Avg Loss: 1.1010 - Accuracy: 37.67%\n",
      "Epoch: 0. Batch 760/4317 - Avg Loss: 1.1010 - Accuracy: 37.64%\n",
      "Epoch: 0. Batch 770/4317 - Avg Loss: 1.1010 - Accuracy: 37.56%\n",
      "Epoch: 0. Batch 780/4317 - Avg Loss: 1.1010 - Accuracy: 37.49%\n",
      "Epoch: 0. Batch 790/4317 - Avg Loss: 1.1009 - Accuracy: 37.52%\n",
      "Epoch: 0. Batch 800/4317 - Avg Loss: 1.1009 - Accuracy: 37.44%\n",
      "Epoch: 0. Batch 810/4317 - Avg Loss: 1.1009 - Accuracy: 37.38%\n",
      "Epoch: 0. Batch 820/4317 - Avg Loss: 1.1009 - Accuracy: 37.35%\n",
      "Epoch: 0. Batch 830/4317 - Avg Loss: 1.1008 - Accuracy: 37.39%\n",
      "Epoch: 0. Batch 840/4317 - Avg Loss: 1.1008 - Accuracy: 37.33%\n",
      "Epoch: 0. Batch 850/4317 - Avg Loss: 1.1008 - Accuracy: 37.39%\n",
      "Epoch: 0. Batch 860/4317 - Avg Loss: 1.1008 - Accuracy: 37.30%\n",
      "Epoch: 0. Batch 870/4317 - Avg Loss: 1.1007 - Accuracy: 37.33%\n",
      "Epoch: 0. Batch 880/4317 - Avg Loss: 1.1007 - Accuracy: 37.27%\n",
      "Epoch: 0. Batch 890/4317 - Avg Loss: 1.1007 - Accuracy: 37.23%\n",
      "Epoch: 0. Batch 900/4317 - Avg Loss: 1.1007 - Accuracy: 37.31%\n",
      "Epoch: 0. Batch 910/4317 - Avg Loss: 1.1006 - Accuracy: 37.26%\n",
      "Epoch: 0. Batch 920/4317 - Avg Loss: 1.1006 - Accuracy: 37.24%\n",
      "Epoch: 0. Batch 930/4317 - Avg Loss: 1.1006 - Accuracy: 37.22%\n",
      "Epoch: 0. Batch 940/4317 - Avg Loss: 1.1006 - Accuracy: 37.12%\n",
      "Epoch: 0. Batch 950/4317 - Avg Loss: 1.1006 - Accuracy: 37.05%\n",
      "Epoch: 0. Batch 960/4317 - Avg Loss: 1.1006 - Accuracy: 37.01%\n",
      "Epoch: 0. Batch 970/4317 - Avg Loss: 1.1006 - Accuracy: 37.00%\n",
      "Epoch: 0. Batch 980/4317 - Avg Loss: 1.1005 - Accuracy: 36.99%\n",
      "Epoch: 0. Batch 990/4317 - Avg Loss: 1.1005 - Accuracy: 36.95%\n",
      "Epoch: 0. Batch 1000/4317 - Avg Loss: 1.1005 - Accuracy: 36.96%\n",
      "Epoch: 0. Batch 1010/4317 - Avg Loss: 1.1005 - Accuracy: 36.95%\n",
      "Epoch: 0. Batch 1020/4317 - Avg Loss: 1.1005 - Accuracy: 36.92%\n",
      "Epoch: 0. Batch 1030/4317 - Avg Loss: 1.1004 - Accuracy: 36.91%\n",
      "Epoch: 0. Batch 1040/4317 - Avg Loss: 1.1004 - Accuracy: 36.89%\n",
      "Epoch: 0. Batch 1050/4317 - Avg Loss: 1.1004 - Accuracy: 36.88%\n",
      "Epoch: 0. Batch 1060/4317 - Avg Loss: 1.1004 - Accuracy: 36.91%\n",
      "Epoch: 0. Batch 1070/4317 - Avg Loss: 1.1004 - Accuracy: 36.95%\n",
      "Epoch: 0. Batch 1080/4317 - Avg Loss: 1.1003 - Accuracy: 37.11%\n",
      "Epoch: 0. Batch 1090/4317 - Avg Loss: 1.1003 - Accuracy: 37.20%\n",
      "Epoch: 0. Batch 1100/4317 - Avg Loss: 1.1003 - Accuracy: 37.22%\n",
      "Epoch: 0. Batch 1110/4317 - Avg Loss: 1.1003 - Accuracy: 37.26%\n",
      "Epoch: 0. Batch 1120/4317 - Avg Loss: 1.1003 - Accuracy: 37.27%\n",
      "Epoch: 0. Batch 1130/4317 - Avg Loss: 1.1002 - Accuracy: 37.35%\n",
      "Epoch: 0. Batch 1140/4317 - Avg Loss: 1.1002 - Accuracy: 37.38%\n",
      "Epoch: 0. Batch 1150/4317 - Avg Loss: 1.1002 - Accuracy: 37.38%\n",
      "Epoch: 0. Batch 1160/4317 - Avg Loss: 1.1002 - Accuracy: 37.44%\n",
      "Epoch: 0. Batch 1170/4317 - Avg Loss: 1.1002 - Accuracy: 37.46%\n",
      "Epoch: 0. Batch 1180/4317 - Avg Loss: 1.1002 - Accuracy: 37.54%\n",
      "Epoch: 0. Batch 1190/4317 - Avg Loss: 1.1002 - Accuracy: 37.65%\n",
      "Epoch: 0. Batch 1200/4317 - Avg Loss: 1.1002 - Accuracy: 37.69%\n",
      "Epoch: 0. Batch 1210/4317 - Avg Loss: 1.1001 - Accuracy: 37.74%\n",
      "Epoch: 0. Batch 1220/4317 - Avg Loss: 1.1002 - Accuracy: 37.74%\n",
      "Epoch: 0. Batch 1230/4317 - Avg Loss: 1.1001 - Accuracy: 37.80%\n",
      "Epoch: 0. Batch 1240/4317 - Avg Loss: 1.1001 - Accuracy: 37.82%\n",
      "Epoch: 0. Batch 1250/4317 - Avg Loss: 1.1001 - Accuracy: 37.90%\n",
      "Epoch: 0. Batch 1260/4317 - Avg Loss: 1.1001 - Accuracy: 37.94%\n",
      "Epoch: 0. Batch 1270/4317 - Avg Loss: 1.1001 - Accuracy: 37.93%\n",
      "Epoch: 0. Batch 1280/4317 - Avg Loss: 1.1001 - Accuracy: 38.01%\n",
      "Epoch: 0. Batch 1290/4317 - Avg Loss: 1.1000 - Accuracy: 38.03%\n",
      "Epoch: 0. Batch 1300/4317 - Avg Loss: 1.1000 - Accuracy: 38.11%\n",
      "Epoch: 0. Batch 1310/4317 - Avg Loss: 1.1000 - Accuracy: 38.12%\n",
      "Epoch: 0. Batch 1320/4317 - Avg Loss: 1.1000 - Accuracy: 38.14%\n",
      "Epoch: 0. Batch 1330/4317 - Avg Loss: 1.1000 - Accuracy: 38.13%\n",
      "Epoch: 0. Batch 1340/4317 - Avg Loss: 1.1000 - Accuracy: 38.17%\n",
      "Epoch: 0. Batch 1350/4317 - Avg Loss: 1.1000 - Accuracy: 38.18%\n",
      "Epoch: 0. Batch 1360/4317 - Avg Loss: 1.1000 - Accuracy: 38.21%\n",
      "Epoch: 0. Batch 1370/4317 - Avg Loss: 1.1000 - Accuracy: 38.22%\n",
      "Epoch: 0. Batch 1380/4317 - Avg Loss: 1.1000 - Accuracy: 38.21%\n",
      "Epoch: 0. Batch 1390/4317 - Avg Loss: 1.1000 - Accuracy: 38.27%\n",
      "Epoch: 0. Batch 1400/4317 - Avg Loss: 1.1000 - Accuracy: 38.32%\n",
      "Epoch: 0. Batch 1410/4317 - Avg Loss: 1.1000 - Accuracy: 38.33%\n",
      "Epoch: 0. Batch 1420/4317 - Avg Loss: 1.1000 - Accuracy: 38.24%\n",
      "Epoch: 0. Batch 1430/4317 - Avg Loss: 1.1000 - Accuracy: 38.12%\n",
      "Epoch: 0. Batch 1440/4317 - Avg Loss: 1.1000 - Accuracy: 38.00%\n",
      "Epoch: 0. Batch 1450/4317 - Avg Loss: 1.0999 - Accuracy: 37.96%\n",
      "Epoch: 0. Batch 1460/4317 - Avg Loss: 1.0999 - Accuracy: 37.86%\n",
      "Epoch: 0. Batch 1470/4317 - Avg Loss: 1.0999 - Accuracy: 37.73%\n",
      "Epoch: 0. Batch 1480/4317 - Avg Loss: 1.0999 - Accuracy: 37.64%\n",
      "Epoch: 0. Batch 1490/4317 - Avg Loss: 1.0999 - Accuracy: 37.58%\n",
      "Epoch: 0. Batch 1500/4317 - Avg Loss: 1.0999 - Accuracy: 37.55%\n",
      "Epoch: 0. Batch 1510/4317 - Avg Loss: 1.0999 - Accuracy: 37.58%\n",
      "Epoch: 0. Batch 1520/4317 - Avg Loss: 1.0999 - Accuracy: 37.68%\n",
      "Epoch: 0. Batch 1530/4317 - Avg Loss: 1.0999 - Accuracy: 37.70%\n",
      "Epoch: 0. Batch 1540/4317 - Avg Loss: 1.0999 - Accuracy: 37.76%\n",
      "Epoch: 0. Batch 1550/4317 - Avg Loss: 1.0999 - Accuracy: 37.82%\n",
      "Epoch: 0. Batch 1560/4317 - Avg Loss: 1.0999 - Accuracy: 37.84%\n",
      "Epoch: 0. Batch 1570/4317 - Avg Loss: 1.0999 - Accuracy: 37.88%\n",
      "Epoch: 0. Batch 1580/4317 - Avg Loss: 1.0999 - Accuracy: 37.87%\n",
      "Epoch: 0. Batch 1590/4317 - Avg Loss: 1.0999 - Accuracy: 37.89%\n",
      "Epoch: 0. Batch 1600/4317 - Avg Loss: 1.0999 - Accuracy: 37.93%\n",
      "Epoch: 0. Batch 1610/4317 - Avg Loss: 1.0998 - Accuracy: 37.95%\n",
      "Epoch: 0. Batch 1620/4317 - Avg Loss: 1.0998 - Accuracy: 38.02%\n",
      "Epoch: 0. Batch 1630/4317 - Avg Loss: 1.0998 - Accuracy: 38.04%\n",
      "Epoch: 0. Batch 1640/4317 - Avg Loss: 1.0998 - Accuracy: 38.09%\n",
      "Epoch: 0. Batch 1650/4317 - Avg Loss: 1.0998 - Accuracy: 38.14%\n",
      "Epoch: 0. Batch 1660/4317 - Avg Loss: 1.0998 - Accuracy: 38.16%\n",
      "Epoch: 0. Batch 1670/4317 - Avg Loss: 1.0998 - Accuracy: 38.21%\n",
      "Epoch: 0. Batch 1680/4317 - Avg Loss: 1.0998 - Accuracy: 38.23%\n",
      "Epoch: 0. Batch 1690/4317 - Avg Loss: 1.0998 - Accuracy: 38.27%\n",
      "Epoch: 0. Batch 1700/4317 - Avg Loss: 1.0998 - Accuracy: 38.27%\n",
      "Epoch: 0. Batch 1710/4317 - Avg Loss: 1.0998 - Accuracy: 38.30%\n",
      "Epoch: 0. Batch 1720/4317 - Avg Loss: 1.0998 - Accuracy: 38.31%\n",
      "Epoch: 0. Batch 1730/4317 - Avg Loss: 1.0998 - Accuracy: 38.29%\n",
      "Epoch: 0. Batch 1740/4317 - Avg Loss: 1.0997 - Accuracy: 38.34%\n",
      "Epoch: 0. Batch 1750/4317 - Avg Loss: 1.0997 - Accuracy: 38.40%\n",
      "Epoch: 0. Batch 1760/4317 - Avg Loss: 1.0997 - Accuracy: 38.43%\n",
      "Epoch: 0. Batch 1770/4317 - Avg Loss: 1.0997 - Accuracy: 38.45%\n",
      "Epoch: 0. Batch 1780/4317 - Avg Loss: 1.0997 - Accuracy: 38.47%\n",
      "Epoch: 0. Batch 1790/4317 - Avg Loss: 1.0997 - Accuracy: 38.45%\n",
      "Epoch: 0. Batch 1800/4317 - Avg Loss: 1.0997 - Accuracy: 38.44%\n",
      "Epoch: 0. Batch 1810/4317 - Avg Loss: 1.0997 - Accuracy: 38.44%\n",
      "Epoch: 0. Batch 1820/4317 - Avg Loss: 1.0997 - Accuracy: 38.43%\n",
      "Epoch: 0. Batch 1830/4317 - Avg Loss: 1.0997 - Accuracy: 38.49%\n",
      "Epoch: 0. Batch 1840/4317 - Avg Loss: 1.0997 - Accuracy: 38.50%\n",
      "Epoch: 0. Batch 1850/4317 - Avg Loss: 1.0997 - Accuracy: 38.56%\n",
      "Epoch: 0. Batch 1860/4317 - Avg Loss: 1.0997 - Accuracy: 38.60%\n",
      "Epoch: 0. Batch 1870/4317 - Avg Loss: 1.0997 - Accuracy: 38.59%\n",
      "Epoch: 0. Batch 1880/4317 - Avg Loss: 1.0997 - Accuracy: 38.60%\n",
      "Epoch: 0. Batch 1890/4317 - Avg Loss: 1.0997 - Accuracy: 38.61%\n",
      "Epoch: 0. Batch 1900/4317 - Avg Loss: 1.0997 - Accuracy: 38.61%\n",
      "Epoch: 0. Batch 1910/4317 - Avg Loss: 1.0997 - Accuracy: 38.65%\n",
      "Epoch: 0. Batch 1920/4317 - Avg Loss: 1.0997 - Accuracy: 38.66%\n",
      "Epoch: 0. Batch 1930/4317 - Avg Loss: 1.0997 - Accuracy: 38.71%\n",
      "Epoch: 0. Batch 1940/4317 - Avg Loss: 1.0997 - Accuracy: 38.75%\n",
      "Epoch: 0. Batch 1950/4317 - Avg Loss: 1.0997 - Accuracy: 38.73%\n",
      "Epoch: 0. Batch 1960/4317 - Avg Loss: 1.0996 - Accuracy: 38.76%\n",
      "Epoch: 0. Batch 1970/4317 - Avg Loss: 1.0996 - Accuracy: 38.79%\n",
      "Epoch: 0. Batch 1980/4317 - Avg Loss: 1.0996 - Accuracy: 38.78%\n",
      "Epoch: 0. Batch 1990/4317 - Avg Loss: 1.0996 - Accuracy: 38.77%\n",
      "Epoch: 0. Batch 2000/4317 - Avg Loss: 1.0996 - Accuracy: 38.76%\n",
      "Epoch: 0. Batch 2010/4317 - Avg Loss: 1.0996 - Accuracy: 38.74%\n",
      "Epoch: 0. Batch 2020/4317 - Avg Loss: 1.0996 - Accuracy: 38.72%\n",
      "Epoch: 0. Batch 2030/4317 - Avg Loss: 1.0996 - Accuracy: 38.70%\n",
      "Epoch: 0. Batch 2040/4317 - Avg Loss: 1.0996 - Accuracy: 38.69%\n",
      "Epoch: 0. Batch 2050/4317 - Avg Loss: 1.0996 - Accuracy: 38.68%\n",
      "Epoch: 0. Batch 2060/4317 - Avg Loss: 1.0996 - Accuracy: 38.65%\n",
      "Epoch: 0. Batch 2070/4317 - Avg Loss: 1.0996 - Accuracy: 38.63%\n",
      "Epoch: 0. Batch 2080/4317 - Avg Loss: 1.0996 - Accuracy: 38.61%\n",
      "Epoch: 0. Batch 2090/4317 - Avg Loss: 1.0996 - Accuracy: 38.60%\n",
      "Epoch: 0. Batch 2100/4317 - Avg Loss: 1.0996 - Accuracy: 38.55%\n",
      "Epoch: 0. Batch 2110/4317 - Avg Loss: 1.0996 - Accuracy: 38.52%\n",
      "Epoch: 0. Batch 2120/4317 - Avg Loss: 1.0996 - Accuracy: 38.52%\n",
      "Epoch: 0. Batch 2130/4317 - Avg Loss: 1.0996 - Accuracy: 38.48%\n",
      "Epoch: 0. Batch 2140/4317 - Avg Loss: 1.0996 - Accuracy: 38.48%\n",
      "Epoch: 0. Batch 2150/4317 - Avg Loss: 1.0996 - Accuracy: 38.47%\n",
      "Epoch: 0. Batch 2160/4317 - Avg Loss: 1.0996 - Accuracy: 38.45%\n",
      "Epoch: 0. Batch 2170/4317 - Avg Loss: 1.0995 - Accuracy: 38.44%\n",
      "Epoch: 0. Batch 2180/4317 - Avg Loss: 1.0996 - Accuracy: 38.39%\n",
      "Epoch: 0. Batch 2190/4317 - Avg Loss: 1.0996 - Accuracy: 38.37%\n",
      "Epoch: 0. Batch 2200/4317 - Avg Loss: 1.0995 - Accuracy: 38.36%\n",
      "Epoch: 0. Batch 2210/4317 - Avg Loss: 1.0995 - Accuracy: 38.35%\n",
      "Epoch: 0. Batch 2220/4317 - Avg Loss: 1.0995 - Accuracy: 38.32%\n",
      "Epoch: 0. Batch 2230/4317 - Avg Loss: 1.0995 - Accuracy: 38.29%\n",
      "Epoch: 0. Batch 2240/4317 - Avg Loss: 1.0995 - Accuracy: 38.28%\n",
      "Epoch: 0. Batch 2250/4317 - Avg Loss: 1.0995 - Accuracy: 38.27%\n",
      "Epoch: 0. Batch 2260/4317 - Avg Loss: 1.0995 - Accuracy: 38.24%\n",
      "Epoch: 0. Batch 2270/4317 - Avg Loss: 1.0995 - Accuracy: 38.22%\n",
      "Epoch: 0. Batch 2280/4317 - Avg Loss: 1.0995 - Accuracy: 38.22%\n",
      "Epoch: 0. Batch 2290/4317 - Avg Loss: 1.0995 - Accuracy: 38.18%\n",
      "Epoch: 0. Batch 2300/4317 - Avg Loss: 1.0995 - Accuracy: 38.14%\n",
      "Epoch: 0. Batch 2310/4317 - Avg Loss: 1.0995 - Accuracy: 38.08%\n",
      "Epoch: 0. Batch 2320/4317 - Avg Loss: 1.0995 - Accuracy: 38.08%\n",
      "Epoch: 0. Batch 2330/4317 - Avg Loss: 1.0995 - Accuracy: 38.12%\n",
      "Epoch: 0. Batch 2340/4317 - Avg Loss: 1.0995 - Accuracy: 38.14%\n",
      "Epoch: 0. Batch 2350/4317 - Avg Loss: 1.0995 - Accuracy: 38.13%\n",
      "Epoch: 0. Batch 2360/4317 - Avg Loss: 1.0995 - Accuracy: 38.15%\n",
      "Epoch: 0. Batch 2370/4317 - Avg Loss: 1.0995 - Accuracy: 38.17%\n",
      "Epoch: 0. Batch 2380/4317 - Avg Loss: 1.0995 - Accuracy: 38.21%\n",
      "Epoch: 0. Batch 2390/4317 - Avg Loss: 1.0995 - Accuracy: 38.23%\n",
      "Epoch: 0. Batch 2400/4317 - Avg Loss: 1.0995 - Accuracy: 38.27%\n",
      "Epoch: 0. Batch 2410/4317 - Avg Loss: 1.0995 - Accuracy: 38.29%\n",
      "Epoch: 0. Batch 2420/4317 - Avg Loss: 1.0995 - Accuracy: 38.33%\n",
      "Epoch: 0. Batch 2430/4317 - Avg Loss: 1.0995 - Accuracy: 38.37%\n",
      "Epoch: 0. Batch 2440/4317 - Avg Loss: 1.0995 - Accuracy: 38.37%\n",
      "Epoch: 0. Batch 2450/4317 - Avg Loss: 1.0995 - Accuracy: 38.36%\n",
      "Epoch: 0. Batch 2460/4317 - Avg Loss: 1.0995 - Accuracy: 38.37%\n",
      "Epoch: 0. Batch 2470/4317 - Avg Loss: 1.0995 - Accuracy: 38.38%\n",
      "Epoch: 0. Batch 2480/4317 - Avg Loss: 1.0994 - Accuracy: 38.41%\n",
      "Epoch: 0. Batch 2490/4317 - Avg Loss: 1.0994 - Accuracy: 38.42%\n",
      "Epoch: 0. Batch 2500/4317 - Avg Loss: 1.0994 - Accuracy: 38.43%\n",
      "Epoch: 0. Batch 2510/4317 - Avg Loss: 1.0994 - Accuracy: 38.45%\n",
      "Epoch: 0. Batch 2520/4317 - Avg Loss: 1.0994 - Accuracy: 38.50%\n",
      "Epoch: 0. Batch 2530/4317 - Avg Loss: 1.0994 - Accuracy: 38.51%\n",
      "Epoch: 0. Batch 2540/4317 - Avg Loss: 1.0994 - Accuracy: 38.53%\n",
      "Epoch: 0. Batch 2550/4317 - Avg Loss: 1.0994 - Accuracy: 38.56%\n",
      "Epoch: 0. Batch 2560/4317 - Avg Loss: 1.0994 - Accuracy: 38.56%\n",
      "Epoch: 0. Batch 2570/4317 - Avg Loss: 1.0994 - Accuracy: 38.58%\n",
      "Epoch: 0. Batch 2580/4317 - Avg Loss: 1.0994 - Accuracy: 38.62%\n",
      "Epoch: 0. Batch 2590/4317 - Avg Loss: 1.0994 - Accuracy: 38.64%\n",
      "Epoch: 0. Batch 2600/4317 - Avg Loss: 1.0994 - Accuracy: 38.63%\n",
      "Epoch: 0. Batch 2610/4317 - Avg Loss: 1.0994 - Accuracy: 38.62%\n",
      "Epoch: 0. Batch 2620/4317 - Avg Loss: 1.0994 - Accuracy: 38.64%\n",
      "Epoch: 0. Batch 2630/4317 - Avg Loss: 1.0994 - Accuracy: 38.67%\n",
      "Epoch: 0. Batch 2640/4317 - Avg Loss: 1.0994 - Accuracy: 38.70%\n",
      "Epoch: 0. Batch 2650/4317 - Avg Loss: 1.0994 - Accuracy: 38.73%\n",
      "Epoch: 0. Batch 2660/4317 - Avg Loss: 1.0994 - Accuracy: 38.76%\n",
      "Epoch: 0. Batch 2670/4317 - Avg Loss: 1.0994 - Accuracy: 38.77%\n",
      "Epoch: 0. Batch 2680/4317 - Avg Loss: 1.0994 - Accuracy: 38.77%\n",
      "Epoch: 0. Batch 2690/4317 - Avg Loss: 1.0994 - Accuracy: 38.79%\n",
      "Epoch: 0. Batch 2700/4317 - Avg Loss: 1.0994 - Accuracy: 38.80%\n",
      "Epoch: 0. Batch 2710/4317 - Avg Loss: 1.0994 - Accuracy: 38.80%\n",
      "Epoch: 0. Batch 2720/4317 - Avg Loss: 1.0994 - Accuracy: 38.80%\n",
      "Epoch: 0. Batch 2730/4317 - Avg Loss: 1.0994 - Accuracy: 38.82%\n",
      "Epoch: 0. Batch 2740/4317 - Avg Loss: 1.0994 - Accuracy: 38.81%\n",
      "Epoch: 0. Batch 2750/4317 - Avg Loss: 1.0993 - Accuracy: 38.85%\n",
      "Epoch: 0. Batch 2760/4317 - Avg Loss: 1.0993 - Accuracy: 38.86%\n",
      "Epoch: 0. Batch 2770/4317 - Avg Loss: 1.0994 - Accuracy: 38.85%\n",
      "Epoch: 0. Batch 2780/4317 - Avg Loss: 1.0993 - Accuracy: 38.87%\n",
      "Epoch: 0. Batch 2790/4317 - Avg Loss: 1.0993 - Accuracy: 38.90%\n",
      "Epoch: 0. Batch 2800/4317 - Avg Loss: 1.0993 - Accuracy: 38.93%\n",
      "Epoch: 0. Batch 2810/4317 - Avg Loss: 1.0993 - Accuracy: 38.96%\n",
      "Epoch: 0. Batch 2820/4317 - Avg Loss: 1.0993 - Accuracy: 38.97%\n",
      "Epoch: 0. Batch 2830/4317 - Avg Loss: 1.0993 - Accuracy: 38.98%\n",
      "Epoch: 0. Batch 2840/4317 - Avg Loss: 1.0993 - Accuracy: 39.02%\n",
      "Epoch: 0. Batch 2850/4317 - Avg Loss: 1.0993 - Accuracy: 39.02%\n",
      "Epoch: 0. Batch 2860/4317 - Avg Loss: 1.0993 - Accuracy: 39.06%\n",
      "Epoch: 0. Batch 2870/4317 - Avg Loss: 1.0993 - Accuracy: 39.07%\n",
      "Epoch: 0. Batch 2880/4317 - Avg Loss: 1.0993 - Accuracy: 39.10%\n",
      "Epoch: 0. Batch 2890/4317 - Avg Loss: 1.0992 - Accuracy: 39.12%\n",
      "Epoch: 0. Batch 2900/4317 - Avg Loss: 1.0992 - Accuracy: 39.15%\n",
      "Epoch: 0. Batch 2910/4317 - Avg Loss: 1.0992 - Accuracy: 39.17%\n",
      "Epoch: 0. Batch 2920/4317 - Avg Loss: 1.0992 - Accuracy: 39.18%\n",
      "Epoch: 0. Batch 2930/4317 - Avg Loss: 1.0992 - Accuracy: 39.20%\n",
      "Epoch: 0. Batch 2940/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 2950/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 2960/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 2970/4317 - Avg Loss: 1.0992 - Accuracy: 39.21%\n",
      "Epoch: 0. Batch 2980/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 2990/4317 - Avg Loss: 1.0992 - Accuracy: 39.23%\n",
      "Epoch: 0. Batch 3000/4317 - Avg Loss: 1.0992 - Accuracy: 39.26%\n",
      "Epoch: 0. Batch 3010/4317 - Avg Loss: 1.0992 - Accuracy: 39.25%\n",
      "Epoch: 0. Batch 3020/4317 - Avg Loss: 1.0992 - Accuracy: 39.29%\n",
      "Epoch: 0. Batch 3030/4317 - Avg Loss: 1.0992 - Accuracy: 39.31%\n",
      "Epoch: 0. Batch 3040/4317 - Avg Loss: 1.0992 - Accuracy: 39.30%\n",
      "Epoch: 0. Batch 3050/4317 - Avg Loss: 1.0993 - Accuracy: 39.29%\n",
      "Epoch: 0. Batch 3060/4317 - Avg Loss: 1.0993 - Accuracy: 39.30%\n",
      "Epoch: 0. Batch 3070/4317 - Avg Loss: 1.0993 - Accuracy: 39.31%\n",
      "Epoch: 0. Batch 3080/4317 - Avg Loss: 1.0993 - Accuracy: 39.31%\n",
      "Epoch: 0. Batch 3090/4317 - Avg Loss: 1.0993 - Accuracy: 39.32%\n",
      "Epoch: 0. Batch 3100/4317 - Avg Loss: 1.0993 - Accuracy: 39.32%\n",
      "Epoch: 0. Batch 3110/4317 - Avg Loss: 1.0993 - Accuracy: 39.34%\n",
      "Epoch: 0. Batch 3120/4317 - Avg Loss: 1.0992 - Accuracy: 39.36%\n",
      "Epoch: 0. Batch 3130/4317 - Avg Loss: 1.0992 - Accuracy: 39.37%\n",
      "Epoch: 0. Batch 3140/4317 - Avg Loss: 1.0992 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3150/4317 - Avg Loss: 1.0992 - Accuracy: 39.40%\n",
      "Epoch: 0. Batch 3160/4317 - Avg Loss: 1.0993 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3170/4317 - Avg Loss: 1.0993 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3180/4317 - Avg Loss: 1.0993 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3190/4317 - Avg Loss: 1.0993 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3200/4317 - Avg Loss: 1.0993 - Accuracy: 39.36%\n",
      "Epoch: 0. Batch 3210/4317 - Avg Loss: 1.0993 - Accuracy: 39.33%\n",
      "Epoch: 0. Batch 3220/4317 - Avg Loss: 1.0993 - Accuracy: 39.32%\n",
      "Epoch: 0. Batch 3230/4317 - Avg Loss: 1.0993 - Accuracy: 39.30%\n",
      "Epoch: 0. Batch 3240/4317 - Avg Loss: 1.0992 - Accuracy: 39.27%\n",
      "Epoch: 0. Batch 3250/4317 - Avg Loss: 1.0992 - Accuracy: 39.24%\n",
      "Epoch: 0. Batch 3260/4317 - Avg Loss: 1.0992 - Accuracy: 39.20%\n",
      "Epoch: 0. Batch 3270/4317 - Avg Loss: 1.0992 - Accuracy: 39.14%\n",
      "Epoch: 0. Batch 3280/4317 - Avg Loss: 1.0992 - Accuracy: 39.08%\n",
      "Epoch: 0. Batch 3290/4317 - Avg Loss: 1.0992 - Accuracy: 39.03%\n",
      "Epoch: 0. Batch 3300/4317 - Avg Loss: 1.0992 - Accuracy: 39.00%\n",
      "Epoch: 0. Batch 3310/4317 - Avg Loss: 1.0992 - Accuracy: 38.99%\n",
      "Epoch: 0. Batch 3320/4317 - Avg Loss: 1.0992 - Accuracy: 38.96%\n",
      "Epoch: 0. Batch 3330/4317 - Avg Loss: 1.0992 - Accuracy: 38.98%\n",
      "Epoch: 0. Batch 3340/4317 - Avg Loss: 1.0992 - Accuracy: 38.98%\n",
      "Epoch: 0. Batch 3350/4317 - Avg Loss: 1.0992 - Accuracy: 39.00%\n",
      "Epoch: 0. Batch 3360/4317 - Avg Loss: 1.0992 - Accuracy: 39.04%\n",
      "Epoch: 0. Batch 3370/4317 - Avg Loss: 1.0992 - Accuracy: 39.05%\n",
      "Epoch: 0. Batch 3380/4317 - Avg Loss: 1.0992 - Accuracy: 39.06%\n",
      "Epoch: 0. Batch 3390/4317 - Avg Loss: 1.0992 - Accuracy: 39.06%\n",
      "Epoch: 0. Batch 3400/4317 - Avg Loss: 1.0992 - Accuracy: 39.09%\n",
      "Epoch: 0. Batch 3410/4317 - Avg Loss: 1.0992 - Accuracy: 39.11%\n",
      "Epoch: 0. Batch 3420/4317 - Avg Loss: 1.0992 - Accuracy: 39.13%\n",
      "Epoch: 0. Batch 3430/4317 - Avg Loss: 1.0992 - Accuracy: 39.13%\n",
      "Epoch: 0. Batch 3440/4317 - Avg Loss: 1.0992 - Accuracy: 39.14%\n",
      "Epoch: 0. Batch 3450/4317 - Avg Loss: 1.0992 - Accuracy: 39.17%\n",
      "Epoch: 0. Batch 3460/4317 - Avg Loss: 1.0992 - Accuracy: 39.18%\n",
      "Epoch: 0. Batch 3470/4317 - Avg Loss: 1.0992 - Accuracy: 39.19%\n",
      "Epoch: 0. Batch 3480/4317 - Avg Loss: 1.0992 - Accuracy: 39.21%\n",
      "Epoch: 0. Batch 3490/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 3500/4317 - Avg Loss: 1.0992 - Accuracy: 39.23%\n",
      "Epoch: 0. Batch 3510/4317 - Avg Loss: 1.0992 - Accuracy: 39.24%\n",
      "Epoch: 0. Batch 3520/4317 - Avg Loss: 1.0992 - Accuracy: 39.22%\n",
      "Epoch: 0. Batch 3530/4317 - Avg Loss: 1.0992 - Accuracy: 39.23%\n",
      "Epoch: 0. Batch 3540/4317 - Avg Loss: 1.0992 - Accuracy: 39.23%\n",
      "Epoch: 0. Batch 3550/4317 - Avg Loss: 1.0992 - Accuracy: 39.24%\n",
      "Epoch: 0. Batch 3560/4317 - Avg Loss: 1.0992 - Accuracy: 39.26%\n",
      "Epoch: 0. Batch 3570/4317 - Avg Loss: 1.0992 - Accuracy: 39.28%\n",
      "Epoch: 0. Batch 3580/4317 - Avg Loss: 1.0992 - Accuracy: 39.29%\n",
      "Epoch: 0. Batch 3590/4317 - Avg Loss: 1.0992 - Accuracy: 39.29%\n",
      "Epoch: 0. Batch 3600/4317 - Avg Loss: 1.0992 - Accuracy: 39.31%\n",
      "Epoch: 0. Batch 3610/4317 - Avg Loss: 1.0992 - Accuracy: 39.33%\n",
      "Epoch: 0. Batch 3620/4317 - Avg Loss: 1.0992 - Accuracy: 39.32%\n",
      "Epoch: 0. Batch 3630/4317 - Avg Loss: 1.0992 - Accuracy: 39.34%\n",
      "Epoch: 0. Batch 3640/4317 - Avg Loss: 1.0992 - Accuracy: 39.35%\n",
      "Epoch: 0. Batch 3650/4317 - Avg Loss: 1.0992 - Accuracy: 39.37%\n",
      "Epoch: 0. Batch 3660/4317 - Avg Loss: 1.0992 - Accuracy: 39.37%\n",
      "Epoch: 0. Batch 3670/4317 - Avg Loss: 1.0992 - Accuracy: 39.37%\n",
      "Epoch: 0. Batch 3680/4317 - Avg Loss: 1.0992 - Accuracy: 39.38%\n",
      "Epoch: 0. Batch 3690/4317 - Avg Loss: 1.0992 - Accuracy: 39.39%\n",
      "Epoch: 0. Batch 3700/4317 - Avg Loss: 1.0992 - Accuracy: 39.41%\n",
      "Epoch: 0. Batch 3710/4317 - Avg Loss: 1.0992 - Accuracy: 39.42%\n",
      "Epoch: 0. Batch 3720/4317 - Avg Loss: 1.0992 - Accuracy: 39.44%\n",
      "Epoch: 0. Batch 3730/4317 - Avg Loss: 1.0992 - Accuracy: 39.44%\n",
      "Epoch: 0. Batch 3740/4317 - Avg Loss: 1.0992 - Accuracy: 39.47%\n",
      "Epoch: 0. Batch 3750/4317 - Avg Loss: 1.0992 - Accuracy: 39.48%\n",
      "Epoch: 0. Batch 3760/4317 - Avg Loss: 1.0991 - Accuracy: 39.49%\n",
      "Epoch: 0. Batch 3770/4317 - Avg Loss: 1.0992 - Accuracy: 39.49%\n",
      "Epoch: 0. Batch 3780/4317 - Avg Loss: 1.0992 - Accuracy: 39.49%\n",
      "Epoch: 0. Batch 3790/4317 - Avg Loss: 1.0991 - Accuracy: 39.50%\n",
      "Epoch: 0. Batch 3800/4317 - Avg Loss: 1.0991 - Accuracy: 39.51%\n",
      "Epoch: 0. Batch 3810/4317 - Avg Loss: 1.0991 - Accuracy: 39.52%\n",
      "Epoch: 0. Batch 3820/4317 - Avg Loss: 1.0991 - Accuracy: 39.55%\n",
      "Epoch: 0. Batch 3830/4317 - Avg Loss: 1.0991 - Accuracy: 39.55%\n",
      "Epoch: 0. Batch 3840/4317 - Avg Loss: 1.0991 - Accuracy: 39.56%\n",
      "Epoch: 0. Batch 3850/4317 - Avg Loss: 1.0991 - Accuracy: 39.56%\n",
      "Epoch: 0. Batch 3860/4317 - Avg Loss: 1.0991 - Accuracy: 39.59%\n",
      "Epoch: 0. Batch 3870/4317 - Avg Loss: 1.0991 - Accuracy: 39.59%\n",
      "Epoch: 0. Batch 3880/4317 - Avg Loss: 1.0991 - Accuracy: 39.61%\n",
      "Epoch: 0. Batch 3890/4317 - Avg Loss: 1.0991 - Accuracy: 39.61%\n",
      "Epoch: 0. Batch 3900/4317 - Avg Loss: 1.0991 - Accuracy: 39.61%\n",
      "Epoch: 0. Batch 3910/4317 - Avg Loss: 1.0991 - Accuracy: 39.63%\n",
      "Epoch: 0. Batch 3920/4317 - Avg Loss: 1.0991 - Accuracy: 39.64%\n",
      "Epoch: 0. Batch 3930/4317 - Avg Loss: 1.0991 - Accuracy: 39.66%\n",
      "Epoch: 0. Batch 3940/4317 - Avg Loss: 1.0991 - Accuracy: 39.68%\n",
      "Epoch: 0. Batch 3950/4317 - Avg Loss: 1.0991 - Accuracy: 39.69%\n",
      "Epoch: 0. Batch 3960/4317 - Avg Loss: 1.0991 - Accuracy: 39.69%\n",
      "Epoch: 0. Batch 3970/4317 - Avg Loss: 1.0991 - Accuracy: 39.70%\n",
      "Epoch: 0. Batch 3980/4317 - Avg Loss: 1.0991 - Accuracy: 39.70%\n",
      "Epoch: 0. Batch 3990/4317 - Avg Loss: 1.0991 - Accuracy: 39.70%\n",
      "Epoch: 0. Batch 4000/4317 - Avg Loss: 1.0991 - Accuracy: 39.71%\n",
      "Epoch: 0. Batch 4010/4317 - Avg Loss: 1.0991 - Accuracy: 39.71%\n",
      "Epoch: 0. Batch 4020/4317 - Avg Loss: 1.0991 - Accuracy: 39.72%\n",
      "Epoch: 0. Batch 4030/4317 - Avg Loss: 1.0991 - Accuracy: 39.73%\n",
      "Epoch: 0. Batch 4040/4317 - Avg Loss: 1.0991 - Accuracy: 39.74%\n",
      "Epoch: 0. Batch 4050/4317 - Avg Loss: 1.0991 - Accuracy: 39.76%\n",
      "Epoch: 0. Batch 4060/4317 - Avg Loss: 1.0991 - Accuracy: 39.76%\n",
      "Epoch: 0. Batch 4070/4317 - Avg Loss: 1.0991 - Accuracy: 39.77%\n",
      "Epoch: 0. Batch 4080/4317 - Avg Loss: 1.0991 - Accuracy: 39.76%\n",
      "Epoch: 0. Batch 4090/4317 - Avg Loss: 1.0991 - Accuracy: 39.78%\n",
      "Epoch: 0. Batch 4100/4317 - Avg Loss: 1.0991 - Accuracy: 39.79%\n",
      "Epoch: 0. Batch 4110/4317 - Avg Loss: 1.0991 - Accuracy: 39.80%\n",
      "Epoch: 0. Batch 4120/4317 - Avg Loss: 1.0991 - Accuracy: 39.81%\n",
      "Epoch: 0. Batch 4130/4317 - Avg Loss: 1.0991 - Accuracy: 39.80%\n",
      "Epoch: 0. Batch 4140/4317 - Avg Loss: 1.0991 - Accuracy: 39.81%\n",
      "Epoch: 0. Batch 4150/4317 - Avg Loss: 1.0991 - Accuracy: 39.82%\n",
      "Epoch: 0. Batch 4160/4317 - Avg Loss: 1.0991 - Accuracy: 39.83%\n",
      "Epoch: 0. Batch 4170/4317 - Avg Loss: 1.0991 - Accuracy: 39.83%\n",
      "Epoch: 0. Batch 4180/4317 - Avg Loss: 1.0991 - Accuracy: 39.83%\n",
      "Epoch: 0. Batch 4190/4317 - Avg Loss: 1.0991 - Accuracy: 39.84%\n",
      "Epoch: 0. Batch 4200/4317 - Avg Loss: 1.0991 - Accuracy: 39.85%\n",
      "Epoch: 0. Batch 4210/4317 - Avg Loss: 1.0991 - Accuracy: 39.85%\n",
      "Epoch: 0. Batch 4220/4317 - Avg Loss: 1.0991 - Accuracy: 39.85%\n",
      "Epoch: 0. Batch 4230/4317 - Avg Loss: 1.0991 - Accuracy: 39.86%\n",
      "Epoch: 0. Batch 4240/4317 - Avg Loss: 1.0991 - Accuracy: 39.87%\n",
      "Epoch: 0. Batch 4250/4317 - Avg Loss: 1.0991 - Accuracy: 39.88%\n",
      "Epoch: 0. Batch 4260/4317 - Avg Loss: 1.0991 - Accuracy: 39.90%\n",
      "Epoch: 0. Batch 4270/4317 - Avg Loss: 1.0991 - Accuracy: 39.89%\n",
      "Epoch: 0. Batch 4280/4317 - Avg Loss: 1.0991 - Accuracy: 39.88%\n",
      "Epoch: 0. Batch 4290/4317 - Avg Loss: 1.0991 - Accuracy: 39.89%\n",
      "Epoch: 0. Batch 4300/4317 - Avg Loss: 1.0991 - Accuracy: 39.87%\n",
      "Epoch: 0. Batch 4310/4317 - Avg Loss: 1.0990 - Accuracy: 39.86%\n",
      "Train loss: 1.0990 - Train accuracy: 39.85%\n",
      "Validation accuracy: 34.3672\n",
      "Epoch: 1. Batch 0/4317 - Avg Loss: 1.0967 - Accuracy: 25.00%\n",
      "Epoch: 1. Batch 10/4317 - Avg Loss: 1.0990 - Accuracy: 32.95%\n",
      "Epoch: 1. Batch 20/4317 - Avg Loss: 1.0987 - Accuracy: 36.31%\n",
      "Epoch: 1. Batch 30/4317 - Avg Loss: 1.0994 - Accuracy: 37.10%\n",
      "Epoch: 1. Batch 40/4317 - Avg Loss: 1.0993 - Accuracy: 39.02%\n",
      "Epoch: 1. Batch 50/4317 - Avg Loss: 1.1000 - Accuracy: 38.24%\n",
      "Epoch: 1. Batch 60/4317 - Avg Loss: 1.0995 - Accuracy: 39.65%\n",
      "Epoch: 1. Batch 70/4317 - Avg Loss: 1.0995 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 80/4317 - Avg Loss: 1.0996 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 90/4317 - Avg Loss: 1.0996 - Accuracy: 40.25%\n",
      "Epoch: 1. Batch 100/4317 - Avg Loss: 1.0995 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 110/4317 - Avg Loss: 1.0993 - Accuracy: 40.93%\n",
      "Epoch: 1. Batch 120/4317 - Avg Loss: 1.0991 - Accuracy: 41.53%\n",
      "Epoch: 1. Batch 130/4317 - Avg Loss: 1.0992 - Accuracy: 41.13%\n",
      "Epoch: 1. Batch 140/4317 - Avg Loss: 1.0991 - Accuracy: 41.71%\n",
      "Epoch: 1. Batch 150/4317 - Avg Loss: 1.0992 - Accuracy: 41.89%\n",
      "Epoch: 1. Batch 160/4317 - Avg Loss: 1.0992 - Accuracy: 42.12%\n",
      "Epoch: 1. Batch 170/4317 - Avg Loss: 1.0989 - Accuracy: 42.76%\n",
      "Epoch: 1. Batch 180/4317 - Avg Loss: 1.0989 - Accuracy: 42.89%\n",
      "Epoch: 1. Batch 190/4317 - Avg Loss: 1.0991 - Accuracy: 42.34%\n",
      "Epoch: 1. Batch 200/4317 - Avg Loss: 1.0990 - Accuracy: 42.29%\n",
      "Epoch: 1. Batch 210/4317 - Avg Loss: 1.0990 - Accuracy: 42.39%\n",
      "Epoch: 1. Batch 220/4317 - Avg Loss: 1.0990 - Accuracy: 42.42%\n",
      "Epoch: 1. Batch 230/4317 - Avg Loss: 1.0990 - Accuracy: 42.32%\n",
      "Epoch: 1. Batch 240/4317 - Avg Loss: 1.0989 - Accuracy: 42.63%\n",
      "Epoch: 1. Batch 250/4317 - Avg Loss: 1.0990 - Accuracy: 42.53%\n",
      "Epoch: 1. Batch 260/4317 - Avg Loss: 1.0990 - Accuracy: 42.34%\n",
      "Epoch: 1. Batch 270/4317 - Avg Loss: 1.0989 - Accuracy: 42.37%\n",
      "Epoch: 1. Batch 280/4317 - Avg Loss: 1.0989 - Accuracy: 42.35%\n",
      "Epoch: 1. Batch 290/4317 - Avg Loss: 1.0989 - Accuracy: 42.42%\n",
      "Epoch: 1. Batch 300/4317 - Avg Loss: 1.0989 - Accuracy: 42.42%\n",
      "Epoch: 1. Batch 310/4317 - Avg Loss: 1.0989 - Accuracy: 42.38%\n",
      "Epoch: 1. Batch 320/4317 - Avg Loss: 1.0989 - Accuracy: 42.37%\n",
      "Epoch: 1. Batch 330/4317 - Avg Loss: 1.0989 - Accuracy: 42.15%\n",
      "Epoch: 1. Batch 340/4317 - Avg Loss: 1.0989 - Accuracy: 42.16%\n",
      "Epoch: 1. Batch 350/4317 - Avg Loss: 1.0989 - Accuracy: 42.20%\n",
      "Epoch: 1. Batch 360/4317 - Avg Loss: 1.0989 - Accuracy: 42.04%\n",
      "Epoch: 1. Batch 370/4317 - Avg Loss: 1.0989 - Accuracy: 42.12%\n",
      "Epoch: 1. Batch 380/4317 - Avg Loss: 1.0988 - Accuracy: 41.98%\n",
      "Epoch: 1. Batch 390/4317 - Avg Loss: 1.0988 - Accuracy: 41.82%\n",
      "Epoch: 1. Batch 400/4317 - Avg Loss: 1.0987 - Accuracy: 41.68%\n",
      "Epoch: 1. Batch 410/4317 - Avg Loss: 1.0987 - Accuracy: 41.53%\n",
      "Epoch: 1. Batch 420/4317 - Avg Loss: 1.0987 - Accuracy: 41.33%\n",
      "Epoch: 1. Batch 430/4317 - Avg Loss: 1.0987 - Accuracy: 41.36%\n",
      "Epoch: 1. Batch 440/4317 - Avg Loss: 1.0987 - Accuracy: 41.14%\n",
      "Epoch: 1. Batch 450/4317 - Avg Loss: 1.0986 - Accuracy: 41.16%\n",
      "Epoch: 1. Batch 460/4317 - Avg Loss: 1.0986 - Accuracy: 40.97%\n",
      "Epoch: 1. Batch 470/4317 - Avg Loss: 1.0986 - Accuracy: 40.67%\n",
      "Epoch: 1. Batch 480/4317 - Avg Loss: 1.0987 - Accuracy: 40.44%\n",
      "Epoch: 1. Batch 490/4317 - Avg Loss: 1.0988 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 500/4317 - Avg Loss: 1.0988 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 510/4317 - Avg Loss: 1.0988 - Accuracy: 40.41%\n",
      "Epoch: 1. Batch 520/4317 - Avg Loss: 1.0987 - Accuracy: 40.48%\n",
      "Epoch: 1. Batch 530/4317 - Avg Loss: 1.0988 - Accuracy: 40.49%\n",
      "Epoch: 1. Batch 540/4317 - Avg Loss: 1.0987 - Accuracy: 40.28%\n",
      "Epoch: 1. Batch 550/4317 - Avg Loss: 1.0987 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 560/4317 - Avg Loss: 1.0987 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 570/4317 - Avg Loss: 1.0986 - Accuracy: 40.08%\n",
      "Epoch: 1. Batch 580/4317 - Avg Loss: 1.0985 - Accuracy: 40.07%\n",
      "Epoch: 1. Batch 590/4317 - Avg Loss: 1.0985 - Accuracy: 40.04%\n",
      "Epoch: 1. Batch 600/4317 - Avg Loss: 1.0985 - Accuracy: 39.91%\n",
      "Epoch: 1. Batch 610/4317 - Avg Loss: 1.0985 - Accuracy: 39.81%\n",
      "Epoch: 1. Batch 620/4317 - Avg Loss: 1.0985 - Accuracy: 39.70%\n",
      "Epoch: 1. Batch 630/4317 - Avg Loss: 1.0985 - Accuracy: 39.52%\n",
      "Epoch: 1. Batch 640/4317 - Avg Loss: 1.0984 - Accuracy: 39.64%\n",
      "Epoch: 1. Batch 650/4317 - Avg Loss: 1.0985 - Accuracy: 39.67%\n",
      "Epoch: 1. Batch 660/4317 - Avg Loss: 1.0985 - Accuracy: 39.63%\n",
      "Epoch: 1. Batch 670/4317 - Avg Loss: 1.0985 - Accuracy: 39.63%\n",
      "Epoch: 1. Batch 680/4317 - Avg Loss: 1.0985 - Accuracy: 39.57%\n",
      "Epoch: 1. Batch 690/4317 - Avg Loss: 1.0985 - Accuracy: 39.51%\n",
      "Epoch: 1. Batch 700/4317 - Avg Loss: 1.0985 - Accuracy: 39.41%\n",
      "Epoch: 1. Batch 710/4317 - Avg Loss: 1.0984 - Accuracy: 39.36%\n",
      "Epoch: 1. Batch 720/4317 - Avg Loss: 1.0985 - Accuracy: 39.26%\n",
      "Epoch: 1. Batch 730/4317 - Avg Loss: 1.0984 - Accuracy: 39.26%\n",
      "Epoch: 1. Batch 740/4317 - Avg Loss: 1.0985 - Accuracy: 39.33%\n",
      "Epoch: 1. Batch 750/4317 - Avg Loss: 1.0984 - Accuracy: 39.42%\n",
      "Epoch: 1. Batch 760/4317 - Avg Loss: 1.0985 - Accuracy: 39.47%\n",
      "Epoch: 1. Batch 770/4317 - Avg Loss: 1.0985 - Accuracy: 39.41%\n",
      "Epoch: 1. Batch 780/4317 - Avg Loss: 1.0985 - Accuracy: 39.52%\n",
      "Epoch: 1. Batch 790/4317 - Avg Loss: 1.0985 - Accuracy: 39.51%\n",
      "Epoch: 1. Batch 800/4317 - Avg Loss: 1.0986 - Accuracy: 39.50%\n",
      "Epoch: 1. Batch 810/4317 - Avg Loss: 1.0986 - Accuracy: 39.57%\n",
      "Epoch: 1. Batch 820/4317 - Avg Loss: 1.0986 - Accuracy: 39.65%\n",
      "Epoch: 1. Batch 830/4317 - Avg Loss: 1.0986 - Accuracy: 39.74%\n",
      "Epoch: 1. Batch 840/4317 - Avg Loss: 1.0986 - Accuracy: 39.76%\n",
      "Epoch: 1. Batch 850/4317 - Avg Loss: 1.0986 - Accuracy: 39.79%\n",
      "Epoch: 1. Batch 860/4317 - Avg Loss: 1.0986 - Accuracy: 39.73%\n",
      "Epoch: 1. Batch 870/4317 - Avg Loss: 1.0986 - Accuracy: 39.79%\n",
      "Epoch: 1. Batch 880/4317 - Avg Loss: 1.0986 - Accuracy: 39.88%\n",
      "Epoch: 1. Batch 890/4317 - Avg Loss: 1.0986 - Accuracy: 39.96%\n",
      "Epoch: 1. Batch 900/4317 - Avg Loss: 1.0986 - Accuracy: 40.04%\n",
      "Epoch: 1. Batch 910/4317 - Avg Loss: 1.0986 - Accuracy: 40.02%\n",
      "Epoch: 1. Batch 920/4317 - Avg Loss: 1.0986 - Accuracy: 40.02%\n",
      "Epoch: 1. Batch 930/4317 - Avg Loss: 1.0986 - Accuracy: 40.02%\n",
      "Epoch: 1. Batch 940/4317 - Avg Loss: 1.0986 - Accuracy: 40.01%\n",
      "Epoch: 1. Batch 950/4317 - Avg Loss: 1.0986 - Accuracy: 40.05%\n",
      "Epoch: 1. Batch 960/4317 - Avg Loss: 1.0986 - Accuracy: 40.09%\n",
      "Epoch: 1. Batch 970/4317 - Avg Loss: 1.0986 - Accuracy: 40.14%\n",
      "Epoch: 1. Batch 980/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 990/4317 - Avg Loss: 1.0986 - Accuracy: 40.19%\n",
      "Epoch: 1. Batch 1000/4317 - Avg Loss: 1.0986 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 1010/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 1020/4317 - Avg Loss: 1.0986 - Accuracy: 40.21%\n",
      "Epoch: 1. Batch 1030/4317 - Avg Loss: 1.0986 - Accuracy: 40.21%\n",
      "Epoch: 1. Batch 1040/4317 - Avg Loss: 1.0986 - Accuracy: 40.27%\n",
      "Epoch: 1. Batch 1050/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1060/4317 - Avg Loss: 1.0986 - Accuracy: 40.31%\n",
      "Epoch: 1. Batch 1070/4317 - Avg Loss: 1.0986 - Accuracy: 40.31%\n",
      "Epoch: 1. Batch 1080/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 1090/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 1100/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 1110/4317 - Avg Loss: 1.0986 - Accuracy: 40.34%\n",
      "Epoch: 1. Batch 1120/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1130/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1140/4317 - Avg Loss: 1.0986 - Accuracy: 40.28%\n",
      "Epoch: 1. Batch 1150/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 1160/4317 - Avg Loss: 1.0986 - Accuracy: 40.19%\n",
      "Epoch: 1. Batch 1170/4317 - Avg Loss: 1.0986 - Accuracy: 40.12%\n",
      "Epoch: 1. Batch 1180/4317 - Avg Loss: 1.0986 - Accuracy: 40.09%\n",
      "Epoch: 1. Batch 1190/4317 - Avg Loss: 1.0986 - Accuracy: 40.09%\n",
      "Epoch: 1. Batch 1200/4317 - Avg Loss: 1.0986 - Accuracy: 40.03%\n",
      "Epoch: 1. Batch 1210/4317 - Avg Loss: 1.0986 - Accuracy: 39.95%\n",
      "Epoch: 1. Batch 1220/4317 - Avg Loss: 1.0986 - Accuracy: 39.93%\n",
      "Epoch: 1. Batch 1230/4317 - Avg Loss: 1.0986 - Accuracy: 39.90%\n",
      "Epoch: 1. Batch 1240/4317 - Avg Loss: 1.0986 - Accuracy: 39.87%\n",
      "Epoch: 1. Batch 1250/4317 - Avg Loss: 1.0986 - Accuracy: 39.80%\n",
      "Epoch: 1. Batch 1260/4317 - Avg Loss: 1.0986 - Accuracy: 39.79%\n",
      "Epoch: 1. Batch 1270/4317 - Avg Loss: 1.0986 - Accuracy: 39.76%\n",
      "Epoch: 1. Batch 1280/4317 - Avg Loss: 1.0986 - Accuracy: 39.72%\n",
      "Epoch: 1. Batch 1290/4317 - Avg Loss: 1.0986 - Accuracy: 39.63%\n",
      "Epoch: 1. Batch 1300/4317 - Avg Loss: 1.0986 - Accuracy: 39.58%\n",
      "Epoch: 1. Batch 1310/4317 - Avg Loss: 1.0986 - Accuracy: 39.48%\n",
      "Epoch: 1. Batch 1320/4317 - Avg Loss: 1.0986 - Accuracy: 39.49%\n",
      "Epoch: 1. Batch 1330/4317 - Avg Loss: 1.0986 - Accuracy: 39.53%\n",
      "Epoch: 1. Batch 1340/4317 - Avg Loss: 1.0986 - Accuracy: 39.56%\n",
      "Epoch: 1. Batch 1350/4317 - Avg Loss: 1.0986 - Accuracy: 39.59%\n",
      "Epoch: 1. Batch 1360/4317 - Avg Loss: 1.0986 - Accuracy: 39.65%\n",
      "Epoch: 1. Batch 1370/4317 - Avg Loss: 1.0986 - Accuracy: 39.64%\n",
      "Epoch: 1. Batch 1380/4317 - Avg Loss: 1.0986 - Accuracy: 39.68%\n",
      "Epoch: 1. Batch 1390/4317 - Avg Loss: 1.0986 - Accuracy: 39.66%\n",
      "Epoch: 1. Batch 1400/4317 - Avg Loss: 1.0986 - Accuracy: 39.71%\n",
      "Epoch: 1. Batch 1410/4317 - Avg Loss: 1.0986 - Accuracy: 39.71%\n",
      "Epoch: 1. Batch 1420/4317 - Avg Loss: 1.0986 - Accuracy: 39.77%\n",
      "Epoch: 1. Batch 1430/4317 - Avg Loss: 1.0985 - Accuracy: 39.81%\n",
      "Epoch: 1. Batch 1440/4317 - Avg Loss: 1.0986 - Accuracy: 39.80%\n",
      "Epoch: 1. Batch 1450/4317 - Avg Loss: 1.0986 - Accuracy: 39.76%\n",
      "Epoch: 1. Batch 1460/4317 - Avg Loss: 1.0986 - Accuracy: 39.71%\n",
      "Epoch: 1. Batch 1470/4317 - Avg Loss: 1.0986 - Accuracy: 39.73%\n",
      "Epoch: 1. Batch 1480/4317 - Avg Loss: 1.0986 - Accuracy: 39.75%\n",
      "Epoch: 1. Batch 1490/4317 - Avg Loss: 1.0986 - Accuracy: 39.77%\n",
      "Epoch: 1. Batch 1500/4317 - Avg Loss: 1.0986 - Accuracy: 39.81%\n",
      "Epoch: 1. Batch 1510/4317 - Avg Loss: 1.0986 - Accuracy: 39.82%\n",
      "Epoch: 1. Batch 1520/4317 - Avg Loss: 1.0986 - Accuracy: 39.83%\n",
      "Epoch: 1. Batch 1530/4317 - Avg Loss: 1.0986 - Accuracy: 39.85%\n",
      "Epoch: 1. Batch 1540/4317 - Avg Loss: 1.0986 - Accuracy: 39.85%\n",
      "Epoch: 1. Batch 1550/4317 - Avg Loss: 1.0986 - Accuracy: 39.87%\n",
      "Epoch: 1. Batch 1560/4317 - Avg Loss: 1.0986 - Accuracy: 39.89%\n",
      "Epoch: 1. Batch 1570/4317 - Avg Loss: 1.0986 - Accuracy: 39.94%\n",
      "Epoch: 1. Batch 1580/4317 - Avg Loss: 1.0986 - Accuracy: 39.91%\n",
      "Epoch: 1. Batch 1590/4317 - Avg Loss: 1.0986 - Accuracy: 39.93%\n",
      "Epoch: 1. Batch 1600/4317 - Avg Loss: 1.0986 - Accuracy: 39.96%\n",
      "Epoch: 1. Batch 1610/4317 - Avg Loss: 1.0986 - Accuracy: 39.95%\n",
      "Epoch: 1. Batch 1620/4317 - Avg Loss: 1.0986 - Accuracy: 39.98%\n",
      "Epoch: 1. Batch 1630/4317 - Avg Loss: 1.0986 - Accuracy: 39.93%\n",
      "Epoch: 1. Batch 1640/4317 - Avg Loss: 1.0986 - Accuracy: 39.94%\n",
      "Epoch: 1. Batch 1650/4317 - Avg Loss: 1.0986 - Accuracy: 39.87%\n",
      "Epoch: 1. Batch 1660/4317 - Avg Loss: 1.0986 - Accuracy: 39.88%\n",
      "Epoch: 1. Batch 1670/4317 - Avg Loss: 1.0986 - Accuracy: 39.91%\n",
      "Epoch: 1. Batch 1680/4317 - Avg Loss: 1.0986 - Accuracy: 39.95%\n",
      "Epoch: 1. Batch 1690/4317 - Avg Loss: 1.0986 - Accuracy: 39.99%\n",
      "Epoch: 1. Batch 1700/4317 - Avg Loss: 1.0986 - Accuracy: 40.03%\n",
      "Epoch: 1. Batch 1710/4317 - Avg Loss: 1.0986 - Accuracy: 40.05%\n",
      "Epoch: 1. Batch 1720/4317 - Avg Loss: 1.0986 - Accuracy: 40.07%\n",
      "Epoch: 1. Batch 1730/4317 - Avg Loss: 1.0986 - Accuracy: 40.07%\n",
      "Epoch: 1. Batch 1740/4317 - Avg Loss: 1.0986 - Accuracy: 40.11%\n",
      "Epoch: 1. Batch 1750/4317 - Avg Loss: 1.0986 - Accuracy: 40.12%\n",
      "Epoch: 1. Batch 1760/4317 - Avg Loss: 1.0986 - Accuracy: 40.11%\n",
      "Epoch: 1. Batch 1770/4317 - Avg Loss: 1.0986 - Accuracy: 40.08%\n",
      "Epoch: 1. Batch 1780/4317 - Avg Loss: 1.0987 - Accuracy: 40.07%\n",
      "Epoch: 1. Batch 1790/4317 - Avg Loss: 1.0987 - Accuracy: 40.05%\n",
      "Epoch: 1. Batch 1800/4317 - Avg Loss: 1.0986 - Accuracy: 40.12%\n",
      "Epoch: 1. Batch 1810/4317 - Avg Loss: 1.0986 - Accuracy: 40.14%\n",
      "Epoch: 1. Batch 1820/4317 - Avg Loss: 1.0986 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 1830/4317 - Avg Loss: 1.0986 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 1840/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 1850/4317 - Avg Loss: 1.0986 - Accuracy: 40.21%\n",
      "Epoch: 1. Batch 1860/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 1870/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 1880/4317 - Avg Loss: 1.0986 - Accuracy: 40.26%\n",
      "Epoch: 1. Batch 1890/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 1900/4317 - Avg Loss: 1.0986 - Accuracy: 40.28%\n",
      "Epoch: 1. Batch 1910/4317 - Avg Loss: 1.0986 - Accuracy: 40.31%\n",
      "Epoch: 1. Batch 1920/4317 - Avg Loss: 1.0986 - Accuracy: 40.31%\n",
      "Epoch: 1. Batch 1930/4317 - Avg Loss: 1.0986 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 1940/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1950/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1960/4317 - Avg Loss: 1.0986 - Accuracy: 40.34%\n",
      "Epoch: 1. Batch 1970/4317 - Avg Loss: 1.0986 - Accuracy: 40.34%\n",
      "Epoch: 1. Batch 1980/4317 - Avg Loss: 1.0986 - Accuracy: 40.33%\n",
      "Epoch: 1. Batch 1990/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 2000/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 2010/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2020/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2030/4317 - Avg Loss: 1.0986 - Accuracy: 40.43%\n",
      "Epoch: 1. Batch 2040/4317 - Avg Loss: 1.0986 - Accuracy: 40.43%\n",
      "Epoch: 1. Batch 2050/4317 - Avg Loss: 1.0986 - Accuracy: 40.43%\n",
      "Epoch: 1. Batch 2060/4317 - Avg Loss: 1.0986 - Accuracy: 40.42%\n",
      "Epoch: 1. Batch 2070/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2080/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2090/4317 - Avg Loss: 1.0987 - Accuracy: 40.32%\n",
      "Epoch: 1. Batch 2100/4317 - Avg Loss: 1.0986 - Accuracy: 40.31%\n",
      "Epoch: 1. Batch 2110/4317 - Avg Loss: 1.0986 - Accuracy: 40.27%\n",
      "Epoch: 1. Batch 2120/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 2130/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 2140/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 2150/4317 - Avg Loss: 1.0986 - Accuracy: 40.18%\n",
      "Epoch: 1. Batch 2160/4317 - Avg Loss: 1.0986 - Accuracy: 40.18%\n",
      "Epoch: 1. Batch 2170/4317 - Avg Loss: 1.0986 - Accuracy: 40.15%\n",
      "Epoch: 1. Batch 2180/4317 - Avg Loss: 1.0986 - Accuracy: 40.14%\n",
      "Epoch: 1. Batch 2190/4317 - Avg Loss: 1.0986 - Accuracy: 40.11%\n",
      "Epoch: 1. Batch 2200/4317 - Avg Loss: 1.0986 - Accuracy: 40.05%\n",
      "Epoch: 1. Batch 2210/4317 - Avg Loss: 1.0986 - Accuracy: 40.03%\n",
      "Epoch: 1. Batch 2220/4317 - Avg Loss: 1.0986 - Accuracy: 40.00%\n",
      "Epoch: 1. Batch 2230/4317 - Avg Loss: 1.0986 - Accuracy: 39.98%\n",
      "Epoch: 1. Batch 2240/4317 - Avg Loss: 1.0986 - Accuracy: 39.94%\n",
      "Epoch: 1. Batch 2250/4317 - Avg Loss: 1.0986 - Accuracy: 39.96%\n",
      "Epoch: 1. Batch 2260/4317 - Avg Loss: 1.0986 - Accuracy: 39.97%\n",
      "Epoch: 1. Batch 2270/4317 - Avg Loss: 1.0986 - Accuracy: 39.99%\n",
      "Epoch: 1. Batch 2280/4317 - Avg Loss: 1.0986 - Accuracy: 40.00%\n",
      "Epoch: 1. Batch 2290/4317 - Avg Loss: 1.0986 - Accuracy: 40.02%\n",
      "Epoch: 1. Batch 2300/4317 - Avg Loss: 1.0986 - Accuracy: 40.04%\n",
      "Epoch: 1. Batch 2310/4317 - Avg Loss: 1.0986 - Accuracy: 40.04%\n",
      "Epoch: 1. Batch 2320/4317 - Avg Loss: 1.0986 - Accuracy: 40.06%\n",
      "Epoch: 1. Batch 2330/4317 - Avg Loss: 1.0986 - Accuracy: 40.08%\n",
      "Epoch: 1. Batch 2340/4317 - Avg Loss: 1.0986 - Accuracy: 40.12%\n",
      "Epoch: 1. Batch 2350/4317 - Avg Loss: 1.0986 - Accuracy: 40.14%\n",
      "Epoch: 1. Batch 2360/4317 - Avg Loss: 1.0986 - Accuracy: 40.15%\n",
      "Epoch: 1. Batch 2370/4317 - Avg Loss: 1.0986 - Accuracy: 40.15%\n",
      "Epoch: 1. Batch 2380/4317 - Avg Loss: 1.0986 - Accuracy: 40.16%\n",
      "Epoch: 1. Batch 2390/4317 - Avg Loss: 1.0986 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 2400/4317 - Avg Loss: 1.0986 - Accuracy: 40.16%\n",
      "Epoch: 1. Batch 2410/4317 - Avg Loss: 1.0986 - Accuracy: 40.16%\n",
      "Epoch: 1. Batch 2420/4317 - Avg Loss: 1.0986 - Accuracy: 40.19%\n",
      "Epoch: 1. Batch 2430/4317 - Avg Loss: 1.0986 - Accuracy: 40.22%\n",
      "Epoch: 1. Batch 2440/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 2450/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 2460/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 2470/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 2480/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 2490/4317 - Avg Loss: 1.0986 - Accuracy: 40.22%\n",
      "Epoch: 1. Batch 2500/4317 - Avg Loss: 1.0986 - Accuracy: 40.22%\n",
      "Epoch: 1. Batch 2510/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 2520/4317 - Avg Loss: 1.0986 - Accuracy: 40.17%\n",
      "Epoch: 1. Batch 2530/4317 - Avg Loss: 1.0986 - Accuracy: 40.16%\n",
      "Epoch: 1. Batch 2540/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 2550/4317 - Avg Loss: 1.0986 - Accuracy: 40.20%\n",
      "Epoch: 1. Batch 2560/4317 - Avg Loss: 1.0986 - Accuracy: 40.22%\n",
      "Epoch: 1. Batch 2570/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 2580/4317 - Avg Loss: 1.0986 - Accuracy: 40.23%\n",
      "Epoch: 1. Batch 2590/4317 - Avg Loss: 1.0986 - Accuracy: 40.25%\n",
      "Epoch: 1. Batch 2600/4317 - Avg Loss: 1.0986 - Accuracy: 40.25%\n",
      "Epoch: 1. Batch 2610/4317 - Avg Loss: 1.0986 - Accuracy: 40.26%\n",
      "Epoch: 1. Batch 2620/4317 - Avg Loss: 1.0986 - Accuracy: 40.24%\n",
      "Epoch: 1. Batch 2630/4317 - Avg Loss: 1.0986 - Accuracy: 40.26%\n",
      "Epoch: 1. Batch 2640/4317 - Avg Loss: 1.0986 - Accuracy: 40.28%\n",
      "Epoch: 1. Batch 2650/4317 - Avg Loss: 1.0986 - Accuracy: 40.32%\n",
      "Epoch: 1. Batch 2660/4317 - Avg Loss: 1.0986 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 2670/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 2680/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2690/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2700/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2710/4317 - Avg Loss: 1.0986 - Accuracy: 40.36%\n",
      "Epoch: 1. Batch 2720/4317 - Avg Loss: 1.0986 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 2730/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2740/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2750/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2760/4317 - Avg Loss: 1.0986 - Accuracy: 40.42%\n",
      "Epoch: 1. Batch 2770/4317 - Avg Loss: 1.0986 - Accuracy: 40.41%\n",
      "Epoch: 1. Batch 2780/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2790/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2800/4317 - Avg Loss: 1.0986 - Accuracy: 40.41%\n",
      "Epoch: 1. Batch 2810/4317 - Avg Loss: 1.0986 - Accuracy: 40.41%\n",
      "Epoch: 1. Batch 2820/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2830/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2840/4317 - Avg Loss: 1.0986 - Accuracy: 40.35%\n",
      "Epoch: 1. Batch 2850/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2860/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2870/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2880/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2890/4317 - Avg Loss: 1.0986 - Accuracy: 40.37%\n",
      "Epoch: 1. Batch 2900/4317 - Avg Loss: 1.0986 - Accuracy: 40.38%\n",
      "Epoch: 1. Batch 2910/4317 - Avg Loss: 1.0986 - Accuracy: 40.39%\n",
      "Epoch: 1. Batch 2920/4317 - Avg Loss: 1.0986 - Accuracy: 40.41%\n",
      "Epoch: 1. Batch 2930/4317 - Avg Loss: 1.0986 - Accuracy: 40.43%\n",
      "Epoch: 1. Batch 2940/4317 - Avg Loss: 1.0986 - Accuracy: 40.44%\n",
      "Epoch: 1. Batch 2950/4317 - Avg Loss: 1.0986 - Accuracy: 40.45%\n",
      "Epoch: 1. Batch 2960/4317 - Avg Loss: 1.0986 - Accuracy: 40.47%\n",
      "Epoch: 1. Batch 2970/4317 - Avg Loss: 1.0986 - Accuracy: 40.47%\n",
      "Epoch: 1. Batch 2980/4317 - Avg Loss: 1.0986 - Accuracy: 40.49%\n",
      "Epoch: 1. Batch 2990/4317 - Avg Loss: 1.0986 - Accuracy: 40.52%\n",
      "Epoch: 1. Batch 3000/4317 - Avg Loss: 1.0986 - Accuracy: 40.54%\n",
      "Epoch: 1. Batch 3010/4317 - Avg Loss: 1.0986 - Accuracy: 40.54%\n",
      "Epoch: 1. Batch 3020/4317 - Avg Loss: 1.0986 - Accuracy: 40.55%\n",
      "Epoch: 1. Batch 3030/4317 - Avg Loss: 1.0986 - Accuracy: 40.56%\n",
      "Epoch: 1. Batch 3040/4317 - Avg Loss: 1.0986 - Accuracy: 40.55%\n",
      "Epoch: 1. Batch 3050/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3060/4317 - Avg Loss: 1.0986 - Accuracy: 40.56%\n",
      "Epoch: 1. Batch 3070/4317 - Avg Loss: 1.0986 - Accuracy: 40.56%\n",
      "Epoch: 1. Batch 3080/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3090/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3100/4317 - Avg Loss: 1.0986 - Accuracy: 40.59%\n",
      "Epoch: 1. Batch 3110/4317 - Avg Loss: 1.0986 - Accuracy: 40.56%\n",
      "Epoch: 1. Batch 3120/4317 - Avg Loss: 1.0986 - Accuracy: 40.57%\n",
      "Epoch: 1. Batch 3130/4317 - Avg Loss: 1.0986 - Accuracy: 40.59%\n",
      "Epoch: 1. Batch 3140/4317 - Avg Loss: 1.0986 - Accuracy: 40.59%\n",
      "Epoch: 1. Batch 3150/4317 - Avg Loss: 1.0986 - Accuracy: 40.60%\n",
      "Epoch: 1. Batch 3160/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3170/4317 - Avg Loss: 1.0986 - Accuracy: 40.61%\n",
      "Epoch: 1. Batch 3180/4317 - Avg Loss: 1.0986 - Accuracy: 40.63%\n",
      "Epoch: 1. Batch 3190/4317 - Avg Loss: 1.0986 - Accuracy: 40.65%\n",
      "Epoch: 1. Batch 3200/4317 - Avg Loss: 1.0986 - Accuracy: 40.64%\n",
      "Epoch: 1. Batch 3210/4317 - Avg Loss: 1.0986 - Accuracy: 40.68%\n",
      "Epoch: 1. Batch 3220/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3230/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3240/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3250/4317 - Avg Loss: 1.0986 - Accuracy: 40.72%\n",
      "Epoch: 1. Batch 3260/4317 - Avg Loss: 1.0986 - Accuracy: 40.70%\n",
      "Epoch: 1. Batch 3270/4317 - Avg Loss: 1.0985 - Accuracy: 40.72%\n",
      "Epoch: 1. Batch 3280/4317 - Avg Loss: 1.0985 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 3290/4317 - Avg Loss: 1.0985 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 3300/4317 - Avg Loss: 1.0986 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 3310/4317 - Avg Loss: 1.0986 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 3320/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 3330/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3340/4317 - Avg Loss: 1.0986 - Accuracy: 40.77%\n",
      "Epoch: 1. Batch 3350/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3360/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 3370/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3380/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3390/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3400/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3410/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3420/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 3430/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3440/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3450/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3460/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3470/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 3480/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3490/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3500/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 3510/4317 - Avg Loss: 1.0986 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 3520/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3530/4317 - Avg Loss: 1.0986 - Accuracy: 40.68%\n",
      "Epoch: 1. Batch 3540/4317 - Avg Loss: 1.0986 - Accuracy: 40.65%\n",
      "Epoch: 1. Batch 3550/4317 - Avg Loss: 1.0986 - Accuracy: 40.66%\n",
      "Epoch: 1. Batch 3560/4317 - Avg Loss: 1.0986 - Accuracy: 40.65%\n",
      "Epoch: 1. Batch 3570/4317 - Avg Loss: 1.0986 - Accuracy: 40.62%\n",
      "Epoch: 1. Batch 3580/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3590/4317 - Avg Loss: 1.0986 - Accuracy: 40.58%\n",
      "Epoch: 1. Batch 3600/4317 - Avg Loss: 1.0986 - Accuracy: 40.60%\n",
      "Epoch: 1. Batch 3610/4317 - Avg Loss: 1.0986 - Accuracy: 40.61%\n",
      "Epoch: 1. Batch 3620/4317 - Avg Loss: 1.0986 - Accuracy: 40.61%\n",
      "Epoch: 1. Batch 3630/4317 - Avg Loss: 1.0986 - Accuracy: 40.61%\n",
      "Epoch: 1. Batch 3640/4317 - Avg Loss: 1.0986 - Accuracy: 40.62%\n",
      "Epoch: 1. Batch 3650/4317 - Avg Loss: 1.0986 - Accuracy: 40.64%\n",
      "Epoch: 1. Batch 3660/4317 - Avg Loss: 1.0986 - Accuracy: 40.66%\n",
      "Epoch: 1. Batch 3670/4317 - Avg Loss: 1.0986 - Accuracy: 40.67%\n",
      "Epoch: 1. Batch 3680/4317 - Avg Loss: 1.0986 - Accuracy: 40.69%\n",
      "Epoch: 1. Batch 3690/4317 - Avg Loss: 1.0986 - Accuracy: 40.70%\n",
      "Epoch: 1. Batch 3700/4317 - Avg Loss: 1.0986 - Accuracy: 40.68%\n",
      "Epoch: 1. Batch 3710/4317 - Avg Loss: 1.0986 - Accuracy: 40.69%\n",
      "Epoch: 1. Batch 3720/4317 - Avg Loss: 1.0986 - Accuracy: 40.70%\n",
      "Epoch: 1. Batch 3730/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3740/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3750/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3760/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 3770/4317 - Avg Loss: 1.0986 - Accuracy: 40.72%\n",
      "Epoch: 1. Batch 3780/4317 - Avg Loss: 1.0986 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 3790/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 3800/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 3810/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3820/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3830/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3840/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3850/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 3860/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3870/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3880/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 3890/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3900/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3910/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3920/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3930/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 3940/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 3950/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3960/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 3970/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 3980/4317 - Avg Loss: 1.0986 - Accuracy: 40.81%\n",
      "Epoch: 1. Batch 3990/4317 - Avg Loss: 1.0986 - Accuracy: 40.81%\n",
      "Epoch: 1. Batch 4000/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4010/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4020/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4030/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4040/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4050/4317 - Avg Loss: 1.0986 - Accuracy: 40.84%\n",
      "Epoch: 1. Batch 4060/4317 - Avg Loss: 1.0986 - Accuracy: 40.84%\n",
      "Epoch: 1. Batch 4070/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4080/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4090/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4100/4317 - Avg Loss: 1.0986 - Accuracy: 40.82%\n",
      "Epoch: 1. Batch 4110/4317 - Avg Loss: 1.0986 - Accuracy: 40.80%\n",
      "Epoch: 1. Batch 4120/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 4130/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 4140/4317 - Avg Loss: 1.0986 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 4150/4317 - Avg Loss: 1.0986 - Accuracy: 40.72%\n",
      "Epoch: 1. Batch 4160/4317 - Avg Loss: 1.0986 - Accuracy: 40.71%\n",
      "Epoch: 1. Batch 4170/4317 - Avg Loss: 1.0986 - Accuracy: 40.72%\n",
      "Epoch: 1. Batch 4180/4317 - Avg Loss: 1.0986 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 4190/4317 - Avg Loss: 1.0986 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 4200/4317 - Avg Loss: 1.0986 - Accuracy: 40.73%\n",
      "Epoch: 1. Batch 4210/4317 - Avg Loss: 1.0986 - Accuracy: 40.74%\n",
      "Epoch: 1. Batch 4220/4317 - Avg Loss: 1.0986 - Accuracy: 40.75%\n",
      "Epoch: 1. Batch 4230/4317 - Avg Loss: 1.0986 - Accuracy: 40.76%\n",
      "Epoch: 1. Batch 4240/4317 - Avg Loss: 1.0986 - Accuracy: 40.77%\n",
      "Epoch: 1. Batch 4250/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 4260/4317 - Avg Loss: 1.0986 - Accuracy: 40.77%\n",
      "Epoch: 1. Batch 4270/4317 - Avg Loss: 1.0986 - Accuracy: 40.77%\n",
      "Epoch: 1. Batch 4280/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 4290/4317 - Avg Loss: 1.0986 - Accuracy: 40.78%\n",
      "Epoch: 1. Batch 4300/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Epoch: 1. Batch 4310/4317 - Avg Loss: 1.0986 - Accuracy: 40.79%\n",
      "Train loss: 1.0986 - Train accuracy: 40.79%\n",
      "Validation accuracy: 42.7123\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 42.7162%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 39.85%        | 34.37%              |\n",
    "| **Epoch 2** | 40.79%        | 42.71%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './LSTM_sentiment_model/lstm_sentiment_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
