{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: LSTM (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a5a3df2e3ce62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.8245 - Accuracy: 6.25%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 1.8687 - Accuracy: 10.23%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 1.8337 - Accuracy: 13.39%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 1.8327 - Accuracy: 13.31%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.8226 - Accuracy: 14.18%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.8174 - Accuracy: 14.95%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.8151 - Accuracy: 14.75%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.8118 - Accuracy: 15.23%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.8120 - Accuracy: 14.97%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.8099 - Accuracy: 15.32%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.8084 - Accuracy: 15.90%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.8096 - Accuracy: 15.43%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.8080 - Accuracy: 15.60%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.8072 - Accuracy: 15.60%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.8076 - Accuracy: 15.38%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.8067 - Accuracy: 15.31%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.8061 - Accuracy: 15.37%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.8054 - Accuracy: 15.20%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.8039 - Accuracy: 15.23%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.8040 - Accuracy: 15.35%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.8040 - Accuracy: 15.36%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.8036 - Accuracy: 15.43%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.8029 - Accuracy: 15.55%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.8025 - Accuracy: 15.53%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.8024 - Accuracy: 15.48%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.8022 - Accuracy: 15.56%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.8017 - Accuracy: 15.71%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.8013 - Accuracy: 15.82%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.8012 - Accuracy: 15.90%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.8011 - Accuracy: 15.96%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 1.8009 - Accuracy: 15.93%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 1.8007 - Accuracy: 15.88%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 1.8005 - Accuracy: 15.83%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 1.8003 - Accuracy: 15.90%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 1.8004 - Accuracy: 15.93%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 1.8002 - Accuracy: 15.99%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 1.8003 - Accuracy: 15.84%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 1.8000 - Accuracy: 15.90%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 1.7999 - Accuracy: 15.86%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 1.7996 - Accuracy: 16.00%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 1.7993 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 1.7989 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 1.7987 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 1.7985 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 1.7985 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 1.7985 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 1.7984 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 1.7983 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 1.7981 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 1.7980 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 1.7979 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 1.7978 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 1.7978 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 1.7977 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 1.7977 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 1.7976 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 1.7976 - Accuracy: 16.11%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 1.7975 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 1.7974 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 1.7974 - Accuracy: 16.09%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 1.7972 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 1.7972 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 1.7971 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 1.7971 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 1.7970 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 1.7969 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 1.7969 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 1.7969 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 1.7968 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 1.7967 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 1.7967 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 1.7966 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 1.7966 - Accuracy: 16.10%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 1.7966 - Accuracy: 15.98%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 1.7965 - Accuracy: 15.96%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 1.7964 - Accuracy: 15.89%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 1.7964 - Accuracy: 15.94%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 1.7962 - Accuracy: 15.99%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 1.7963 - Accuracy: 16.00%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 1.7961 - Accuracy: 16.02%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 1.7960 - Accuracy: 16.04%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 1.7960 - Accuracy: 16.09%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 1.7959 - Accuracy: 16.06%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 1.7959 - Accuracy: 16.08%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 1.7960 - Accuracy: 16.03%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 1.7960 - Accuracy: 16.01%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 1.7959 - Accuracy: 16.00%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 1.7958 - Accuracy: 16.03%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 1.7958 - Accuracy: 16.05%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 1.7958 - Accuracy: 16.06%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 1.7958 - Accuracy: 16.07%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 1.7958 - Accuracy: 16.09%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 1.7958 - Accuracy: 16.10%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 1.7957 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 1.7957 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 1.7956 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 1.7956 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 1.7956 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 1.7956 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 1.7955 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 1.7955 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 1.7955 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 1.7954 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 1.7954 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 1.7954 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 1.7954 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 1.7954 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 1.7953 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 1.7953 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 1.7952 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 1.7952 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 1.7952 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 1.7951 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 1.7952 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 1.7951 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 1.7950 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 1.7950 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 1.7949 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 1.7949 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 1.7949 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 1.7949 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 1.7948 - Accuracy: 16.40%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 1.7948 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 1.7949 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 1.7949 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 1.7949 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 1.7949 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 1.7949 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 1.7949 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 1.7949 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 1.7949 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 1.7948 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 1.7948 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 1.7948 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 1.7948 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 1.7948 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 1.7947 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 1.7947 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 1.7947 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 1.7947 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 1.7947 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 1.7947 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 1.7946 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 1.7946 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 1.7945 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 1.7945 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 1.7945 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 1.7945 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 1.7945 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 1.7945 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 1.7945 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 1.7945 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 1.7945 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 1.7945 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 1.7945 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 1.7945 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 1.7945 - Accuracy: 16.14%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 1.7945 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 1.7945 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 1.7945 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 1.7944 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 1.7944 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 1.7944 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 1.7944 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 1.7944 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 1.7944 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 1.7944 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 1.7944 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 1.7944 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 1.7943 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 1.7943 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 1.7943 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 1.7943 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 1.7943 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 1.7943 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 1.7942 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 1.7942 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 1.7942 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 1.7942 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 1.7942 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 1.7941 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 1.7941 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 1.7941 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 1.7941 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 1.7942 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 1.7942 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 1.7942 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 1.7942 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 1.7942 - Accuracy: 16.15%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 1.7941 - Accuracy: 16.16%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 1.7941 - Accuracy: 16.17%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 1.7941 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 1.7941 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 1.7941 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 1.7941 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 1.7941 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 1.7941 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 1.7941 - Accuracy: 16.19%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 1.7941 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 1.7941 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 1.7940 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 1.7940 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 1.7940 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 1.7940 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 1.7940 - Accuracy: 16.21%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 1.7940 - Accuracy: 16.18%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 1.7940 - Accuracy: 16.20%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 1.7940 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 1.7940 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 1.7939 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 1.7940 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 1.7939 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 1.7939 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 1.7939 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 1.7939 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 1.7939 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 1.7939 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 1.7939 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 1.7939 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 1.7939 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 1.7939 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 1.7938 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 1.7938 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 1.7938 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 1.7938 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 1.7938 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 1.7938 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 1.7938 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 1.7938 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 1.7938 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 1.7937 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 1.7937 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 1.7937 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 1.7937 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 1.7937 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 1.7937 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 1.7937 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 1.7937 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 1.7937 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 1.7937 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 1.7937 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 1.7937 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 1.7936 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 1.7936 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 1.7936 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 1.7936 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 1.7936 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 1.7936 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 1.7936 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 1.7936 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 1.7936 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 1.7936 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 1.7936 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 1.7936 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 1.7936 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 1.7936 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 1.7936 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 1.7936 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 1.7936 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 1.7936 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 1.7936 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 1.7936 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 1.7936 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 1.7936 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 1.7936 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 1.7936 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 1.7936 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 1.7935 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 1.7935 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 1.7935 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 1.7935 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 1.7935 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 1.7935 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 1.7935 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 1.7935 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 1.7935 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 1.7935 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 1.7935 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 1.7935 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 1.7935 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 1.7934 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 1.7934 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 1.7934 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 1.7934 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 1.7934 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 1.7934 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 1.7934 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 1.7934 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 1.7934 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 1.7935 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 1.7934 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 1.7935 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 1.7935 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 1.7934 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 1.7934 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 1.7934 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 1.7934 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 1.7934 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 1.7934 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 1.7934 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 1.7934 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 1.7934 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 1.7934 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 1.7934 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 1.7934 - Accuracy: 16.25%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 1.7934 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 1.7934 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 1.7934 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 1.7934 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 1.7934 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 1.7934 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 1.7934 - Accuracy: 16.24%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 1.7934 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 1.7934 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 1.7934 - Accuracy: 16.22%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 1.7934 - Accuracy: 16.23%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 1.7934 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 1.7934 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 1.7934 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 1.7934 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 1.7934 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 1.7933 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 1.7933 - Accuracy: 16.27%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 1.7933 - Accuracy: 16.26%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 1.7933 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 1.7933 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 1.7933 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 1.7933 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 1.7933 - Accuracy: 16.28%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 1.7933 - Accuracy: 16.29%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 1.7933 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 1.7932 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 1.7932 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 1.7932 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 1.7932 - Accuracy: 16.30%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 1.7932 - Accuracy: 16.31%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 1.7932 - Accuracy: 16.32%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 1.7932 - Accuracy: 16.33%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 1.7932 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 1.7932 - Accuracy: 16.34%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 1.7932 - Accuracy: 16.35%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 1.7932 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 1.7932 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 1.7932 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 1.7932 - Accuracy: 16.37%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 1.7932 - Accuracy: 16.36%\n",
      "Train loss: 1.7932 - Train accuracy: 16.36%\n",
      "Validation accuracy: 16.9999\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 1.7900 - Accuracy: 6.25%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 1.7926 - Accuracy: 12.50%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 1.7922 - Accuracy: 14.88%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 1.7922 - Accuracy: 14.72%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 1.7922 - Accuracy: 15.40%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 1.7927 - Accuracy: 15.44%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 1.7924 - Accuracy: 15.37%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 1.7923 - Accuracy: 15.32%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 1.7922 - Accuracy: 15.82%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 1.7919 - Accuracy: 16.41%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 1.7920 - Accuracy: 16.34%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 1.7921 - Accuracy: 16.33%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 1.7922 - Accuracy: 16.37%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 1.7923 - Accuracy: 16.41%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 1.7921 - Accuracy: 16.62%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 1.7923 - Accuracy: 16.68%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 1.7924 - Accuracy: 16.50%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 1.7923 - Accuracy: 16.41%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 1.7924 - Accuracy: 16.82%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 1.7922 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 1.7923 - Accuracy: 17.07%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 1.7922 - Accuracy: 17.12%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 1.7922 - Accuracy: 17.05%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 1.7922 - Accuracy: 17.10%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 1.7921 - Accuracy: 17.22%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 1.7921 - Accuracy: 17.03%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 1.7920 - Accuracy: 17.05%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 1.7920 - Accuracy: 17.09%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 1.7918 - Accuracy: 17.06%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 1.7917 - Accuracy: 17.07%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 1.7917 - Accuracy: 17.11%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 1.7916 - Accuracy: 17.20%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 1.7916 - Accuracy: 17.11%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 1.7916 - Accuracy: 17.26%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 1.7916 - Accuracy: 17.28%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 1.7916 - Accuracy: 17.22%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 1.7917 - Accuracy: 17.30%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 1.7917 - Accuracy: 17.33%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 1.7917 - Accuracy: 17.26%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 1.7917 - Accuracy: 17.23%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 1.7917 - Accuracy: 17.27%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 1.7917 - Accuracy: 17.37%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 1.7919 - Accuracy: 17.19%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 1.7918 - Accuracy: 17.18%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 1.7917 - Accuracy: 17.13%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 1.7916 - Accuracy: 17.21%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 1.7916 - Accuracy: 17.15%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 1.7916 - Accuracy: 17.09%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 1.7916 - Accuracy: 17.13%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 1.7916 - Accuracy: 17.22%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 1.7916 - Accuracy: 17.23%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 1.7915 - Accuracy: 17.34%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 1.7916 - Accuracy: 17.42%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 1.7915 - Accuracy: 17.55%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 1.7914 - Accuracy: 17.63%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 1.7914 - Accuracy: 17.64%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 1.7914 - Accuracy: 17.71%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 1.7913 - Accuracy: 17.71%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 1.7913 - Accuracy: 17.66%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 1.7915 - Accuracy: 17.59%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 1.7915 - Accuracy: 17.57%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 1.7915 - Accuracy: 17.59%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 1.7915 - Accuracy: 17.61%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 1.7915 - Accuracy: 17.71%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 1.7915 - Accuracy: 17.72%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 1.7915 - Accuracy: 17.74%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 1.7915 - Accuracy: 17.70%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 1.7915 - Accuracy: 17.70%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 1.7914 - Accuracy: 17.79%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 1.7914 - Accuracy: 17.73%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 1.7915 - Accuracy: 17.73%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 1.7916 - Accuracy: 17.64%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 1.7916 - Accuracy: 17.62%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 1.7917 - Accuracy: 17.62%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 1.7917 - Accuracy: 17.59%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 1.7917 - Accuracy: 17.63%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 1.7917 - Accuracy: 17.60%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 1.7918 - Accuracy: 17.54%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 1.7917 - Accuracy: 17.58%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 1.7918 - Accuracy: 17.58%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 1.7918 - Accuracy: 17.53%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 1.7917 - Accuracy: 17.54%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 1.7917 - Accuracy: 17.55%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 1.7918 - Accuracy: 17.51%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 1.7917 - Accuracy: 17.49%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 1.7917 - Accuracy: 17.51%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 1.7917 - Accuracy: 17.47%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 1.7917 - Accuracy: 17.50%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 1.7916 - Accuracy: 17.52%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 1.7917 - Accuracy: 17.47%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 1.7916 - Accuracy: 17.52%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 1.7918 - Accuracy: 17.47%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 1.7917 - Accuracy: 17.47%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 1.7917 - Accuracy: 17.51%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 1.7917 - Accuracy: 17.54%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 1.7917 - Accuracy: 17.56%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 1.7917 - Accuracy: 17.55%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 1.7917 - Accuracy: 17.53%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 1.7917 - Accuracy: 17.51%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 1.7918 - Accuracy: 17.51%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 1.7918 - Accuracy: 17.47%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 1.7918 - Accuracy: 17.45%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 1.7918 - Accuracy: 17.45%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 1.7918 - Accuracy: 17.45%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 1.7919 - Accuracy: 17.45%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 1.7919 - Accuracy: 17.41%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 1.7919 - Accuracy: 17.40%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 1.7919 - Accuracy: 17.39%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 1.7919 - Accuracy: 17.36%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 1.7919 - Accuracy: 17.34%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 1.7919 - Accuracy: 17.36%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 1.7919 - Accuracy: 17.38%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 1.7919 - Accuracy: 17.37%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 1.7919 - Accuracy: 17.36%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 1.7919 - Accuracy: 17.31%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 1.7919 - Accuracy: 17.33%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 1.7919 - Accuracy: 17.34%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 1.7919 - Accuracy: 17.34%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 1.7919 - Accuracy: 17.31%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 1.7919 - Accuracy: 17.25%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 1.7919 - Accuracy: 17.20%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 1.7919 - Accuracy: 17.21%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 1.7919 - Accuracy: 17.19%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 1.7919 - Accuracy: 17.19%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 1.7919 - Accuracy: 17.19%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 1.7919 - Accuracy: 17.20%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 1.7920 - Accuracy: 17.17%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 1.7919 - Accuracy: 17.16%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 1.7919 - Accuracy: 17.15%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 1.7919 - Accuracy: 17.16%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 1.7920 - Accuracy: 17.15%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 1.7920 - Accuracy: 17.12%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 1.7919 - Accuracy: 17.12%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 1.7919 - Accuracy: 17.09%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 1.7920 - Accuracy: 17.06%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 1.7920 - Accuracy: 17.04%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 1.7920 - Accuracy: 17.00%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 1.7920 - Accuracy: 16.94%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 1.7920 - Accuracy: 16.99%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 1.7920 - Accuracy: 17.00%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 1.7920 - Accuracy: 16.96%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 1.7920 - Accuracy: 16.96%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 1.7920 - Accuracy: 16.99%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 1.7920 - Accuracy: 16.99%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 1.7920 - Accuracy: 17.02%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 1.7920 - Accuracy: 17.00%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 1.7920 - Accuracy: 17.03%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 1.7919 - Accuracy: 17.05%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 1.7920 - Accuracy: 17.04%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 1.7920 - Accuracy: 17.03%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 1.7920 - Accuracy: 17.03%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 1.7920 - Accuracy: 17.04%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 1.7920 - Accuracy: 17.06%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 1.7920 - Accuracy: 17.04%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 1.7920 - Accuracy: 17.02%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 1.7920 - Accuracy: 17.02%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 1.7920 - Accuracy: 17.02%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 1.7920 - Accuracy: 16.99%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 1.7920 - Accuracy: 16.96%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 1.7920 - Accuracy: 16.94%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 1.7920 - Accuracy: 16.85%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 1.7920 - Accuracy: 16.94%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 1.7920 - Accuracy: 16.94%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 1.7920 - Accuracy: 16.96%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 1.7920 - Accuracy: 16.98%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 1.7920 - Accuracy: 16.96%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 1.7920 - Accuracy: 16.97%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 1.7920 - Accuracy: 16.95%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 1.7920 - Accuracy: 16.82%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 1.7920 - Accuracy: 16.83%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 1.7920 - Accuracy: 16.83%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 1.7920 - Accuracy: 16.84%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 1.7920 - Accuracy: 16.88%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 1.7920 - Accuracy: 16.93%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 1.7920 - Accuracy: 16.92%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 1.7920 - Accuracy: 16.90%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 1.7920 - Accuracy: 16.91%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 1.7920 - Accuracy: 16.89%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 1.7920 - Accuracy: 16.87%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 1.7920 - Accuracy: 16.86%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 1.7920 - Accuracy: 16.83%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 1.7920 - Accuracy: 16.83%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 1.7920 - Accuracy: 16.82%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 1.7920 - Accuracy: 16.81%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 1.7920 - Accuracy: 16.80%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 1.7920 - Accuracy: 16.81%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 1.7920 - Accuracy: 16.80%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 1.7920 - Accuracy: 16.80%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 1.7920 - Accuracy: 16.71%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 1.7920 - Accuracy: 16.70%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 1.7920 - Accuracy: 16.70%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 1.7920 - Accuracy: 16.71%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 1.7920 - Accuracy: 16.71%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 1.7920 - Accuracy: 16.70%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 1.7920 - Accuracy: 16.69%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 1.7920 - Accuracy: 16.70%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 1.7920 - Accuracy: 16.69%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 1.7920 - Accuracy: 16.68%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 1.7920 - Accuracy: 16.67%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 1.7920 - Accuracy: 16.64%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 1.7920 - Accuracy: 16.65%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 1.7920 - Accuracy: 16.65%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 1.7920 - Accuracy: 16.64%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 1.7920 - Accuracy: 16.64%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 1.7920 - Accuracy: 16.64%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 1.7920 - Accuracy: 16.62%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 1.7920 - Accuracy: 16.64%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 1.7920 - Accuracy: 16.63%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 1.7920 - Accuracy: 16.65%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 1.7920 - Accuracy: 16.65%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 1.7920 - Accuracy: 16.66%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 1.7920 - Accuracy: 16.68%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 1.7920 - Accuracy: 16.67%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 1.7920 - Accuracy: 16.67%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 1.7920 - Accuracy: 16.70%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 1.7920 - Accuracy: 16.71%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 1.7920 - Accuracy: 16.75%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 1.7920 - Accuracy: 16.72%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 1.7920 - Accuracy: 16.73%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 1.7920 - Accuracy: 16.74%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 1.7920 - Accuracy: 16.80%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 1.7920 - Accuracy: 16.79%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 1.7920 - Accuracy: 16.76%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 1.7920 - Accuracy: 16.77%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 1.7920 - Accuracy: 16.78%\n",
      "Train loss: 1.7920 - Train accuracy: 16.79%\n",
      "Validation accuracy: 16.9999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16.9999%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 16.36%        | 17.00%              |\n",
    "| **Epoch 2** | 16.79%        | 17.00%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** iand **validation accuracy** remains nearly constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './LSTM_emotion_model/lstm_emotion_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
