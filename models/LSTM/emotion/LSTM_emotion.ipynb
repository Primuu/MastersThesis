{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: LSTM (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be52828e9b80fb42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c45861ca61805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09258150d349e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = encode_texts(train_data['text'])\n",
    "X_val = encode_texts(val_data['text'])\n",
    "X_test = encode_texts(test_data['text'])\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41875fcce7177fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.X[idx], 'label': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train)\n",
    "val_dataset = TokenizedTextDataset(X_val, y_val)\n",
    "test_dataset = TokenizedTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a5a3df2e3ce62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        pooled = torch.mean(lstm_out, dim=1)\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(self.relu(dropped))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16538adb7fbd6f38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=MAX_NUM_WORDS, embed_dim=256, hidden_dim=256, num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddeaca0092b0b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b5fe0991150431",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b1cc521016b62b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return 100. * correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.8370 - Accuracy: 12.50%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 1.8297 - Accuracy: 19.32%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 1.8351 - Accuracy: 19.35%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 1.8288 - Accuracy: 19.96%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.8209 - Accuracy: 19.82%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.8257 - Accuracy: 18.26%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.8218 - Accuracy: 18.55%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.8218 - Accuracy: 18.13%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.8201 - Accuracy: 18.06%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.8182 - Accuracy: 18.06%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.8159 - Accuracy: 18.07%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.8152 - Accuracy: 17.74%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.8126 - Accuracy: 18.03%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.8118 - Accuracy: 17.56%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.8114 - Accuracy: 17.46%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.8114 - Accuracy: 17.34%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.8111 - Accuracy: 17.27%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.8105 - Accuracy: 17.14%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.8092 - Accuracy: 17.44%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.8090 - Accuracy: 17.41%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.8087 - Accuracy: 17.26%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.8080 - Accuracy: 17.30%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.8075 - Accuracy: 17.34%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.8075 - Accuracy: 17.23%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.8067 - Accuracy: 17.38%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.8067 - Accuracy: 17.26%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.8065 - Accuracy: 17.29%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.8064 - Accuracy: 17.46%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.8062 - Accuracy: 17.44%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.8059 - Accuracy: 17.44%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 1.8050 - Accuracy: 17.55%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 1.8041 - Accuracy: 17.64%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 1.8041 - Accuracy: 17.60%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 1.8042 - Accuracy: 17.56%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 1.8036 - Accuracy: 17.58%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 1.8032 - Accuracy: 17.54%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 1.8028 - Accuracy: 17.62%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 1.8023 - Accuracy: 17.60%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 1.8020 - Accuracy: 17.62%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 1.8017 - Accuracy: 17.60%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 1.8015 - Accuracy: 17.63%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 1.8008 - Accuracy: 17.66%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 1.8005 - Accuracy: 17.73%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 1.8001 - Accuracy: 17.82%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 1.7996 - Accuracy: 17.94%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 1.7993 - Accuracy: 17.90%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 1.7988 - Accuracy: 18.02%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 1.7988 - Accuracy: 17.98%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 1.7988 - Accuracy: 17.97%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 1.7983 - Accuracy: 18.05%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 1.7977 - Accuracy: 18.13%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 1.7973 - Accuracy: 18.24%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 1.7967 - Accuracy: 18.37%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 1.7961 - Accuracy: 18.57%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 1.7958 - Accuracy: 18.54%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 1.7954 - Accuracy: 18.61%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 1.7948 - Accuracy: 18.72%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 1.7940 - Accuracy: 18.80%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 1.7932 - Accuracy: 18.90%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 1.7924 - Accuracy: 19.06%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 1.7925 - Accuracy: 19.16%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 1.7915 - Accuracy: 19.21%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 1.7886 - Accuracy: 19.33%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 1.7868 - Accuracy: 19.42%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 1.7849 - Accuracy: 19.60%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 1.7834 - Accuracy: 19.80%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 1.7832 - Accuracy: 19.86%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 1.7816 - Accuracy: 19.91%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 1.7795 - Accuracy: 20.12%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 1.7787 - Accuracy: 20.17%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 1.7775 - Accuracy: 20.26%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 1.7739 - Accuracy: 20.48%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 1.7715 - Accuracy: 20.66%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 1.7677 - Accuracy: 20.86%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 1.7641 - Accuracy: 21.00%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 1.7612 - Accuracy: 21.11%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 1.7591 - Accuracy: 21.32%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 1.7555 - Accuracy: 21.54%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 1.7541 - Accuracy: 21.67%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 1.7510 - Accuracy: 21.86%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 1.7485 - Accuracy: 22.00%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 1.7461 - Accuracy: 22.09%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 1.7442 - Accuracy: 22.13%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 1.7414 - Accuracy: 22.31%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 1.7386 - Accuracy: 22.46%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 1.7349 - Accuracy: 22.70%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 1.7313 - Accuracy: 22.89%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 1.7281 - Accuracy: 23.08%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 1.7256 - Accuracy: 23.27%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 1.7234 - Accuracy: 23.39%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 1.7204 - Accuracy: 23.58%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 1.7171 - Accuracy: 23.75%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 1.7140 - Accuracy: 23.93%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 1.7109 - Accuracy: 24.09%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 1.7077 - Accuracy: 24.25%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 1.7046 - Accuracy: 24.39%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 1.7008 - Accuracy: 24.55%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 1.6966 - Accuracy: 24.72%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 1.6944 - Accuracy: 24.83%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 1.6917 - Accuracy: 24.97%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 1.6875 - Accuracy: 25.17%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 1.6840 - Accuracy: 25.34%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 1.6812 - Accuracy: 25.47%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 1.6781 - Accuracy: 25.69%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 1.6738 - Accuracy: 25.90%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 1.6704 - Accuracy: 26.10%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 1.6672 - Accuracy: 26.28%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 1.6639 - Accuracy: 26.41%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 1.6603 - Accuracy: 26.59%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 1.6574 - Accuracy: 26.75%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 1.6531 - Accuracy: 26.96%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 1.6495 - Accuracy: 27.15%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 1.6462 - Accuracy: 27.36%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 1.6429 - Accuracy: 27.54%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 1.6395 - Accuracy: 27.63%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 1.6361 - Accuracy: 27.79%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 1.6321 - Accuracy: 27.98%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 1.6281 - Accuracy: 28.17%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 1.6249 - Accuracy: 28.36%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 1.6217 - Accuracy: 28.48%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 1.6175 - Accuracy: 28.62%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 1.6140 - Accuracy: 28.76%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 1.6112 - Accuracy: 28.87%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 1.6084 - Accuracy: 29.04%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 1.6056 - Accuracy: 29.16%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 1.6022 - Accuracy: 29.34%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 1.5989 - Accuracy: 29.50%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 1.5954 - Accuracy: 29.65%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 1.5910 - Accuracy: 29.85%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 1.5864 - Accuracy: 30.05%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 1.5831 - Accuracy: 30.19%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 1.5784 - Accuracy: 30.39%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 1.5750 - Accuracy: 30.55%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 1.5717 - Accuracy: 30.74%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 1.5673 - Accuracy: 30.96%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 1.5633 - Accuracy: 31.13%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 1.5603 - Accuracy: 31.30%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 1.5561 - Accuracy: 31.50%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 1.5527 - Accuracy: 31.66%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 1.5483 - Accuracy: 31.84%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 1.5453 - Accuracy: 32.01%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 1.5414 - Accuracy: 32.19%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 1.5375 - Accuracy: 32.36%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 1.5341 - Accuracy: 32.53%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 1.5303 - Accuracy: 32.72%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 1.5266 - Accuracy: 32.92%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 1.5227 - Accuracy: 33.12%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 1.5193 - Accuracy: 33.26%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 1.5158 - Accuracy: 33.47%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 1.5115 - Accuracy: 33.65%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 1.5073 - Accuracy: 33.84%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 1.5029 - Accuracy: 34.03%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 1.4986 - Accuracy: 34.28%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 1.4948 - Accuracy: 34.48%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 1.4910 - Accuracy: 34.65%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 1.4869 - Accuracy: 34.88%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 1.4829 - Accuracy: 35.08%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 1.4793 - Accuracy: 35.26%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 1.4756 - Accuracy: 35.43%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 1.4720 - Accuracy: 35.63%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 1.4685 - Accuracy: 35.79%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 1.4651 - Accuracy: 35.94%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 1.4614 - Accuracy: 36.12%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 1.4576 - Accuracy: 36.33%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 1.4541 - Accuracy: 36.49%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 1.4499 - Accuracy: 36.68%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 1.4462 - Accuracy: 36.87%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 1.4420 - Accuracy: 37.06%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 1.4384 - Accuracy: 37.24%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 1.4354 - Accuracy: 37.39%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 1.4320 - Accuracy: 37.57%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 1.4286 - Accuracy: 37.77%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 1.4250 - Accuracy: 37.95%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 1.4201 - Accuracy: 38.20%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 1.4158 - Accuracy: 38.42%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 1.4118 - Accuracy: 38.62%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 1.4084 - Accuracy: 38.81%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 1.4051 - Accuracy: 39.00%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 1.4019 - Accuracy: 39.18%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 1.3986 - Accuracy: 39.36%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 1.3957 - Accuracy: 39.54%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 1.3917 - Accuracy: 39.74%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 1.3887 - Accuracy: 39.92%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 1.3850 - Accuracy: 40.11%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 1.3811 - Accuracy: 40.31%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 1.3769 - Accuracy: 40.53%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 1.3729 - Accuracy: 40.74%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 1.3691 - Accuracy: 40.92%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 1.3653 - Accuracy: 41.13%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 1.3618 - Accuracy: 41.29%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 1.3578 - Accuracy: 41.49%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 1.3544 - Accuracy: 41.66%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 1.3505 - Accuracy: 41.86%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 1.3464 - Accuracy: 42.07%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 1.3432 - Accuracy: 42.25%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 1.3400 - Accuracy: 42.41%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 1.3363 - Accuracy: 42.59%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 1.3324 - Accuracy: 42.79%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 1.3291 - Accuracy: 42.98%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 1.3250 - Accuracy: 43.18%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 1.3213 - Accuracy: 43.34%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 1.3174 - Accuracy: 43.54%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 1.3134 - Accuracy: 43.74%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 1.3099 - Accuracy: 43.91%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 1.3066 - Accuracy: 44.10%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 1.3035 - Accuracy: 44.25%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 1.2998 - Accuracy: 44.44%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 1.2962 - Accuracy: 44.61%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 1.2924 - Accuracy: 44.81%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 1.2896 - Accuracy: 44.96%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 1.2863 - Accuracy: 45.15%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 1.2833 - Accuracy: 45.33%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 1.2802 - Accuracy: 45.50%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 1.2771 - Accuracy: 45.67%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 1.2737 - Accuracy: 45.85%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 1.2705 - Accuracy: 46.02%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 1.2671 - Accuracy: 46.19%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 1.2639 - Accuracy: 46.36%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 1.2600 - Accuracy: 46.54%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 1.2572 - Accuracy: 46.69%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 1.2542 - Accuracy: 46.84%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 1.2509 - Accuracy: 47.00%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 1.2478 - Accuracy: 47.16%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 1.2444 - Accuracy: 47.33%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 1.2415 - Accuracy: 47.47%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 1.2378 - Accuracy: 47.64%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 1.2352 - Accuracy: 47.81%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 1.2324 - Accuracy: 47.96%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 1.2288 - Accuracy: 48.15%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 1.2261 - Accuracy: 48.29%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 1.2228 - Accuracy: 48.47%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 1.2199 - Accuracy: 48.62%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 1.2168 - Accuracy: 48.79%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 1.2136 - Accuracy: 48.94%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 1.2105 - Accuracy: 49.08%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 1.2073 - Accuracy: 49.24%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 1.2046 - Accuracy: 49.38%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 1.2015 - Accuracy: 49.54%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 1.1983 - Accuracy: 49.70%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 1.1957 - Accuracy: 49.85%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 1.1926 - Accuracy: 50.01%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 1.1895 - Accuracy: 50.16%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 1.1865 - Accuracy: 50.30%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 1.1840 - Accuracy: 50.44%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 1.1809 - Accuracy: 50.60%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 1.1777 - Accuracy: 50.75%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 1.1748 - Accuracy: 50.90%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 1.1720 - Accuracy: 51.04%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 1.1689 - Accuracy: 51.19%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 1.1661 - Accuracy: 51.33%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 1.1625 - Accuracy: 51.49%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 1.1595 - Accuracy: 51.63%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 1.1566 - Accuracy: 51.78%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 1.1536 - Accuracy: 51.93%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 1.1508 - Accuracy: 52.05%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 1.1483 - Accuracy: 52.17%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 1.1454 - Accuracy: 52.30%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 1.1431 - Accuracy: 52.42%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 1.1400 - Accuracy: 52.57%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 1.1372 - Accuracy: 52.71%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 1.1343 - Accuracy: 52.85%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 1.1319 - Accuracy: 52.97%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 1.1295 - Accuracy: 53.10%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 1.1269 - Accuracy: 53.23%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 1.1244 - Accuracy: 53.36%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 1.1220 - Accuracy: 53.48%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 1.1200 - Accuracy: 53.59%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 1.1173 - Accuracy: 53.72%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 1.1142 - Accuracy: 53.87%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 1.1113 - Accuracy: 54.01%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 1.1087 - Accuracy: 54.12%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 1.1064 - Accuracy: 54.24%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 1.1040 - Accuracy: 54.37%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 1.1016 - Accuracy: 54.49%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 1.0993 - Accuracy: 54.59%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 1.0969 - Accuracy: 54.72%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 1.0941 - Accuracy: 54.84%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 1.0915 - Accuracy: 54.97%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 1.0892 - Accuracy: 55.10%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 1.0864 - Accuracy: 55.23%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 1.0839 - Accuracy: 55.36%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 1.0821 - Accuracy: 55.45%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 1.0797 - Accuracy: 55.57%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 1.0770 - Accuracy: 55.69%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 1.0751 - Accuracy: 55.79%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 1.0726 - Accuracy: 55.90%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 1.0700 - Accuracy: 56.02%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 1.0676 - Accuracy: 56.13%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 1.0654 - Accuracy: 56.23%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 1.0631 - Accuracy: 56.34%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 1.0607 - Accuracy: 56.46%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 1.0584 - Accuracy: 56.57%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 1.0561 - Accuracy: 56.67%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 1.0536 - Accuracy: 56.79%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 1.0513 - Accuracy: 56.90%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 1.0495 - Accuracy: 56.99%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 1.0472 - Accuracy: 57.10%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 1.0448 - Accuracy: 57.20%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 1.0424 - Accuracy: 57.30%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 1.0402 - Accuracy: 57.42%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 1.0377 - Accuracy: 57.53%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 1.0354 - Accuracy: 57.64%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 1.0331 - Accuracy: 57.75%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 1.0310 - Accuracy: 57.85%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 1.0290 - Accuracy: 57.94%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 1.0272 - Accuracy: 58.03%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 1.0253 - Accuracy: 58.13%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 1.0240 - Accuracy: 58.20%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 1.0226 - Accuracy: 58.27%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 1.0204 - Accuracy: 58.37%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 1.0184 - Accuracy: 58.47%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 1.0162 - Accuracy: 58.56%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 1.0144 - Accuracy: 58.64%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 1.0126 - Accuracy: 58.72%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 1.0104 - Accuracy: 58.82%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 1.0083 - Accuracy: 58.92%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 1.0065 - Accuracy: 59.01%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 1.0045 - Accuracy: 59.10%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 1.0027 - Accuracy: 59.18%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 1.0009 - Accuracy: 59.27%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 0.9989 - Accuracy: 59.36%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 0.9969 - Accuracy: 59.46%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 0.9945 - Accuracy: 59.57%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 0.9925 - Accuracy: 59.65%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 0.9910 - Accuracy: 59.74%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 0.9891 - Accuracy: 59.83%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 0.9873 - Accuracy: 59.92%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 0.9854 - Accuracy: 60.01%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 0.9836 - Accuracy: 60.10%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 0.9821 - Accuracy: 60.18%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 0.9801 - Accuracy: 60.27%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 0.9780 - Accuracy: 60.35%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 0.9763 - Accuracy: 60.43%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 0.9740 - Accuracy: 60.53%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 0.9721 - Accuracy: 60.62%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 0.9703 - Accuracy: 60.71%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 0.9684 - Accuracy: 60.79%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 0.9664 - Accuracy: 60.88%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 0.9645 - Accuracy: 60.96%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 0.9625 - Accuracy: 61.05%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 0.9605 - Accuracy: 61.13%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 0.9586 - Accuracy: 61.20%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 0.9564 - Accuracy: 61.30%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 0.9545 - Accuracy: 61.39%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 0.9525 - Accuracy: 61.48%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 0.9509 - Accuracy: 61.56%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 0.9490 - Accuracy: 61.65%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 0.9473 - Accuracy: 61.73%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 0.9456 - Accuracy: 61.81%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 0.9439 - Accuracy: 61.88%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 0.9421 - Accuracy: 61.96%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 0.9404 - Accuracy: 62.04%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 0.9387 - Accuracy: 62.12%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 0.9369 - Accuracy: 62.19%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 0.9348 - Accuracy: 62.28%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 0.9333 - Accuracy: 62.36%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 0.9318 - Accuracy: 62.44%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 0.9300 - Accuracy: 62.52%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 0.9283 - Accuracy: 62.59%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 0.9267 - Accuracy: 62.66%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 0.9250 - Accuracy: 62.74%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 0.9234 - Accuracy: 62.81%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 0.9215 - Accuracy: 62.89%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 0.9198 - Accuracy: 62.97%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 0.9183 - Accuracy: 63.03%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 0.9171 - Accuracy: 63.10%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 0.9153 - Accuracy: 63.17%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 0.9137 - Accuracy: 63.26%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 0.9121 - Accuracy: 63.33%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 0.9106 - Accuracy: 63.40%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 0.9087 - Accuracy: 63.48%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 0.9071 - Accuracy: 63.56%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 0.9054 - Accuracy: 63.63%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 0.9037 - Accuracy: 63.70%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 0.9023 - Accuracy: 63.77%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 0.9006 - Accuracy: 63.84%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 0.8993 - Accuracy: 63.91%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 0.8978 - Accuracy: 63.98%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 0.8961 - Accuracy: 64.06%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 0.8943 - Accuracy: 64.14%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 0.8928 - Accuracy: 64.21%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 0.8913 - Accuracy: 64.27%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 0.8898 - Accuracy: 64.34%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 0.8882 - Accuracy: 64.41%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 0.8865 - Accuracy: 64.49%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 0.8849 - Accuracy: 64.56%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 0.8832 - Accuracy: 64.64%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 0.8817 - Accuracy: 64.70%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 0.8801 - Accuracy: 64.78%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 0.8787 - Accuracy: 64.83%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 0.8769 - Accuracy: 64.91%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 0.8755 - Accuracy: 64.97%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 0.8741 - Accuracy: 65.03%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 0.8725 - Accuracy: 65.09%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 0.8712 - Accuracy: 65.16%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 0.8698 - Accuracy: 65.22%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 0.8683 - Accuracy: 65.29%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 0.8671 - Accuracy: 65.35%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 0.8656 - Accuracy: 65.41%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 0.8641 - Accuracy: 65.48%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 0.8626 - Accuracy: 65.54%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 0.8614 - Accuracy: 65.59%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 0.8598 - Accuracy: 65.66%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 0.8583 - Accuracy: 65.72%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 0.8568 - Accuracy: 65.79%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 0.8554 - Accuracy: 65.85%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 0.8543 - Accuracy: 65.90%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 0.8532 - Accuracy: 65.96%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 0.8517 - Accuracy: 66.03%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 0.8503 - Accuracy: 66.10%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 0.8489 - Accuracy: 66.15%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 0.8475 - Accuracy: 66.21%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 0.8460 - Accuracy: 66.27%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 0.8446 - Accuracy: 66.33%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 0.8431 - Accuracy: 66.40%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 0.8418 - Accuracy: 66.45%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 0.8403 - Accuracy: 66.51%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 0.8388 - Accuracy: 66.58%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 0.8374 - Accuracy: 66.64%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 0.8360 - Accuracy: 66.70%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 0.8347 - Accuracy: 66.75%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 0.8331 - Accuracy: 66.81%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 0.8322 - Accuracy: 66.87%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 0.8308 - Accuracy: 66.93%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 0.8296 - Accuracy: 66.98%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 0.8282 - Accuracy: 67.04%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 0.8269 - Accuracy: 67.10%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 0.8254 - Accuracy: 67.16%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 0.8240 - Accuracy: 67.22%\n",
      "Train loss: 0.8229 - Train accuracy: 67.27%\n",
      "Validation accuracy: 92.0645\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 1.1626 - Accuracy: 81.25%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 0.2845 - Accuracy: 93.75%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 0.2814 - Accuracy: 93.15%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 0.2719 - Accuracy: 91.73%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 0.2615 - Accuracy: 91.77%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 0.2409 - Accuracy: 92.28%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 0.2411 - Accuracy: 92.52%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 0.2466 - Accuracy: 92.52%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 0.2457 - Accuracy: 92.28%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 0.2426 - Accuracy: 92.17%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 0.2466 - Accuracy: 91.77%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 0.2494 - Accuracy: 91.78%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 0.2451 - Accuracy: 91.79%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 0.2413 - Accuracy: 91.75%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 0.2391 - Accuracy: 91.80%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 0.2379 - Accuracy: 91.85%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 0.2471 - Accuracy: 91.58%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 0.2461 - Accuracy: 91.70%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 0.2480 - Accuracy: 91.64%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 0.2491 - Accuracy: 91.72%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 0.2475 - Accuracy: 91.88%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 0.2484 - Accuracy: 91.80%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 0.2460 - Accuracy: 91.88%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 0.2443 - Accuracy: 91.94%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 0.2476 - Accuracy: 91.80%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 0.2447 - Accuracy: 91.96%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 0.2410 - Accuracy: 92.05%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 0.2409 - Accuracy: 92.00%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 0.2417 - Accuracy: 92.04%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 0.2389 - Accuracy: 92.12%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 0.2365 - Accuracy: 92.25%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 0.2383 - Accuracy: 92.20%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 0.2429 - Accuracy: 92.10%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 0.2397 - Accuracy: 92.20%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 0.2393 - Accuracy: 92.19%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 0.2389 - Accuracy: 92.25%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 0.2351 - Accuracy: 92.40%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 0.2369 - Accuracy: 92.33%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 0.2362 - Accuracy: 92.34%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 0.2346 - Accuracy: 92.38%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 0.2320 - Accuracy: 92.44%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 0.2311 - Accuracy: 92.46%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 0.2305 - Accuracy: 92.47%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 0.2288 - Accuracy: 92.53%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 0.2270 - Accuracy: 92.57%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 0.2253 - Accuracy: 92.60%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 0.2246 - Accuracy: 92.62%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 0.2238 - Accuracy: 92.65%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 0.2237 - Accuracy: 92.68%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 0.2224 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 0.2210 - Accuracy: 92.76%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 0.2209 - Accuracy: 92.77%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 0.2213 - Accuracy: 92.75%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 0.2210 - Accuracy: 92.76%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 0.2201 - Accuracy: 92.81%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 0.2198 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 0.2184 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 0.2178 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 0.2171 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 0.2167 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 0.2171 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 0.2175 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 0.2162 - Accuracy: 93.00%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 0.2167 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 0.2164 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 0.2182 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 0.2185 - Accuracy: 92.85%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 0.2186 - Accuracy: 92.86%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 0.2182 - Accuracy: 92.86%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 0.2183 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 0.2175 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 0.2166 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 0.2176 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 0.2177 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 0.2176 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 0.2174 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 0.2183 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 0.2178 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 0.2165 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 0.2161 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 0.2151 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 0.2152 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 0.2144 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 0.2144 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 0.2161 - Accuracy: 92.81%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 0.2161 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 0.2156 - Accuracy: 92.84%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 0.2150 - Accuracy: 92.85%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 0.2143 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 0.2140 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 0.2138 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 0.2146 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 0.2140 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 0.2136 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 0.2144 - Accuracy: 92.86%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 0.2154 - Accuracy: 92.81%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 0.2154 - Accuracy: 92.80%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 0.2155 - Accuracy: 92.78%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 0.2156 - Accuracy: 92.78%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 0.2146 - Accuracy: 92.83%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 0.2147 - Accuracy: 92.84%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 0.2151 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 0.2148 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 0.2146 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 0.2155 - Accuracy: 92.77%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 0.2155 - Accuracy: 92.76%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 0.2158 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 0.2157 - Accuracy: 92.75%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 0.2153 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 0.2157 - Accuracy: 92.71%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 0.2149 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 0.2149 - Accuracy: 92.73%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 0.2141 - Accuracy: 92.76%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 0.2146 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 0.2146 - Accuracy: 92.75%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 0.2150 - Accuracy: 92.74%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 0.2155 - Accuracy: 92.73%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 0.2155 - Accuracy: 92.73%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 0.2149 - Accuracy: 92.76%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 0.2149 - Accuracy: 92.77%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 0.2147 - Accuracy: 92.79%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 0.2146 - Accuracy: 92.81%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 0.2141 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 0.2142 - Accuracy: 92.81%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 0.2139 - Accuracy: 92.82%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 0.2135 - Accuracy: 92.84%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 0.2129 - Accuracy: 92.86%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 0.2124 - Accuracy: 92.87%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 0.2121 - Accuracy: 92.88%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 0.2119 - Accuracy: 92.88%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 0.2114 - Accuracy: 92.89%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 0.2103 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 0.2112 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 0.2107 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 0.2105 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 0.2112 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 0.2109 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 0.2111 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 0.2108 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 0.2106 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 0.2105 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 0.2101 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 0.2098 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 0.2103 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 0.2098 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 0.2096 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 0.2097 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 0.2095 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 0.2094 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 0.2096 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 0.2096 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 0.2096 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 0.2103 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 0.2099 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 0.2098 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 0.2097 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 0.2090 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 0.2095 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 0.2094 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 0.2091 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 0.2086 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 0.2084 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 0.2080 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 0.2080 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 0.2076 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 0.2076 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 0.2077 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 0.2071 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 0.2072 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 0.2075 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 0.2079 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 0.2082 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 0.2079 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 0.2079 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 0.2082 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 0.2086 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 0.2085 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 0.2088 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 0.2088 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 0.2085 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 0.2082 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 0.2084 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 0.2085 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 0.2090 - Accuracy: 92.92%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 0.2092 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 0.2093 - Accuracy: 92.90%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 0.2088 - Accuracy: 92.91%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 0.2083 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 0.2078 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 0.2077 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 0.2082 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 0.2083 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 0.2080 - Accuracy: 92.93%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 0.2076 - Accuracy: 92.94%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 0.2076 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 0.2075 - Accuracy: 92.95%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 0.2073 - Accuracy: 92.96%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 0.2067 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 0.2068 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 0.2071 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 0.2070 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 0.2068 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 0.2067 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 0.2066 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 0.2062 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 0.2062 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 0.2064 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 0.2062 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 0.2062 - Accuracy: 92.97%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 0.2060 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 0.2057 - Accuracy: 92.99%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 0.2058 - Accuracy: 92.98%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 0.2053 - Accuracy: 93.00%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 0.2052 - Accuracy: 93.01%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 0.2050 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 0.2054 - Accuracy: 93.01%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 0.2052 - Accuracy: 93.00%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 0.2049 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 0.2048 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 0.2048 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 0.2048 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 0.2047 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 0.2048 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 0.2048 - Accuracy: 93.01%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 0.2045 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 0.2045 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 0.2046 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 0.2045 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 0.2042 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 0.2041 - Accuracy: 93.02%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 0.2039 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 0.2036 - Accuracy: 93.04%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 0.2035 - Accuracy: 93.04%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 0.2040 - Accuracy: 93.03%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 0.2036 - Accuracy: 93.05%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 0.2035 - Accuracy: 93.05%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 0.2034 - Accuracy: 93.05%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 0.2032 - Accuracy: 93.04%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 0.2030 - Accuracy: 93.04%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 0.2026 - Accuracy: 93.05%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 0.2024 - Accuracy: 93.06%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 0.2023 - Accuracy: 93.06%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 0.2019 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 0.2016 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 0.2016 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 0.2014 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 0.2015 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 0.2017 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 0.2016 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 0.2021 - Accuracy: 93.06%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 0.2017 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 0.2016 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 0.2016 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 0.2015 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 0.2016 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 0.2019 - Accuracy: 93.06%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 0.2016 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 0.2017 - Accuracy: 93.06%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 0.2014 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 0.2013 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 0.2011 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 0.2010 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 0.2010 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 0.2008 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 0.2007 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 0.2006 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 0.2008 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 0.2005 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 0.2002 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 0.2003 - Accuracy: 93.07%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 0.2002 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 0.2001 - Accuracy: 93.08%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 0.2000 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 0.1999 - Accuracy: 93.09%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 0.1996 - Accuracy: 93.10%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 0.1994 - Accuracy: 93.10%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 0.1995 - Accuracy: 93.10%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 0.1991 - Accuracy: 93.11%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 0.1989 - Accuracy: 93.11%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 0.1988 - Accuracy: 93.12%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 0.1985 - Accuracy: 93.13%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 0.1986 - Accuracy: 93.12%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 0.1985 - Accuracy: 93.13%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 0.1981 - Accuracy: 93.14%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 0.1980 - Accuracy: 93.14%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 0.1978 - Accuracy: 93.14%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 0.1980 - Accuracy: 93.13%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 0.1980 - Accuracy: 93.13%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 0.1978 - Accuracy: 93.14%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 0.1976 - Accuracy: 93.14%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 0.1973 - Accuracy: 93.15%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 0.1970 - Accuracy: 93.16%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 0.1969 - Accuracy: 93.16%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 0.1967 - Accuracy: 93.16%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 0.1965 - Accuracy: 93.16%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 0.1963 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 0.1963 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 0.1961 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 0.1959 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 0.1959 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 0.1957 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 0.1956 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 0.1953 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 0.1952 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 0.1950 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 0.1950 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 0.1951 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 0.1951 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 0.1951 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 0.1950 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 0.1950 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 0.1948 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 0.1948 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 0.1950 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 0.1949 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 0.1949 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 0.1949 - Accuracy: 93.17%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 0.1948 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 0.1947 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 0.1945 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 0.1947 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 0.1946 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 0.1943 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 0.1943 - Accuracy: 93.18%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 0.1941 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 0.1939 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 0.1940 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 0.1938 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 0.1937 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 0.1938 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 0.1936 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 0.1934 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 0.1933 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 0.1931 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 0.1931 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 0.1930 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 0.1929 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 0.1930 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 0.1928 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 0.1927 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 0.1925 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 0.1924 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 0.1923 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 0.1923 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 0.1922 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 0.1920 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 0.1919 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 0.1918 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 0.1917 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 0.1918 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 0.1918 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 0.1918 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 0.1917 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 0.1916 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 0.1915 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 0.1913 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 0.1912 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 0.1911 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 0.1911 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 0.1912 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 0.1914 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 0.1915 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 0.1914 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 0.1913 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 0.1913 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 0.1912 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 0.1911 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 0.1910 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 0.1910 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 0.1910 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 0.1910 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 0.1908 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 0.1908 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 0.1906 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 0.1909 - Accuracy: 93.19%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 0.1907 - Accuracy: 93.20%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 0.1905 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 0.1903 - Accuracy: 93.21%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 0.1900 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 0.1900 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 0.1901 - Accuracy: 93.22%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 0.1900 - Accuracy: 93.23%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 0.1897 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 0.1898 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 0.1898 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 0.1900 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 0.1900 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 0.1899 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 0.1898 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 0.1898 - Accuracy: 93.24%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 0.1896 - Accuracy: 93.25%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 0.1894 - Accuracy: 93.25%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 0.1893 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 0.1893 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 0.1894 - Accuracy: 93.25%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 0.1894 - Accuracy: 93.25%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 0.1892 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 0.1890 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 0.1889 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 0.1888 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 0.1887 - Accuracy: 93.26%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 0.1885 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 0.1884 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 0.1884 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 0.1884 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 0.1882 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 0.1881 - Accuracy: 93.27%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 0.1879 - Accuracy: 93.28%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 0.1878 - Accuracy: 93.28%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 0.1878 - Accuracy: 93.28%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 0.1876 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 0.1875 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 0.1873 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 0.1875 - Accuracy: 93.28%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 0.1874 - Accuracy: 93.28%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 0.1872 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 0.1872 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 0.1872 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 0.1870 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 0.1871 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 0.1871 - Accuracy: 93.29%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 0.1870 - Accuracy: 93.30%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 0.1867 - Accuracy: 93.30%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 0.1867 - Accuracy: 93.31%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 0.1865 - Accuracy: 93.32%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 0.1864 - Accuracy: 93.32%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 0.1863 - Accuracy: 93.33%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 0.1861 - Accuracy: 93.33%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 0.1859 - Accuracy: 93.34%\n",
      "Train loss: 0.1858 - Train accuracy: 93.34%\n",
      "Validation accuracy: 94.7640\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.6620%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 67.27%        | 92.06%              |\n",
    "| **Epoch 2** | 93.34%        | 94.76%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** and **validation accuracy** constantly icreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './LSTM_emotion_model/lstm_emotion_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
