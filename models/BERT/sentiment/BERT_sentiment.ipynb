{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: BERT (sentiment)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "\n",
    "train_file = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b87c99bfacf903",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=pd.unique(train_data['label']), y=train_data['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d8e07052fdcbf5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.texts = data['text']\n",
    "        self.labels = data['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed48f2cf48216d0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_loader(data, tokenizer, max_len, batch_size):\n",
    "    dataset = LabeledDataset(data, tokenizer, max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45240e9d76e8773",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = create_data_loader(train_data, tokenizer, max_len=256, batch_size=16)\n",
    "val_loader = create_data_loader(val_data, tokenizer, max_len=256, batch_size=16)\n",
    "test_loader = create_data_loader(test_data, tokenizer, max_len=256, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54d574d38451a7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f74f6ffa591dd5a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olga\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9719f2c2fa628513",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = outputs.logits.max(1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd71793d5dbf352b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    return correct_predictions.double() / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4317 - Avg Loss: 1.0575 - Accuracy: 50.00%\n",
      "Epoch: 0. Batch 10/4317 - Avg Loss: 1.0665 - Accuracy: 36.36%\n",
      "Epoch: 0. Batch 20/4317 - Avg Loss: 1.0556 - Accuracy: 39.58%\n",
      "Epoch: 0. Batch 30/4317 - Avg Loss: 1.0669 - Accuracy: 39.52%\n",
      "Epoch: 0. Batch 40/4317 - Avg Loss: 1.0491 - Accuracy: 42.23%\n",
      "Epoch: 0. Batch 50/4317 - Avg Loss: 1.0397 - Accuracy: 42.52%\n",
      "Epoch: 0. Batch 60/4317 - Avg Loss: 1.0302 - Accuracy: 43.85%\n",
      "Epoch: 0. Batch 70/4317 - Avg Loss: 1.0181 - Accuracy: 45.69%\n",
      "Epoch: 0. Batch 80/4317 - Avg Loss: 1.0087 - Accuracy: 46.60%\n",
      "Epoch: 0. Batch 90/4317 - Avg Loss: 0.9880 - Accuracy: 48.08%\n",
      "Epoch: 0. Batch 100/4317 - Avg Loss: 0.9703 - Accuracy: 49.81%\n",
      "Epoch: 0. Batch 110/4317 - Avg Loss: 0.9591 - Accuracy: 50.51%\n",
      "Epoch: 0. Batch 120/4317 - Avg Loss: 0.9480 - Accuracy: 51.19%\n",
      "Epoch: 0. Batch 130/4317 - Avg Loss: 0.9407 - Accuracy: 52.15%\n",
      "Epoch: 0. Batch 140/4317 - Avg Loss: 0.9323 - Accuracy: 53.01%\n",
      "Epoch: 0. Batch 150/4317 - Avg Loss: 0.9277 - Accuracy: 53.31%\n",
      "Epoch: 0. Batch 160/4317 - Avg Loss: 0.9212 - Accuracy: 53.92%\n",
      "Epoch: 0. Batch 170/4317 - Avg Loss: 0.9101 - Accuracy: 54.71%\n",
      "Epoch: 0. Batch 180/4317 - Avg Loss: 0.9011 - Accuracy: 55.46%\n",
      "Epoch: 0. Batch 190/4317 - Avg Loss: 0.8919 - Accuracy: 55.86%\n",
      "Epoch: 0. Batch 200/4317 - Avg Loss: 0.8910 - Accuracy: 56.06%\n",
      "Epoch: 0. Batch 210/4317 - Avg Loss: 0.8877 - Accuracy: 56.40%\n",
      "Epoch: 0. Batch 220/4317 - Avg Loss: 0.8795 - Accuracy: 56.99%\n",
      "Epoch: 0. Batch 230/4317 - Avg Loss: 0.8739 - Accuracy: 57.49%\n",
      "Epoch: 0. Batch 240/4317 - Avg Loss: 0.8636 - Accuracy: 58.35%\n",
      "Epoch: 0. Batch 250/4317 - Avg Loss: 0.8568 - Accuracy: 58.72%\n",
      "Epoch: 0. Batch 260/4317 - Avg Loss: 0.8502 - Accuracy: 59.22%\n",
      "Epoch: 0. Batch 270/4317 - Avg Loss: 0.8457 - Accuracy: 59.66%\n",
      "Epoch: 0. Batch 280/4317 - Avg Loss: 0.8418 - Accuracy: 59.96%\n",
      "Epoch: 0. Batch 290/4317 - Avg Loss: 0.8400 - Accuracy: 60.16%\n",
      "Epoch: 0. Batch 300/4317 - Avg Loss: 0.8359 - Accuracy: 60.42%\n",
      "Epoch: 0. Batch 310/4317 - Avg Loss: 0.8294 - Accuracy: 60.87%\n",
      "Epoch: 0. Batch 320/4317 - Avg Loss: 0.8256 - Accuracy: 61.18%\n",
      "Epoch: 0. Batch 330/4317 - Avg Loss: 0.8226 - Accuracy: 61.42%\n",
      "Epoch: 0. Batch 340/4317 - Avg Loss: 0.8203 - Accuracy: 61.42%\n",
      "Epoch: 0. Batch 350/4317 - Avg Loss: 0.8174 - Accuracy: 61.59%\n",
      "Epoch: 0. Batch 360/4317 - Avg Loss: 0.8142 - Accuracy: 61.81%\n",
      "Epoch: 0. Batch 370/4317 - Avg Loss: 0.8125 - Accuracy: 61.93%\n",
      "Epoch: 0. Batch 380/4317 - Avg Loss: 0.8090 - Accuracy: 62.17%\n",
      "Epoch: 0. Batch 390/4317 - Avg Loss: 0.8069 - Accuracy: 62.31%\n",
      "Epoch: 0. Batch 400/4317 - Avg Loss: 0.8048 - Accuracy: 62.45%\n",
      "Epoch: 0. Batch 410/4317 - Avg Loss: 0.8031 - Accuracy: 62.59%\n",
      "Epoch: 0. Batch 420/4317 - Avg Loss: 0.8010 - Accuracy: 62.66%\n",
      "Epoch: 0. Batch 430/4317 - Avg Loss: 0.7988 - Accuracy: 62.76%\n",
      "Epoch: 0. Batch 440/4317 - Avg Loss: 0.7950 - Accuracy: 62.93%\n",
      "Epoch: 0. Batch 450/4317 - Avg Loss: 0.7937 - Accuracy: 63.05%\n",
      "Epoch: 0. Batch 460/4317 - Avg Loss: 0.7920 - Accuracy: 63.10%\n",
      "Epoch: 0. Batch 470/4317 - Avg Loss: 0.7933 - Accuracy: 62.90%\n",
      "Epoch: 0. Batch 480/4317 - Avg Loss: 0.7916 - Accuracy: 63.03%\n",
      "Epoch: 0. Batch 490/4317 - Avg Loss: 0.7896 - Accuracy: 63.17%\n",
      "Epoch: 0. Batch 500/4317 - Avg Loss: 0.7878 - Accuracy: 63.26%\n",
      "Epoch: 0. Batch 510/4317 - Avg Loss: 0.7853 - Accuracy: 63.36%\n",
      "Epoch: 0. Batch 520/4317 - Avg Loss: 0.7858 - Accuracy: 63.39%\n",
      "Epoch: 0. Batch 530/4317 - Avg Loss: 0.7843 - Accuracy: 63.49%\n",
      "Epoch: 0. Batch 540/4317 - Avg Loss: 0.7834 - Accuracy: 63.60%\n",
      "Epoch: 0. Batch 550/4317 - Avg Loss: 0.7818 - Accuracy: 63.71%\n",
      "Epoch: 0. Batch 560/4317 - Avg Loss: 0.7791 - Accuracy: 63.87%\n",
      "Epoch: 0. Batch 570/4317 - Avg Loss: 0.7768 - Accuracy: 63.96%\n",
      "Epoch: 0. Batch 580/4317 - Avg Loss: 0.7759 - Accuracy: 64.02%\n",
      "Epoch: 0. Batch 590/4317 - Avg Loss: 0.7734 - Accuracy: 64.05%\n",
      "Epoch: 0. Batch 600/4317 - Avg Loss: 0.7711 - Accuracy: 64.24%\n",
      "Epoch: 0. Batch 610/4317 - Avg Loss: 0.7703 - Accuracy: 64.37%\n",
      "Epoch: 0. Batch 620/4317 - Avg Loss: 0.7690 - Accuracy: 64.47%\n",
      "Epoch: 0. Batch 630/4317 - Avg Loss: 0.7673 - Accuracy: 64.60%\n",
      "Epoch: 0. Batch 640/4317 - Avg Loss: 0.7658 - Accuracy: 64.64%\n",
      "Epoch: 0. Batch 650/4317 - Avg Loss: 0.7630 - Accuracy: 64.78%\n",
      "Epoch: 0. Batch 660/4317 - Avg Loss: 0.7631 - Accuracy: 64.79%\n",
      "Epoch: 0. Batch 670/4317 - Avg Loss: 0.7626 - Accuracy: 64.83%\n",
      "Epoch: 0. Batch 680/4317 - Avg Loss: 0.7609 - Accuracy: 64.88%\n",
      "Epoch: 0. Batch 690/4317 - Avg Loss: 0.7604 - Accuracy: 64.91%\n",
      "Epoch: 0. Batch 700/4317 - Avg Loss: 0.7600 - Accuracy: 64.92%\n",
      "Epoch: 0. Batch 710/4317 - Avg Loss: 0.7586 - Accuracy: 65.01%\n",
      "Epoch: 0. Batch 720/4317 - Avg Loss: 0.7589 - Accuracy: 65.00%\n",
      "Epoch: 0. Batch 730/4317 - Avg Loss: 0.7588 - Accuracy: 64.96%\n",
      "Epoch: 0. Batch 740/4317 - Avg Loss: 0.7580 - Accuracy: 64.96%\n",
      "Epoch: 0. Batch 750/4317 - Avg Loss: 0.7573 - Accuracy: 65.03%\n",
      "Epoch: 0. Batch 760/4317 - Avg Loss: 0.7564 - Accuracy: 65.12%\n",
      "Epoch: 0. Batch 770/4317 - Avg Loss: 0.7559 - Accuracy: 65.16%\n",
      "Epoch: 0. Batch 780/4317 - Avg Loss: 0.7536 - Accuracy: 65.36%\n",
      "Epoch: 0. Batch 790/4317 - Avg Loss: 0.7527 - Accuracy: 65.42%\n",
      "Epoch: 0. Batch 800/4317 - Avg Loss: 0.7520 - Accuracy: 65.45%\n",
      "Epoch: 0. Batch 810/4317 - Avg Loss: 0.7510 - Accuracy: 65.51%\n",
      "Epoch: 0. Batch 820/4317 - Avg Loss: 0.7504 - Accuracy: 65.51%\n",
      "Epoch: 0. Batch 830/4317 - Avg Loss: 0.7495 - Accuracy: 65.57%\n",
      "Epoch: 0. Batch 840/4317 - Avg Loss: 0.7477 - Accuracy: 65.67%\n",
      "Epoch: 0. Batch 850/4317 - Avg Loss: 0.7471 - Accuracy: 65.70%\n",
      "Epoch: 0. Batch 860/4317 - Avg Loss: 0.7455 - Accuracy: 65.80%\n",
      "Epoch: 0. Batch 870/4317 - Avg Loss: 0.7448 - Accuracy: 65.87%\n",
      "Epoch: 0. Batch 880/4317 - Avg Loss: 0.7444 - Accuracy: 65.95%\n",
      "Epoch: 0. Batch 890/4317 - Avg Loss: 0.7430 - Accuracy: 66.01%\n",
      "Epoch: 0. Batch 900/4317 - Avg Loss: 0.7420 - Accuracy: 66.08%\n",
      "Epoch: 0. Batch 910/4317 - Avg Loss: 0.7418 - Accuracy: 66.12%\n",
      "Epoch: 0. Batch 920/4317 - Avg Loss: 0.7409 - Accuracy: 66.18%\n",
      "Epoch: 0. Batch 930/4317 - Avg Loss: 0.7404 - Accuracy: 66.21%\n",
      "Epoch: 0. Batch 940/4317 - Avg Loss: 0.7387 - Accuracy: 66.31%\n",
      "Epoch: 0. Batch 950/4317 - Avg Loss: 0.7372 - Accuracy: 66.36%\n",
      "Epoch: 0. Batch 960/4317 - Avg Loss: 0.7371 - Accuracy: 66.37%\n",
      "Epoch: 0. Batch 970/4317 - Avg Loss: 0.7361 - Accuracy: 66.41%\n",
      "Epoch: 0. Batch 980/4317 - Avg Loss: 0.7351 - Accuracy: 66.46%\n",
      "Epoch: 0. Batch 990/4317 - Avg Loss: 0.7347 - Accuracy: 66.47%\n",
      "Epoch: 0. Batch 1000/4317 - Avg Loss: 0.7349 - Accuracy: 66.45%\n",
      "Epoch: 0. Batch 1010/4317 - Avg Loss: 0.7337 - Accuracy: 66.52%\n",
      "Epoch: 0. Batch 1020/4317 - Avg Loss: 0.7328 - Accuracy: 66.56%\n",
      "Epoch: 0. Batch 1030/4317 - Avg Loss: 0.7320 - Accuracy: 66.61%\n",
      "Epoch: 0. Batch 1040/4317 - Avg Loss: 0.7304 - Accuracy: 66.70%\n",
      "Epoch: 0. Batch 1050/4317 - Avg Loss: 0.7291 - Accuracy: 66.76%\n",
      "Epoch: 0. Batch 1060/4317 - Avg Loss: 0.7283 - Accuracy: 66.83%\n",
      "Epoch: 0. Batch 1070/4317 - Avg Loss: 0.7270 - Accuracy: 66.89%\n",
      "Epoch: 0. Batch 1080/4317 - Avg Loss: 0.7256 - Accuracy: 66.98%\n",
      "Epoch: 0. Batch 1090/4317 - Avg Loss: 0.7253 - Accuracy: 67.00%\n",
      "Epoch: 0. Batch 1100/4317 - Avg Loss: 0.7247 - Accuracy: 67.04%\n",
      "Epoch: 0. Batch 1110/4317 - Avg Loss: 0.7243 - Accuracy: 67.01%\n",
      "Epoch: 0. Batch 1120/4317 - Avg Loss: 0.7230 - Accuracy: 67.09%\n",
      "Epoch: 0. Batch 1130/4317 - Avg Loss: 0.7228 - Accuracy: 67.11%\n",
      "Epoch: 0. Batch 1140/4317 - Avg Loss: 0.7216 - Accuracy: 67.18%\n",
      "Epoch: 0. Batch 1150/4317 - Avg Loss: 0.7211 - Accuracy: 67.25%\n",
      "Epoch: 0. Batch 1160/4317 - Avg Loss: 0.7209 - Accuracy: 67.26%\n",
      "Epoch: 0. Batch 1170/4317 - Avg Loss: 0.7205 - Accuracy: 67.26%\n",
      "Epoch: 0. Batch 1180/4317 - Avg Loss: 0.7199 - Accuracy: 67.34%\n",
      "Epoch: 0. Batch 1190/4317 - Avg Loss: 0.7195 - Accuracy: 67.39%\n",
      "Epoch: 0. Batch 1200/4317 - Avg Loss: 0.7200 - Accuracy: 67.39%\n",
      "Epoch: 0. Batch 1210/4317 - Avg Loss: 0.7194 - Accuracy: 67.45%\n",
      "Epoch: 0. Batch 1220/4317 - Avg Loss: 0.7191 - Accuracy: 67.48%\n",
      "Epoch: 0. Batch 1230/4317 - Avg Loss: 0.7182 - Accuracy: 67.53%\n",
      "Epoch: 0. Batch 1240/4317 - Avg Loss: 0.7172 - Accuracy: 67.58%\n",
      "Epoch: 0. Batch 1250/4317 - Avg Loss: 0.7166 - Accuracy: 67.59%\n",
      "Epoch: 0. Batch 1260/4317 - Avg Loss: 0.7167 - Accuracy: 67.60%\n",
      "Epoch: 0. Batch 1270/4317 - Avg Loss: 0.7154 - Accuracy: 67.68%\n",
      "Epoch: 0. Batch 1280/4317 - Avg Loss: 0.7151 - Accuracy: 67.67%\n",
      "Epoch: 0. Batch 1290/4317 - Avg Loss: 0.7150 - Accuracy: 67.67%\n",
      "Epoch: 0. Batch 1300/4317 - Avg Loss: 0.7149 - Accuracy: 67.67%\n",
      "Epoch: 0. Batch 1310/4317 - Avg Loss: 0.7144 - Accuracy: 67.70%\n",
      "Epoch: 0. Batch 1320/4317 - Avg Loss: 0.7135 - Accuracy: 67.73%\n",
      "Epoch: 0. Batch 1330/4317 - Avg Loss: 0.7129 - Accuracy: 67.75%\n",
      "Epoch: 0. Batch 1340/4317 - Avg Loss: 0.7130 - Accuracy: 67.71%\n",
      "Epoch: 0. Batch 1350/4317 - Avg Loss: 0.7127 - Accuracy: 67.70%\n",
      "Epoch: 0. Batch 1360/4317 - Avg Loss: 0.7121 - Accuracy: 67.75%\n",
      "Epoch: 0. Batch 1370/4317 - Avg Loss: 0.7121 - Accuracy: 67.76%\n",
      "Epoch: 0. Batch 1380/4317 - Avg Loss: 0.7118 - Accuracy: 67.77%\n",
      "Epoch: 0. Batch 1390/4317 - Avg Loss: 0.7114 - Accuracy: 67.76%\n",
      "Epoch: 0. Batch 1400/4317 - Avg Loss: 0.7112 - Accuracy: 67.76%\n",
      "Epoch: 0. Batch 1410/4317 - Avg Loss: 0.7107 - Accuracy: 67.80%\n",
      "Epoch: 0. Batch 1420/4317 - Avg Loss: 0.7099 - Accuracy: 67.84%\n",
      "Epoch: 0. Batch 1430/4317 - Avg Loss: 0.7089 - Accuracy: 67.89%\n",
      "Epoch: 0. Batch 1440/4317 - Avg Loss: 0.7081 - Accuracy: 67.93%\n",
      "Epoch: 0. Batch 1450/4317 - Avg Loss: 0.7081 - Accuracy: 67.95%\n",
      "Epoch: 0. Batch 1460/4317 - Avg Loss: 0.7075 - Accuracy: 67.95%\n",
      "Epoch: 0. Batch 1470/4317 - Avg Loss: 0.7072 - Accuracy: 67.98%\n",
      "Epoch: 0. Batch 1480/4317 - Avg Loss: 0.7070 - Accuracy: 67.98%\n",
      "Epoch: 0. Batch 1490/4317 - Avg Loss: 0.7069 - Accuracy: 67.96%\n",
      "Epoch: 0. Batch 1500/4317 - Avg Loss: 0.7062 - Accuracy: 68.03%\n",
      "Epoch: 0. Batch 1510/4317 - Avg Loss: 0.7060 - Accuracy: 68.06%\n",
      "Epoch: 0. Batch 1520/4317 - Avg Loss: 0.7055 - Accuracy: 68.08%\n",
      "Epoch: 0. Batch 1530/4317 - Avg Loss: 0.7044 - Accuracy: 68.11%\n",
      "Epoch: 0. Batch 1540/4317 - Avg Loss: 0.7039 - Accuracy: 68.14%\n",
      "Epoch: 0. Batch 1550/4317 - Avg Loss: 0.7036 - Accuracy: 68.13%\n",
      "Epoch: 0. Batch 1560/4317 - Avg Loss: 0.7029 - Accuracy: 68.15%\n",
      "Epoch: 0. Batch 1570/4317 - Avg Loss: 0.7029 - Accuracy: 68.15%\n",
      "Epoch: 0. Batch 1580/4317 - Avg Loss: 0.7026 - Accuracy: 68.17%\n",
      "Epoch: 0. Batch 1590/4317 - Avg Loss: 0.7025 - Accuracy: 68.21%\n",
      "Epoch: 0. Batch 1600/4317 - Avg Loss: 0.7017 - Accuracy: 68.24%\n",
      "Epoch: 0. Batch 1610/4317 - Avg Loss: 0.7011 - Accuracy: 68.27%\n",
      "Epoch: 0. Batch 1620/4317 - Avg Loss: 0.7016 - Accuracy: 68.26%\n",
      "Epoch: 0. Batch 1630/4317 - Avg Loss: 0.7013 - Accuracy: 68.29%\n",
      "Epoch: 0. Batch 1640/4317 - Avg Loss: 0.7011 - Accuracy: 68.31%\n",
      "Epoch: 0. Batch 1650/4317 - Avg Loss: 0.7012 - Accuracy: 68.30%\n",
      "Epoch: 0. Batch 1660/4317 - Avg Loss: 0.7003 - Accuracy: 68.36%\n",
      "Epoch: 0. Batch 1670/4317 - Avg Loss: 0.7000 - Accuracy: 68.36%\n",
      "Epoch: 0. Batch 1680/4317 - Avg Loss: 0.6996 - Accuracy: 68.38%\n",
      "Epoch: 0. Batch 1690/4317 - Avg Loss: 0.6992 - Accuracy: 68.38%\n",
      "Epoch: 0. Batch 1700/4317 - Avg Loss: 0.6987 - Accuracy: 68.39%\n",
      "Epoch: 0. Batch 1710/4317 - Avg Loss: 0.6985 - Accuracy: 68.40%\n",
      "Epoch: 0. Batch 1720/4317 - Avg Loss: 0.6979 - Accuracy: 68.44%\n",
      "Epoch: 0. Batch 1730/4317 - Avg Loss: 0.6973 - Accuracy: 68.48%\n",
      "Epoch: 0. Batch 1740/4317 - Avg Loss: 0.6974 - Accuracy: 68.47%\n",
      "Epoch: 0. Batch 1750/4317 - Avg Loss: 0.6969 - Accuracy: 68.50%\n",
      "Epoch: 0. Batch 1760/4317 - Avg Loss: 0.6965 - Accuracy: 68.50%\n",
      "Epoch: 0. Batch 1770/4317 - Avg Loss: 0.6963 - Accuracy: 68.49%\n",
      "Epoch: 0. Batch 1780/4317 - Avg Loss: 0.6961 - Accuracy: 68.50%\n",
      "Epoch: 0. Batch 1790/4317 - Avg Loss: 0.6957 - Accuracy: 68.51%\n",
      "Epoch: 0. Batch 1800/4317 - Avg Loss: 0.6955 - Accuracy: 68.53%\n",
      "Epoch: 0. Batch 1810/4317 - Avg Loss: 0.6952 - Accuracy: 68.57%\n",
      "Epoch: 0. Batch 1820/4317 - Avg Loss: 0.6952 - Accuracy: 68.57%\n",
      "Epoch: 0. Batch 1830/4317 - Avg Loss: 0.6947 - Accuracy: 68.59%\n",
      "Epoch: 0. Batch 1840/4317 - Avg Loss: 0.6945 - Accuracy: 68.62%\n",
      "Epoch: 0. Batch 1850/4317 - Avg Loss: 0.6951 - Accuracy: 68.59%\n",
      "Epoch: 0. Batch 1860/4317 - Avg Loss: 0.6945 - Accuracy: 68.61%\n",
      "Epoch: 0. Batch 1870/4317 - Avg Loss: 0.6946 - Accuracy: 68.62%\n",
      "Epoch: 0. Batch 1880/4317 - Avg Loss: 0.6942 - Accuracy: 68.65%\n",
      "Epoch: 0. Batch 1890/4317 - Avg Loss: 0.6939 - Accuracy: 68.68%\n",
      "Epoch: 0. Batch 1900/4317 - Avg Loss: 0.6934 - Accuracy: 68.69%\n",
      "Epoch: 0. Batch 1910/4317 - Avg Loss: 0.6934 - Accuracy: 68.71%\n",
      "Epoch: 0. Batch 1920/4317 - Avg Loss: 0.6927 - Accuracy: 68.71%\n",
      "Epoch: 0. Batch 1930/4317 - Avg Loss: 0.6922 - Accuracy: 68.75%\n",
      "Epoch: 0. Batch 1940/4317 - Avg Loss: 0.6917 - Accuracy: 68.79%\n",
      "Epoch: 0. Batch 1950/4317 - Avg Loss: 0.6912 - Accuracy: 68.81%\n",
      "Epoch: 0. Batch 1960/4317 - Avg Loss: 0.6910 - Accuracy: 68.82%\n",
      "Epoch: 0. Batch 1970/4317 - Avg Loss: 0.6910 - Accuracy: 68.83%\n",
      "Epoch: 0. Batch 1980/4317 - Avg Loss: 0.6907 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 1990/4317 - Avg Loss: 0.6908 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 2000/4317 - Avg Loss: 0.6908 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 2010/4317 - Avg Loss: 0.6910 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 2020/4317 - Avg Loss: 0.6910 - Accuracy: 68.83%\n",
      "Epoch: 0. Batch 2030/4317 - Avg Loss: 0.6907 - Accuracy: 68.86%\n",
      "Epoch: 0. Batch 2040/4317 - Avg Loss: 0.6907 - Accuracy: 68.88%\n",
      "Epoch: 0. Batch 2050/4317 - Avg Loss: 0.6904 - Accuracy: 68.89%\n",
      "Epoch: 0. Batch 2060/4317 - Avg Loss: 0.6904 - Accuracy: 68.88%\n",
      "Epoch: 0. Batch 2070/4317 - Avg Loss: 0.6904 - Accuracy: 68.87%\n",
      "Epoch: 0. Batch 2080/4317 - Avg Loss: 0.6910 - Accuracy: 68.83%\n",
      "Epoch: 0. Batch 2090/4317 - Avg Loss: 0.6909 - Accuracy: 68.84%\n",
      "Epoch: 0. Batch 2100/4317 - Avg Loss: 0.6909 - Accuracy: 68.85%\n",
      "Epoch: 0. Batch 2110/4317 - Avg Loss: 0.6902 - Accuracy: 68.90%\n",
      "Epoch: 0. Batch 2120/4317 - Avg Loss: 0.6898 - Accuracy: 68.93%\n",
      "Epoch: 0. Batch 2130/4317 - Avg Loss: 0.6901 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2140/4317 - Avg Loss: 0.6899 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2150/4317 - Avg Loss: 0.6901 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2160/4317 - Avg Loss: 0.6899 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2170/4317 - Avg Loss: 0.6896 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2180/4317 - Avg Loss: 0.6895 - Accuracy: 68.91%\n",
      "Epoch: 0. Batch 2190/4317 - Avg Loss: 0.6891 - Accuracy: 68.92%\n",
      "Epoch: 0. Batch 2200/4317 - Avg Loss: 0.6885 - Accuracy: 68.96%\n",
      "Epoch: 0. Batch 2210/4317 - Avg Loss: 0.6882 - Accuracy: 68.99%\n",
      "Epoch: 0. Batch 2220/4317 - Avg Loss: 0.6881 - Accuracy: 69.00%\n",
      "Epoch: 0. Batch 2230/4317 - Avg Loss: 0.6881 - Accuracy: 69.01%\n",
      "Epoch: 0. Batch 2240/4317 - Avg Loss: 0.6879 - Accuracy: 69.01%\n",
      "Epoch: 0. Batch 2250/4317 - Avg Loss: 0.6875 - Accuracy: 69.03%\n",
      "Epoch: 0. Batch 2260/4317 - Avg Loss: 0.6868 - Accuracy: 69.07%\n",
      "Epoch: 0. Batch 2270/4317 - Avg Loss: 0.6865 - Accuracy: 69.08%\n",
      "Epoch: 0. Batch 2280/4317 - Avg Loss: 0.6868 - Accuracy: 69.07%\n",
      "Epoch: 0. Batch 2290/4317 - Avg Loss: 0.6864 - Accuracy: 69.10%\n",
      "Epoch: 0. Batch 2300/4317 - Avg Loss: 0.6860 - Accuracy: 69.11%\n",
      "Epoch: 0. Batch 2310/4317 - Avg Loss: 0.6862 - Accuracy: 69.10%\n",
      "Epoch: 0. Batch 2320/4317 - Avg Loss: 0.6861 - Accuracy: 69.09%\n",
      "Epoch: 0. Batch 2330/4317 - Avg Loss: 0.6859 - Accuracy: 69.08%\n",
      "Epoch: 0. Batch 2340/4317 - Avg Loss: 0.6857 - Accuracy: 69.08%\n",
      "Epoch: 0. Batch 2350/4317 - Avg Loss: 0.6855 - Accuracy: 69.11%\n",
      "Epoch: 0. Batch 2360/4317 - Avg Loss: 0.6854 - Accuracy: 69.12%\n",
      "Epoch: 0. Batch 2370/4317 - Avg Loss: 0.6855 - Accuracy: 69.12%\n",
      "Epoch: 0. Batch 2380/4317 - Avg Loss: 0.6854 - Accuracy: 69.13%\n",
      "Epoch: 0. Batch 2390/4317 - Avg Loss: 0.6850 - Accuracy: 69.16%\n",
      "Epoch: 0. Batch 2400/4317 - Avg Loss: 0.6845 - Accuracy: 69.18%\n",
      "Epoch: 0. Batch 2410/4317 - Avg Loss: 0.6843 - Accuracy: 69.19%\n",
      "Epoch: 0. Batch 2420/4317 - Avg Loss: 0.6843 - Accuracy: 69.19%\n",
      "Epoch: 0. Batch 2430/4317 - Avg Loss: 0.6843 - Accuracy: 69.19%\n",
      "Epoch: 0. Batch 2440/4317 - Avg Loss: 0.6842 - Accuracy: 69.20%\n",
      "Epoch: 0. Batch 2450/4317 - Avg Loss: 0.6837 - Accuracy: 69.21%\n",
      "Epoch: 0. Batch 2460/4317 - Avg Loss: 0.6836 - Accuracy: 69.23%\n",
      "Epoch: 0. Batch 2470/4317 - Avg Loss: 0.6833 - Accuracy: 69.25%\n",
      "Epoch: 0. Batch 2480/4317 - Avg Loss: 0.6829 - Accuracy: 69.28%\n",
      "Epoch: 0. Batch 2490/4317 - Avg Loss: 0.6825 - Accuracy: 69.29%\n",
      "Epoch: 0. Batch 2500/4317 - Avg Loss: 0.6823 - Accuracy: 69.29%\n",
      "Epoch: 0. Batch 2510/4317 - Avg Loss: 0.6820 - Accuracy: 69.30%\n",
      "Epoch: 0. Batch 2520/4317 - Avg Loss: 0.6818 - Accuracy: 69.32%\n",
      "Epoch: 0. Batch 2530/4317 - Avg Loss: 0.6819 - Accuracy: 69.31%\n",
      "Epoch: 0. Batch 2540/4317 - Avg Loss: 0.6817 - Accuracy: 69.33%\n",
      "Epoch: 0. Batch 2550/4317 - Avg Loss: 0.6815 - Accuracy: 69.33%\n",
      "Epoch: 0. Batch 2560/4317 - Avg Loss: 0.6811 - Accuracy: 69.36%\n",
      "Epoch: 0. Batch 2570/4317 - Avg Loss: 0.6807 - Accuracy: 69.38%\n",
      "Epoch: 0. Batch 2580/4317 - Avg Loss: 0.6805 - Accuracy: 69.39%\n",
      "Epoch: 0. Batch 2590/4317 - Avg Loss: 0.6800 - Accuracy: 69.42%\n",
      "Epoch: 0. Batch 2600/4317 - Avg Loss: 0.6797 - Accuracy: 69.44%\n",
      "Epoch: 0. Batch 2610/4317 - Avg Loss: 0.6792 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2620/4317 - Avg Loss: 0.6794 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2630/4317 - Avg Loss: 0.6794 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2640/4317 - Avg Loss: 0.6791 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2650/4317 - Avg Loss: 0.6790 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2660/4317 - Avg Loss: 0.6789 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2670/4317 - Avg Loss: 0.6792 - Accuracy: 69.44%\n",
      "Epoch: 0. Batch 2680/4317 - Avg Loss: 0.6791 - Accuracy: 69.45%\n",
      "Epoch: 0. Batch 2690/4317 - Avg Loss: 0.6789 - Accuracy: 69.47%\n",
      "Epoch: 0. Batch 2700/4317 - Avg Loss: 0.6784 - Accuracy: 69.49%\n",
      "Epoch: 0. Batch 2710/4317 - Avg Loss: 0.6782 - Accuracy: 69.50%\n",
      "Epoch: 0. Batch 2720/4317 - Avg Loss: 0.6776 - Accuracy: 69.53%\n",
      "Epoch: 0. Batch 2730/4317 - Avg Loss: 0.6776 - Accuracy: 69.52%\n",
      "Epoch: 0. Batch 2740/4317 - Avg Loss: 0.6773 - Accuracy: 69.54%\n",
      "Epoch: 0. Batch 2750/4317 - Avg Loss: 0.6773 - Accuracy: 69.56%\n",
      "Epoch: 0. Batch 2760/4317 - Avg Loss: 0.6769 - Accuracy: 69.57%\n",
      "Epoch: 0. Batch 2770/4317 - Avg Loss: 0.6766 - Accuracy: 69.58%\n",
      "Epoch: 0. Batch 2780/4317 - Avg Loss: 0.6765 - Accuracy: 69.58%\n",
      "Epoch: 0. Batch 2790/4317 - Avg Loss: 0.6764 - Accuracy: 69.57%\n",
      "Epoch: 0. Batch 2800/4317 - Avg Loss: 0.6762 - Accuracy: 69.59%\n",
      "Epoch: 0. Batch 2810/4317 - Avg Loss: 0.6759 - Accuracy: 69.60%\n",
      "Epoch: 0. Batch 2820/4317 - Avg Loss: 0.6757 - Accuracy: 69.61%\n",
      "Epoch: 0. Batch 2830/4317 - Avg Loss: 0.6754 - Accuracy: 69.63%\n",
      "Epoch: 0. Batch 2840/4317 - Avg Loss: 0.6757 - Accuracy: 69.62%\n",
      "Epoch: 0. Batch 2850/4317 - Avg Loss: 0.6759 - Accuracy: 69.60%\n",
      "Epoch: 0. Batch 2860/4317 - Avg Loss: 0.6758 - Accuracy: 69.62%\n",
      "Epoch: 0. Batch 2870/4317 - Avg Loss: 0.6753 - Accuracy: 69.65%\n",
      "Epoch: 0. Batch 2880/4317 - Avg Loss: 0.6754 - Accuracy: 69.66%\n",
      "Epoch: 0. Batch 2890/4317 - Avg Loss: 0.6752 - Accuracy: 69.65%\n",
      "Epoch: 0. Batch 2900/4317 - Avg Loss: 0.6753 - Accuracy: 69.64%\n",
      "Epoch: 0. Batch 2910/4317 - Avg Loss: 0.6751 - Accuracy: 69.66%\n",
      "Epoch: 0. Batch 2920/4317 - Avg Loss: 0.6751 - Accuracy: 69.67%\n",
      "Epoch: 0. Batch 2930/4317 - Avg Loss: 0.6748 - Accuracy: 69.68%\n",
      "Epoch: 0. Batch 2940/4317 - Avg Loss: 0.6750 - Accuracy: 69.68%\n",
      "Epoch: 0. Batch 2950/4317 - Avg Loss: 0.6752 - Accuracy: 69.66%\n",
      "Epoch: 0. Batch 2960/4317 - Avg Loss: 0.6749 - Accuracy: 69.68%\n",
      "Epoch: 0. Batch 2970/4317 - Avg Loss: 0.6745 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 2980/4317 - Avg Loss: 0.6746 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 2990/4317 - Avg Loss: 0.6747 - Accuracy: 69.68%\n",
      "Epoch: 0. Batch 3000/4317 - Avg Loss: 0.6747 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 3010/4317 - Avg Loss: 0.6745 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 3020/4317 - Avg Loss: 0.6745 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 3030/4317 - Avg Loss: 0.6742 - Accuracy: 69.72%\n",
      "Epoch: 0. Batch 3040/4317 - Avg Loss: 0.6736 - Accuracy: 69.74%\n",
      "Epoch: 0. Batch 3050/4317 - Avg Loss: 0.6739 - Accuracy: 69.73%\n",
      "Epoch: 0. Batch 3060/4317 - Avg Loss: 0.6739 - Accuracy: 69.74%\n",
      "Epoch: 0. Batch 3070/4317 - Avg Loss: 0.6737 - Accuracy: 69.75%\n",
      "Epoch: 0. Batch 3080/4317 - Avg Loss: 0.6736 - Accuracy: 69.76%\n",
      "Epoch: 0. Batch 3090/4317 - Avg Loss: 0.6733 - Accuracy: 69.77%\n",
      "Epoch: 0. Batch 3100/4317 - Avg Loss: 0.6727 - Accuracy: 69.80%\n",
      "Epoch: 0. Batch 3110/4317 - Avg Loss: 0.6726 - Accuracy: 69.80%\n",
      "Epoch: 0. Batch 3120/4317 - Avg Loss: 0.6727 - Accuracy: 69.80%\n",
      "Epoch: 0. Batch 3130/4317 - Avg Loss: 0.6726 - Accuracy: 69.80%\n",
      "Epoch: 0. Batch 3140/4317 - Avg Loss: 0.6722 - Accuracy: 69.82%\n",
      "Epoch: 0. Batch 3150/4317 - Avg Loss: 0.6720 - Accuracy: 69.84%\n",
      "Epoch: 0. Batch 3160/4317 - Avg Loss: 0.6719 - Accuracy: 69.84%\n",
      "Epoch: 0. Batch 3170/4317 - Avg Loss: 0.6717 - Accuracy: 69.84%\n",
      "Epoch: 0. Batch 3180/4317 - Avg Loss: 0.6714 - Accuracy: 69.86%\n",
      "Epoch: 0. Batch 3190/4317 - Avg Loss: 0.6710 - Accuracy: 69.89%\n",
      "Epoch: 0. Batch 3200/4317 - Avg Loss: 0.6707 - Accuracy: 69.90%\n",
      "Epoch: 0. Batch 3210/4317 - Avg Loss: 0.6704 - Accuracy: 69.91%\n",
      "Epoch: 0. Batch 3220/4317 - Avg Loss: 0.6706 - Accuracy: 69.90%\n",
      "Epoch: 0. Batch 3230/4317 - Avg Loss: 0.6702 - Accuracy: 69.93%\n",
      "Epoch: 0. Batch 3240/4317 - Avg Loss: 0.6703 - Accuracy: 69.93%\n",
      "Epoch: 0. Batch 3250/4317 - Avg Loss: 0.6701 - Accuracy: 69.93%\n",
      "Epoch: 0. Batch 3260/4317 - Avg Loss: 0.6700 - Accuracy: 69.94%\n",
      "Epoch: 0. Batch 3270/4317 - Avg Loss: 0.6700 - Accuracy: 69.94%\n",
      "Epoch: 0. Batch 3280/4317 - Avg Loss: 0.6698 - Accuracy: 69.95%\n",
      "Epoch: 0. Batch 3290/4317 - Avg Loss: 0.6698 - Accuracy: 69.95%\n",
      "Epoch: 0. Batch 3300/4317 - Avg Loss: 0.6696 - Accuracy: 69.97%\n",
      "Epoch: 0. Batch 3310/4317 - Avg Loss: 0.6696 - Accuracy: 69.97%\n",
      "Epoch: 0. Batch 3320/4317 - Avg Loss: 0.6698 - Accuracy: 69.95%\n",
      "Epoch: 0. Batch 3330/4317 - Avg Loss: 0.6695 - Accuracy: 69.96%\n",
      "Epoch: 0. Batch 3340/4317 - Avg Loss: 0.6695 - Accuracy: 69.96%\n",
      "Epoch: 0. Batch 3350/4317 - Avg Loss: 0.6694 - Accuracy: 69.97%\n",
      "Epoch: 0. Batch 3360/4317 - Avg Loss: 0.6694 - Accuracy: 69.99%\n",
      "Epoch: 0. Batch 3370/4317 - Avg Loss: 0.6693 - Accuracy: 69.99%\n",
      "Epoch: 0. Batch 3380/4317 - Avg Loss: 0.6691 - Accuracy: 69.99%\n",
      "Epoch: 0. Batch 3390/4317 - Avg Loss: 0.6688 - Accuracy: 70.00%\n",
      "Epoch: 0. Batch 3400/4317 - Avg Loss: 0.6688 - Accuracy: 70.00%\n",
      "Epoch: 0. Batch 3410/4317 - Avg Loss: 0.6688 - Accuracy: 70.00%\n",
      "Epoch: 0. Batch 3420/4317 - Avg Loss: 0.6686 - Accuracy: 70.01%\n",
      "Epoch: 0. Batch 3430/4317 - Avg Loss: 0.6686 - Accuracy: 70.01%\n",
      "Epoch: 0. Batch 3440/4317 - Avg Loss: 0.6684 - Accuracy: 70.03%\n",
      "Epoch: 0. Batch 3450/4317 - Avg Loss: 0.6687 - Accuracy: 70.02%\n",
      "Epoch: 0. Batch 3460/4317 - Avg Loss: 0.6683 - Accuracy: 70.06%\n",
      "Epoch: 0. Batch 3470/4317 - Avg Loss: 0.6681 - Accuracy: 70.06%\n",
      "Epoch: 0. Batch 3480/4317 - Avg Loss: 0.6680 - Accuracy: 70.07%\n",
      "Epoch: 0. Batch 3490/4317 - Avg Loss: 0.6679 - Accuracy: 70.06%\n",
      "Epoch: 0. Batch 3500/4317 - Avg Loss: 0.6677 - Accuracy: 70.07%\n",
      "Epoch: 0. Batch 3510/4317 - Avg Loss: 0.6675 - Accuracy: 70.07%\n",
      "Epoch: 0. Batch 3520/4317 - Avg Loss: 0.6675 - Accuracy: 70.08%\n",
      "Epoch: 0. Batch 3530/4317 - Avg Loss: 0.6672 - Accuracy: 70.10%\n",
      "Epoch: 0. Batch 3540/4317 - Avg Loss: 0.6672 - Accuracy: 70.09%\n",
      "Epoch: 0. Batch 3550/4317 - Avg Loss: 0.6672 - Accuracy: 70.09%\n",
      "Epoch: 0. Batch 3560/4317 - Avg Loss: 0.6668 - Accuracy: 70.12%\n",
      "Epoch: 0. Batch 3570/4317 - Avg Loss: 0.6664 - Accuracy: 70.14%\n",
      "Epoch: 0. Batch 3580/4317 - Avg Loss: 0.6663 - Accuracy: 70.14%\n",
      "Epoch: 0. Batch 3590/4317 - Avg Loss: 0.6664 - Accuracy: 70.14%\n",
      "Epoch: 0. Batch 3600/4317 - Avg Loss: 0.6662 - Accuracy: 70.15%\n",
      "Epoch: 0. Batch 3610/4317 - Avg Loss: 0.6660 - Accuracy: 70.17%\n",
      "Epoch: 0. Batch 3620/4317 - Avg Loss: 0.6661 - Accuracy: 70.18%\n",
      "Epoch: 0. Batch 3630/4317 - Avg Loss: 0.6658 - Accuracy: 70.20%\n",
      "Epoch: 0. Batch 3640/4317 - Avg Loss: 0.6656 - Accuracy: 70.22%\n",
      "Epoch: 0. Batch 3650/4317 - Avg Loss: 0.6653 - Accuracy: 70.23%\n",
      "Epoch: 0. Batch 3660/4317 - Avg Loss: 0.6653 - Accuracy: 70.23%\n",
      "Epoch: 0. Batch 3670/4317 - Avg Loss: 0.6651 - Accuracy: 70.25%\n",
      "Epoch: 0. Batch 3680/4317 - Avg Loss: 0.6650 - Accuracy: 70.25%\n",
      "Epoch: 0. Batch 3690/4317 - Avg Loss: 0.6652 - Accuracy: 70.25%\n",
      "Epoch: 0. Batch 3700/4317 - Avg Loss: 0.6650 - Accuracy: 70.26%\n",
      "Epoch: 0. Batch 3710/4317 - Avg Loss: 0.6649 - Accuracy: 70.27%\n",
      "Epoch: 0. Batch 3720/4317 - Avg Loss: 0.6646 - Accuracy: 70.28%\n",
      "Epoch: 0. Batch 3730/4317 - Avg Loss: 0.6645 - Accuracy: 70.29%\n",
      "Epoch: 0. Batch 3740/4317 - Avg Loss: 0.6645 - Accuracy: 70.29%\n",
      "Epoch: 0. Batch 3750/4317 - Avg Loss: 0.6647 - Accuracy: 70.29%\n",
      "Epoch: 0. Batch 3760/4317 - Avg Loss: 0.6647 - Accuracy: 70.28%\n",
      "Epoch: 0. Batch 3770/4317 - Avg Loss: 0.6646 - Accuracy: 70.29%\n",
      "Epoch: 0. Batch 3780/4317 - Avg Loss: 0.6646 - Accuracy: 70.29%\n",
      "Epoch: 0. Batch 3790/4317 - Avg Loss: 0.6646 - Accuracy: 70.30%\n",
      "Epoch: 0. Batch 3800/4317 - Avg Loss: 0.6644 - Accuracy: 70.32%\n",
      "Epoch: 0. Batch 3810/4317 - Avg Loss: 0.6640 - Accuracy: 70.34%\n",
      "Epoch: 0. Batch 3820/4317 - Avg Loss: 0.6639 - Accuracy: 70.35%\n",
      "Epoch: 0. Batch 3830/4317 - Avg Loss: 0.6639 - Accuracy: 70.36%\n",
      "Epoch: 0. Batch 3840/4317 - Avg Loss: 0.6636 - Accuracy: 70.37%\n",
      "Epoch: 0. Batch 3850/4317 - Avg Loss: 0.6638 - Accuracy: 70.35%\n",
      "Epoch: 0. Batch 3860/4317 - Avg Loss: 0.6635 - Accuracy: 70.37%\n",
      "Epoch: 0. Batch 3870/4317 - Avg Loss: 0.6633 - Accuracy: 70.38%\n",
      "Epoch: 0. Batch 3880/4317 - Avg Loss: 0.6631 - Accuracy: 70.39%\n",
      "Epoch: 0. Batch 3890/4317 - Avg Loss: 0.6630 - Accuracy: 70.40%\n",
      "Epoch: 0. Batch 3900/4317 - Avg Loss: 0.6630 - Accuracy: 70.41%\n",
      "Epoch: 0. Batch 3910/4317 - Avg Loss: 0.6629 - Accuracy: 70.42%\n",
      "Epoch: 0. Batch 3920/4317 - Avg Loss: 0.6631 - Accuracy: 70.40%\n",
      "Epoch: 0. Batch 3930/4317 - Avg Loss: 0.6628 - Accuracy: 70.43%\n",
      "Epoch: 0. Batch 3940/4317 - Avg Loss: 0.6629 - Accuracy: 70.43%\n",
      "Epoch: 0. Batch 3950/4317 - Avg Loss: 0.6628 - Accuracy: 70.43%\n",
      "Epoch: 0. Batch 3960/4317 - Avg Loss: 0.6629 - Accuracy: 70.43%\n",
      "Epoch: 0. Batch 3970/4317 - Avg Loss: 0.6627 - Accuracy: 70.44%\n",
      "Epoch: 0. Batch 3980/4317 - Avg Loss: 0.6629 - Accuracy: 70.42%\n",
      "Epoch: 0. Batch 3990/4317 - Avg Loss: 0.6627 - Accuracy: 70.43%\n",
      "Epoch: 0. Batch 4000/4317 - Avg Loss: 0.6625 - Accuracy: 70.44%\n",
      "Epoch: 0. Batch 4010/4317 - Avg Loss: 0.6624 - Accuracy: 70.45%\n",
      "Epoch: 0. Batch 4020/4317 - Avg Loss: 0.6622 - Accuracy: 70.46%\n",
      "Epoch: 0. Batch 4030/4317 - Avg Loss: 0.6620 - Accuracy: 70.47%\n",
      "Epoch: 0. Batch 4040/4317 - Avg Loss: 0.6622 - Accuracy: 70.47%\n",
      "Epoch: 0. Batch 4050/4317 - Avg Loss: 0.6621 - Accuracy: 70.46%\n",
      "Epoch: 0. Batch 4060/4317 - Avg Loss: 0.6618 - Accuracy: 70.47%\n",
      "Epoch: 0. Batch 4070/4317 - Avg Loss: 0.6615 - Accuracy: 70.48%\n",
      "Epoch: 0. Batch 4080/4317 - Avg Loss: 0.6613 - Accuracy: 70.50%\n",
      "Epoch: 0. Batch 4090/4317 - Avg Loss: 0.6614 - Accuracy: 70.50%\n",
      "Epoch: 0. Batch 4100/4317 - Avg Loss: 0.6613 - Accuracy: 70.50%\n",
      "Epoch: 0. Batch 4110/4317 - Avg Loss: 0.6612 - Accuracy: 70.50%\n",
      "Epoch: 0. Batch 4120/4317 - Avg Loss: 0.6611 - Accuracy: 70.50%\n",
      "Epoch: 0. Batch 4130/4317 - Avg Loss: 0.6611 - Accuracy: 70.51%\n",
      "Epoch: 0. Batch 4140/4317 - Avg Loss: 0.6608 - Accuracy: 70.52%\n",
      "Epoch: 0. Batch 4150/4317 - Avg Loss: 0.6606 - Accuracy: 70.53%\n",
      "Epoch: 0. Batch 4160/4317 - Avg Loss: 0.6606 - Accuracy: 70.53%\n",
      "Epoch: 0. Batch 4170/4317 - Avg Loss: 0.6606 - Accuracy: 70.54%\n",
      "Epoch: 0. Batch 4180/4317 - Avg Loss: 0.6604 - Accuracy: 70.55%\n",
      "Epoch: 0. Batch 4190/4317 - Avg Loss: 0.6603 - Accuracy: 70.55%\n",
      "Epoch: 0. Batch 4200/4317 - Avg Loss: 0.6603 - Accuracy: 70.54%\n",
      "Epoch: 0. Batch 4210/4317 - Avg Loss: 0.6599 - Accuracy: 70.57%\n",
      "Epoch: 0. Batch 4220/4317 - Avg Loss: 0.6598 - Accuracy: 70.57%\n",
      "Epoch: 0. Batch 4230/4317 - Avg Loss: 0.6597 - Accuracy: 70.58%\n",
      "Epoch: 0. Batch 4240/4317 - Avg Loss: 0.6596 - Accuracy: 70.58%\n",
      "Epoch: 0. Batch 4250/4317 - Avg Loss: 0.6592 - Accuracy: 70.60%\n",
      "Epoch: 0. Batch 4260/4317 - Avg Loss: 0.6592 - Accuracy: 70.60%\n",
      "Epoch: 0. Batch 4270/4317 - Avg Loss: 0.6591 - Accuracy: 70.61%\n",
      "Epoch: 0. Batch 4280/4317 - Avg Loss: 0.6589 - Accuracy: 70.62%\n",
      "Epoch: 0. Batch 4290/4317 - Avg Loss: 0.6589 - Accuracy: 70.62%\n",
      "Epoch: 0. Batch 4300/4317 - Avg Loss: 0.6588 - Accuracy: 70.63%\n",
      "Epoch: 0. Batch 4310/4317 - Avg Loss: 0.6587 - Accuracy: 70.63%\n",
      "Train loss: 0.6586 - Train accuracy: 70.64%\n",
      "Validation accuracy: 0.7361\n",
      "Epoch: 1. Batch 0/4317 - Avg Loss: 0.4759 - Accuracy: 87.50%\n",
      "Epoch: 1. Batch 10/4317 - Avg Loss: 0.4967 - Accuracy: 81.25%\n",
      "Epoch: 1. Batch 20/4317 - Avg Loss: 0.4522 - Accuracy: 81.85%\n",
      "Epoch: 1. Batch 30/4317 - Avg Loss: 0.4890 - Accuracy: 78.83%\n",
      "Epoch: 1. Batch 40/4317 - Avg Loss: 0.5037 - Accuracy: 77.74%\n",
      "Epoch: 1. Batch 50/4317 - Avg Loss: 0.4965 - Accuracy: 78.68%\n",
      "Epoch: 1. Batch 60/4317 - Avg Loss: 0.5148 - Accuracy: 78.28%\n",
      "Epoch: 1. Batch 70/4317 - Avg Loss: 0.5222 - Accuracy: 78.08%\n",
      "Epoch: 1. Batch 80/4317 - Avg Loss: 0.5139 - Accuracy: 78.40%\n",
      "Epoch: 1. Batch 90/4317 - Avg Loss: 0.5130 - Accuracy: 77.88%\n",
      "Epoch: 1. Batch 100/4317 - Avg Loss: 0.5082 - Accuracy: 78.09%\n",
      "Epoch: 1. Batch 110/4317 - Avg Loss: 0.5084 - Accuracy: 77.98%\n",
      "Epoch: 1. Batch 120/4317 - Avg Loss: 0.4998 - Accuracy: 78.36%\n",
      "Epoch: 1. Batch 130/4317 - Avg Loss: 0.5037 - Accuracy: 78.44%\n",
      "Epoch: 1. Batch 140/4317 - Avg Loss: 0.5035 - Accuracy: 78.37%\n",
      "Epoch: 1. Batch 150/4317 - Avg Loss: 0.4976 - Accuracy: 78.81%\n",
      "Epoch: 1. Batch 160/4317 - Avg Loss: 0.4914 - Accuracy: 78.80%\n",
      "Epoch: 1. Batch 170/4317 - Avg Loss: 0.4943 - Accuracy: 78.87%\n",
      "Epoch: 1. Batch 180/4317 - Avg Loss: 0.4911 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 190/4317 - Avg Loss: 0.4960 - Accuracy: 78.60%\n",
      "Epoch: 1. Batch 200/4317 - Avg Loss: 0.4980 - Accuracy: 78.61%\n",
      "Epoch: 1. Batch 210/4317 - Avg Loss: 0.4991 - Accuracy: 78.47%\n",
      "Epoch: 1. Batch 220/4317 - Avg Loss: 0.4955 - Accuracy: 78.51%\n",
      "Epoch: 1. Batch 230/4317 - Avg Loss: 0.4948 - Accuracy: 78.41%\n",
      "Epoch: 1. Batch 240/4317 - Avg Loss: 0.4928 - Accuracy: 78.45%\n",
      "Epoch: 1. Batch 250/4317 - Avg Loss: 0.4935 - Accuracy: 78.49%\n",
      "Epoch: 1. Batch 260/4317 - Avg Loss: 0.4927 - Accuracy: 78.57%\n",
      "Epoch: 1. Batch 270/4317 - Avg Loss: 0.4928 - Accuracy: 78.53%\n",
      "Epoch: 1. Batch 280/4317 - Avg Loss: 0.4913 - Accuracy: 78.69%\n",
      "Epoch: 1. Batch 290/4317 - Avg Loss: 0.4909 - Accuracy: 78.67%\n",
      "Epoch: 1. Batch 300/4317 - Avg Loss: 0.4918 - Accuracy: 78.68%\n",
      "Epoch: 1. Batch 310/4317 - Avg Loss: 0.4896 - Accuracy: 78.84%\n",
      "Epoch: 1. Batch 320/4317 - Avg Loss: 0.4878 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 330/4317 - Avg Loss: 0.4897 - Accuracy: 78.87%\n",
      "Epoch: 1. Batch 340/4317 - Avg Loss: 0.4950 - Accuracy: 78.76%\n",
      "Epoch: 1. Batch 350/4317 - Avg Loss: 0.4931 - Accuracy: 78.85%\n",
      "Epoch: 1. Batch 360/4317 - Avg Loss: 0.4940 - Accuracy: 78.81%\n",
      "Epoch: 1. Batch 370/4317 - Avg Loss: 0.4913 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 380/4317 - Avg Loss: 0.4898 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 390/4317 - Avg Loss: 0.4905 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 400/4317 - Avg Loss: 0.4905 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 410/4317 - Avg Loss: 0.4898 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 420/4317 - Avg Loss: 0.4897 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 430/4317 - Avg Loss: 0.4918 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 440/4317 - Avg Loss: 0.4941 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 450/4317 - Avg Loss: 0.4953 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 460/4317 - Avg Loss: 0.4960 - Accuracy: 78.86%\n",
      "Epoch: 1. Batch 470/4317 - Avg Loss: 0.4967 - Accuracy: 78.80%\n",
      "Epoch: 1. Batch 480/4317 - Avg Loss: 0.4952 - Accuracy: 78.89%\n",
      "Epoch: 1. Batch 490/4317 - Avg Loss: 0.4952 - Accuracy: 78.88%\n",
      "Epoch: 1. Batch 500/4317 - Avg Loss: 0.4958 - Accuracy: 78.89%\n",
      "Epoch: 1. Batch 510/4317 - Avg Loss: 0.4948 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 520/4317 - Avg Loss: 0.4952 - Accuracy: 78.93%\n",
      "Epoch: 1. Batch 530/4317 - Avg Loss: 0.4947 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 540/4317 - Avg Loss: 0.4956 - Accuracy: 78.95%\n",
      "Epoch: 1. Batch 550/4317 - Avg Loss: 0.4953 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 560/4317 - Avg Loss: 0.4950 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 570/4317 - Avg Loss: 0.4948 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 580/4317 - Avg Loss: 0.4957 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 590/4317 - Avg Loss: 0.4962 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 600/4317 - Avg Loss: 0.4975 - Accuracy: 78.88%\n",
      "Epoch: 1. Batch 610/4317 - Avg Loss: 0.4975 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 620/4317 - Avg Loss: 0.4975 - Accuracy: 78.93%\n",
      "Epoch: 1. Batch 630/4317 - Avg Loss: 0.4991 - Accuracy: 78.84%\n",
      "Epoch: 1. Batch 640/4317 - Avg Loss: 0.4989 - Accuracy: 78.89%\n",
      "Epoch: 1. Batch 650/4317 - Avg Loss: 0.4972 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 660/4317 - Avg Loss: 0.4970 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 670/4317 - Avg Loss: 0.4966 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 680/4317 - Avg Loss: 0.4972 - Accuracy: 78.96%\n",
      "Epoch: 1. Batch 690/4317 - Avg Loss: 0.4956 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 700/4317 - Avg Loss: 0.4961 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 710/4317 - Avg Loss: 0.4959 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 720/4317 - Avg Loss: 0.4962 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 730/4317 - Avg Loss: 0.4975 - Accuracy: 78.87%\n",
      "Epoch: 1. Batch 740/4317 - Avg Loss: 0.4975 - Accuracy: 78.88%\n",
      "Epoch: 1. Batch 750/4317 - Avg Loss: 0.4973 - Accuracy: 78.95%\n",
      "Epoch: 1. Batch 760/4317 - Avg Loss: 0.4969 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 770/4317 - Avg Loss: 0.4971 - Accuracy: 78.93%\n",
      "Epoch: 1. Batch 780/4317 - Avg Loss: 0.4970 - Accuracy: 78.95%\n",
      "Epoch: 1. Batch 790/4317 - Avg Loss: 0.4964 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 800/4317 - Avg Loss: 0.4963 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 810/4317 - Avg Loss: 0.4959 - Accuracy: 78.96%\n",
      "Epoch: 1. Batch 820/4317 - Avg Loss: 0.4949 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 830/4317 - Avg Loss: 0.4958 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 840/4317 - Avg Loss: 0.4955 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 850/4317 - Avg Loss: 0.4948 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 860/4317 - Avg Loss: 0.4950 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 870/4317 - Avg Loss: 0.4958 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 880/4317 - Avg Loss: 0.4959 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 890/4317 - Avg Loss: 0.4955 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 900/4317 - Avg Loss: 0.4957 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 910/4317 - Avg Loss: 0.4960 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 920/4317 - Avg Loss: 0.4957 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 930/4317 - Avg Loss: 0.4952 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 940/4317 - Avg Loss: 0.4960 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 950/4317 - Avg Loss: 0.4957 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 960/4317 - Avg Loss: 0.4952 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 970/4317 - Avg Loss: 0.4960 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 980/4317 - Avg Loss: 0.4958 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 990/4317 - Avg Loss: 0.4950 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1000/4317 - Avg Loss: 0.4946 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1010/4317 - Avg Loss: 0.4944 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1020/4317 - Avg Loss: 0.4940 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1030/4317 - Avg Loss: 0.4952 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1040/4317 - Avg Loss: 0.4953 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1050/4317 - Avg Loss: 0.4951 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 1060/4317 - Avg Loss: 0.4948 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1070/4317 - Avg Loss: 0.4940 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1080/4317 - Avg Loss: 0.4941 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1090/4317 - Avg Loss: 0.4935 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1100/4317 - Avg Loss: 0.4936 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1110/4317 - Avg Loss: 0.4934 - Accuracy: 79.25%\n",
      "Epoch: 1. Batch 1120/4317 - Avg Loss: 0.4939 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1130/4317 - Avg Loss: 0.4945 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1140/4317 - Avg Loss: 0.4946 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1150/4317 - Avg Loss: 0.4944 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 1160/4317 - Avg Loss: 0.4941 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 1170/4317 - Avg Loss: 0.4933 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1180/4317 - Avg Loss: 0.4934 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 1190/4317 - Avg Loss: 0.4939 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 1200/4317 - Avg Loss: 0.4943 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1210/4317 - Avg Loss: 0.4947 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1220/4317 - Avg Loss: 0.4945 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1230/4317 - Avg Loss: 0.4945 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1240/4317 - Avg Loss: 0.4943 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 1250/4317 - Avg Loss: 0.4937 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1260/4317 - Avg Loss: 0.4941 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1270/4317 - Avg Loss: 0.4937 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1280/4317 - Avg Loss: 0.4933 - Accuracy: 79.32%\n",
      "Epoch: 1. Batch 1290/4317 - Avg Loss: 0.4935 - Accuracy: 79.30%\n",
      "Epoch: 1. Batch 1300/4317 - Avg Loss: 0.4934 - Accuracy: 79.31%\n",
      "Epoch: 1. Batch 1310/4317 - Avg Loss: 0.4928 - Accuracy: 79.32%\n",
      "Epoch: 1. Batch 1320/4317 - Avg Loss: 0.4930 - Accuracy: 79.31%\n",
      "Epoch: 1. Batch 1330/4317 - Avg Loss: 0.4933 - Accuracy: 79.28%\n",
      "Epoch: 1. Batch 1340/4317 - Avg Loss: 0.4933 - Accuracy: 79.28%\n",
      "Epoch: 1. Batch 1350/4317 - Avg Loss: 0.4938 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1360/4317 - Avg Loss: 0.4935 - Accuracy: 79.30%\n",
      "Epoch: 1. Batch 1370/4317 - Avg Loss: 0.4939 - Accuracy: 79.28%\n",
      "Epoch: 1. Batch 1380/4317 - Avg Loss: 0.4940 - Accuracy: 79.29%\n",
      "Epoch: 1. Batch 1390/4317 - Avg Loss: 0.4938 - Accuracy: 79.29%\n",
      "Epoch: 1. Batch 1400/4317 - Avg Loss: 0.4941 - Accuracy: 79.30%\n",
      "Epoch: 1. Batch 1410/4317 - Avg Loss: 0.4944 - Accuracy: 79.29%\n",
      "Epoch: 1. Batch 1420/4317 - Avg Loss: 0.4943 - Accuracy: 79.30%\n",
      "Epoch: 1. Batch 1430/4317 - Avg Loss: 0.4944 - Accuracy: 79.31%\n",
      "Epoch: 1. Batch 1440/4317 - Avg Loss: 0.4951 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1450/4317 - Avg Loss: 0.4950 - Accuracy: 79.26%\n",
      "Epoch: 1. Batch 1460/4317 - Avg Loss: 0.4950 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1470/4317 - Avg Loss: 0.4953 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1480/4317 - Avg Loss: 0.4948 - Accuracy: 79.30%\n",
      "Epoch: 1. Batch 1490/4317 - Avg Loss: 0.4951 - Accuracy: 79.28%\n",
      "Epoch: 1. Batch 1500/4317 - Avg Loss: 0.4951 - Accuracy: 79.29%\n",
      "Epoch: 1. Batch 1510/4317 - Avg Loss: 0.4953 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1520/4317 - Avg Loss: 0.4955 - Accuracy: 79.26%\n",
      "Epoch: 1. Batch 1530/4317 - Avg Loss: 0.4959 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1540/4317 - Avg Loss: 0.4963 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1550/4317 - Avg Loss: 0.4958 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1560/4317 - Avg Loss: 0.4956 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1570/4317 - Avg Loss: 0.4956 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1580/4317 - Avg Loss: 0.4955 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1590/4317 - Avg Loss: 0.4961 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 1600/4317 - Avg Loss: 0.4963 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1610/4317 - Avg Loss: 0.4969 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 1620/4317 - Avg Loss: 0.4968 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 1630/4317 - Avg Loss: 0.4965 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 1640/4317 - Avg Loss: 0.4957 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1650/4317 - Avg Loss: 0.4953 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1660/4317 - Avg Loss: 0.4952 - Accuracy: 79.24%\n",
      "Epoch: 1. Batch 1670/4317 - Avg Loss: 0.4946 - Accuracy: 79.27%\n",
      "Epoch: 1. Batch 1680/4317 - Avg Loss: 0.4952 - Accuracy: 79.24%\n",
      "Epoch: 1. Batch 1690/4317 - Avg Loss: 0.4955 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1700/4317 - Avg Loss: 0.4958 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1710/4317 - Avg Loss: 0.4961 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 1720/4317 - Avg Loss: 0.4962 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 1730/4317 - Avg Loss: 0.4962 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1740/4317 - Avg Loss: 0.4962 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 1750/4317 - Avg Loss: 0.4960 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 1760/4317 - Avg Loss: 0.4959 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1770/4317 - Avg Loss: 0.4955 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1780/4317 - Avg Loss: 0.4953 - Accuracy: 79.25%\n",
      "Epoch: 1. Batch 1790/4317 - Avg Loss: 0.4958 - Accuracy: 79.25%\n",
      "Epoch: 1. Batch 1800/4317 - Avg Loss: 0.4956 - Accuracy: 79.25%\n",
      "Epoch: 1. Batch 1810/4317 - Avg Loss: 0.4960 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1820/4317 - Avg Loss: 0.4960 - Accuracy: 79.24%\n",
      "Epoch: 1. Batch 1830/4317 - Avg Loss: 0.4966 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1840/4317 - Avg Loss: 0.4962 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 1850/4317 - Avg Loss: 0.4963 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1860/4317 - Avg Loss: 0.4959 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1870/4317 - Avg Loss: 0.4958 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 1880/4317 - Avg Loss: 0.4958 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 1890/4317 - Avg Loss: 0.4956 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 1900/4317 - Avg Loss: 0.4957 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1910/4317 - Avg Loss: 0.4960 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1920/4317 - Avg Loss: 0.4966 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1930/4317 - Avg Loss: 0.4961 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1940/4317 - Avg Loss: 0.4961 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 1950/4317 - Avg Loss: 0.4963 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 1960/4317 - Avg Loss: 0.4967 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 1970/4317 - Avg Loss: 0.4967 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 1980/4317 - Avg Loss: 0.4970 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 1990/4317 - Avg Loss: 0.4974 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2000/4317 - Avg Loss: 0.4972 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2010/4317 - Avg Loss: 0.4969 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2020/4317 - Avg Loss: 0.4973 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2030/4317 - Avg Loss: 0.4973 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2040/4317 - Avg Loss: 0.4976 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2050/4317 - Avg Loss: 0.4980 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2060/4317 - Avg Loss: 0.4982 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 2070/4317 - Avg Loss: 0.4980 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2080/4317 - Avg Loss: 0.4980 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2090/4317 - Avg Loss: 0.4983 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2100/4317 - Avg Loss: 0.4981 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2110/4317 - Avg Loss: 0.4977 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2120/4317 - Avg Loss: 0.4971 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 2130/4317 - Avg Loss: 0.4966 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2140/4317 - Avg Loss: 0.4966 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2150/4317 - Avg Loss: 0.4970 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 2160/4317 - Avg Loss: 0.4967 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 2170/4317 - Avg Loss: 0.4964 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 2180/4317 - Avg Loss: 0.4966 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2190/4317 - Avg Loss: 0.4967 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 2200/4317 - Avg Loss: 0.4965 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2210/4317 - Avg Loss: 0.4961 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 2220/4317 - Avg Loss: 0.4957 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 2230/4317 - Avg Loss: 0.4956 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 2240/4317 - Avg Loss: 0.4953 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 2250/4317 - Avg Loss: 0.4954 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2260/4317 - Avg Loss: 0.4956 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2270/4317 - Avg Loss: 0.4956 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2280/4317 - Avg Loss: 0.4955 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2290/4317 - Avg Loss: 0.4957 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 2300/4317 - Avg Loss: 0.4956 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 2310/4317 - Avg Loss: 0.4957 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 2320/4317 - Avg Loss: 0.4953 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2330/4317 - Avg Loss: 0.4958 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 2340/4317 - Avg Loss: 0.4955 - Accuracy: 79.20%\n",
      "Epoch: 1. Batch 2350/4317 - Avg Loss: 0.4953 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 2360/4317 - Avg Loss: 0.4955 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 2370/4317 - Avg Loss: 0.4950 - Accuracy: 79.22%\n",
      "Epoch: 1. Batch 2380/4317 - Avg Loss: 0.4947 - Accuracy: 79.23%\n",
      "Epoch: 1. Batch 2390/4317 - Avg Loss: 0.4945 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 2400/4317 - Avg Loss: 0.4944 - Accuracy: 79.21%\n",
      "Epoch: 1. Batch 2410/4317 - Avg Loss: 0.4947 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 2420/4317 - Avg Loss: 0.4944 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 2430/4317 - Avg Loss: 0.4944 - Accuracy: 79.19%\n",
      "Epoch: 1. Batch 2440/4317 - Avg Loss: 0.4946 - Accuracy: 79.18%\n",
      "Epoch: 1. Batch 2450/4317 - Avg Loss: 0.4947 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 2460/4317 - Avg Loss: 0.4947 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 2470/4317 - Avg Loss: 0.4945 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 2480/4317 - Avg Loss: 0.4946 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 2490/4317 - Avg Loss: 0.4952 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 2500/4317 - Avg Loss: 0.4952 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2510/4317 - Avg Loss: 0.4955 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2520/4317 - Avg Loss: 0.4954 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2530/4317 - Avg Loss: 0.4955 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2540/4317 - Avg Loss: 0.4954 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2550/4317 - Avg Loss: 0.4954 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2560/4317 - Avg Loss: 0.4955 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2570/4317 - Avg Loss: 0.4954 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2580/4317 - Avg Loss: 0.4950 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2590/4317 - Avg Loss: 0.4950 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2600/4317 - Avg Loss: 0.4947 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2610/4317 - Avg Loss: 0.4950 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2620/4317 - Avg Loss: 0.4948 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2630/4317 - Avg Loss: 0.4949 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2640/4317 - Avg Loss: 0.4950 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2650/4317 - Avg Loss: 0.4949 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 2660/4317 - Avg Loss: 0.4947 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2670/4317 - Avg Loss: 0.4947 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2680/4317 - Avg Loss: 0.4946 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2690/4317 - Avg Loss: 0.4947 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2700/4317 - Avg Loss: 0.4946 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 2710/4317 - Avg Loss: 0.4949 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2720/4317 - Avg Loss: 0.4949 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2730/4317 - Avg Loss: 0.4948 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2740/4317 - Avg Loss: 0.4949 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2750/4317 - Avg Loss: 0.4953 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 2760/4317 - Avg Loss: 0.4954 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 2770/4317 - Avg Loss: 0.4957 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 2780/4317 - Avg Loss: 0.4956 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 2790/4317 - Avg Loss: 0.4955 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 2800/4317 - Avg Loss: 0.4953 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2810/4317 - Avg Loss: 0.4952 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2820/4317 - Avg Loss: 0.4953 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 2830/4317 - Avg Loss: 0.4951 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 2840/4317 - Avg Loss: 0.4952 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 2850/4317 - Avg Loss: 0.4951 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 2860/4317 - Avg Loss: 0.4951 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 2870/4317 - Avg Loss: 0.4952 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 2880/4317 - Avg Loss: 0.4952 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2890/4317 - Avg Loss: 0.4950 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2900/4317 - Avg Loss: 0.4950 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2910/4317 - Avg Loss: 0.4951 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 2920/4317 - Avg Loss: 0.4951 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 2930/4317 - Avg Loss: 0.4948 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 2940/4317 - Avg Loss: 0.4946 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 2950/4317 - Avg Loss: 0.4947 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 2960/4317 - Avg Loss: 0.4943 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2970/4317 - Avg Loss: 0.4943 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 2980/4317 - Avg Loss: 0.4945 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 2990/4317 - Avg Loss: 0.4944 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3000/4317 - Avg Loss: 0.4943 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3010/4317 - Avg Loss: 0.4942 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3020/4317 - Avg Loss: 0.4940 - Accuracy: 79.17%\n",
      "Epoch: 1. Batch 3030/4317 - Avg Loss: 0.4940 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 3040/4317 - Avg Loss: 0.4943 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 3050/4317 - Avg Loss: 0.4944 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 3060/4317 - Avg Loss: 0.4944 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 3070/4317 - Avg Loss: 0.4942 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3080/4317 - Avg Loss: 0.4941 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3090/4317 - Avg Loss: 0.4940 - Accuracy: 79.16%\n",
      "Epoch: 1. Batch 3100/4317 - Avg Loss: 0.4941 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3110/4317 - Avg Loss: 0.4941 - Accuracy: 79.15%\n",
      "Epoch: 1. Batch 3120/4317 - Avg Loss: 0.4943 - Accuracy: 79.14%\n",
      "Epoch: 1. Batch 3130/4317 - Avg Loss: 0.4945 - Accuracy: 79.13%\n",
      "Epoch: 1. Batch 3140/4317 - Avg Loss: 0.4946 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3150/4317 - Avg Loss: 0.4947 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3160/4317 - Avg Loss: 0.4949 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3170/4317 - Avg Loss: 0.4947 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3180/4317 - Avg Loss: 0.4945 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3190/4317 - Avg Loss: 0.4945 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3200/4317 - Avg Loss: 0.4944 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3210/4317 - Avg Loss: 0.4945 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3220/4317 - Avg Loss: 0.4946 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 3230/4317 - Avg Loss: 0.4946 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 3240/4317 - Avg Loss: 0.4946 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3250/4317 - Avg Loss: 0.4944 - Accuracy: 79.12%\n",
      "Epoch: 1. Batch 3260/4317 - Avg Loss: 0.4945 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 3270/4317 - Avg Loss: 0.4943 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3280/4317 - Avg Loss: 0.4945 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 3290/4317 - Avg Loss: 0.4947 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 3300/4317 - Avg Loss: 0.4946 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 3310/4317 - Avg Loss: 0.4945 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 3320/4317 - Avg Loss: 0.4942 - Accuracy: 79.11%\n",
      "Epoch: 1. Batch 3330/4317 - Avg Loss: 0.4944 - Accuracy: 79.10%\n",
      "Epoch: 1. Batch 3340/4317 - Avg Loss: 0.4944 - Accuracy: 79.09%\n",
      "Epoch: 1. Batch 3350/4317 - Avg Loss: 0.4948 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3360/4317 - Avg Loss: 0.4948 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3370/4317 - Avg Loss: 0.4949 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3380/4317 - Avg Loss: 0.4949 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3390/4317 - Avg Loss: 0.4949 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3400/4317 - Avg Loss: 0.4950 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3410/4317 - Avg Loss: 0.4949 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3420/4317 - Avg Loss: 0.4946 - Accuracy: 79.08%\n",
      "Epoch: 1. Batch 3430/4317 - Avg Loss: 0.4948 - Accuracy: 79.07%\n",
      "Epoch: 1. Batch 3440/4317 - Avg Loss: 0.4948 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3450/4317 - Avg Loss: 0.4949 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3460/4317 - Avg Loss: 0.4949 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3470/4317 - Avg Loss: 0.4949 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3480/4317 - Avg Loss: 0.4949 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3490/4317 - Avg Loss: 0.4951 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3500/4317 - Avg Loss: 0.4951 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 3510/4317 - Avg Loss: 0.4953 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3520/4317 - Avg Loss: 0.4951 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3530/4317 - Avg Loss: 0.4951 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3540/4317 - Avg Loss: 0.4950 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3550/4317 - Avg Loss: 0.4949 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3560/4317 - Avg Loss: 0.4953 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3570/4317 - Avg Loss: 0.4952 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3580/4317 - Avg Loss: 0.4951 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3590/4317 - Avg Loss: 0.4953 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3600/4317 - Avg Loss: 0.4952 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3610/4317 - Avg Loss: 0.4952 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3620/4317 - Avg Loss: 0.4951 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3630/4317 - Avg Loss: 0.4951 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 3640/4317 - Avg Loss: 0.4948 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3650/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3660/4317 - Avg Loss: 0.4950 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3670/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3680/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3690/4317 - Avg Loss: 0.4948 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3700/4317 - Avg Loss: 0.4949 - Accuracy: 79.06%\n",
      "Epoch: 1. Batch 3710/4317 - Avg Loss: 0.4951 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3720/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3730/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3740/4317 - Avg Loss: 0.4950 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3750/4317 - Avg Loss: 0.4951 - Accuracy: 79.05%\n",
      "Epoch: 1. Batch 3760/4317 - Avg Loss: 0.4952 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 3770/4317 - Avg Loss: 0.4952 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 3780/4317 - Avg Loss: 0.4955 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3790/4317 - Avg Loss: 0.4954 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3800/4317 - Avg Loss: 0.4956 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3810/4317 - Avg Loss: 0.4955 - Accuracy: 79.03%\n",
      "Epoch: 1. Batch 3820/4317 - Avg Loss: 0.4954 - Accuracy: 79.04%\n",
      "Epoch: 1. Batch 3830/4317 - Avg Loss: 0.4958 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3840/4317 - Avg Loss: 0.4961 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3850/4317 - Avg Loss: 0.4961 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 3860/4317 - Avg Loss: 0.4962 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 3870/4317 - Avg Loss: 0.4959 - Accuracy: 79.02%\n",
      "Epoch: 1. Batch 3880/4317 - Avg Loss: 0.4959 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 3890/4317 - Avg Loss: 0.4962 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3900/4317 - Avg Loss: 0.4960 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3910/4317 - Avg Loss: 0.4959 - Accuracy: 79.01%\n",
      "Epoch: 1. Batch 3920/4317 - Avg Loss: 0.4961 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 3930/4317 - Avg Loss: 0.4960 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3940/4317 - Avg Loss: 0.4959 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3950/4317 - Avg Loss: 0.4958 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3960/4317 - Avg Loss: 0.4958 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3970/4317 - Avg Loss: 0.4958 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3980/4317 - Avg Loss: 0.4960 - Accuracy: 79.00%\n",
      "Epoch: 1. Batch 3990/4317 - Avg Loss: 0.4960 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 4000/4317 - Avg Loss: 0.4961 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 4010/4317 - Avg Loss: 0.4964 - Accuracy: 78.99%\n",
      "Epoch: 1. Batch 4020/4317 - Avg Loss: 0.4964 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 4030/4317 - Avg Loss: 0.4965 - Accuracy: 78.98%\n",
      "Epoch: 1. Batch 4040/4317 - Avg Loss: 0.4968 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 4050/4317 - Avg Loss: 0.4969 - Accuracy: 78.97%\n",
      "Epoch: 1. Batch 4060/4317 - Avg Loss: 0.4973 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 4070/4317 - Avg Loss: 0.4974 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 4080/4317 - Avg Loss: 0.4975 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 4090/4317 - Avg Loss: 0.4973 - Accuracy: 78.94%\n",
      "Epoch: 1. Batch 4100/4317 - Avg Loss: 0.4976 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4110/4317 - Avg Loss: 0.4977 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4120/4317 - Avg Loss: 0.4978 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4130/4317 - Avg Loss: 0.4979 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4140/4317 - Avg Loss: 0.4979 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4150/4317 - Avg Loss: 0.4979 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4160/4317 - Avg Loss: 0.4978 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4170/4317 - Avg Loss: 0.4977 - Accuracy: 78.93%\n",
      "Epoch: 1. Batch 4180/4317 - Avg Loss: 0.4977 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4190/4317 - Avg Loss: 0.4977 - Accuracy: 78.92%\n",
      "Epoch: 1. Batch 4200/4317 - Avg Loss: 0.4976 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4210/4317 - Avg Loss: 0.4977 - Accuracy: 78.90%\n",
      "Epoch: 1. Batch 4220/4317 - Avg Loss: 0.4977 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4230/4317 - Avg Loss: 0.4975 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4240/4317 - Avg Loss: 0.4978 - Accuracy: 78.90%\n",
      "Epoch: 1. Batch 4250/4317 - Avg Loss: 0.4977 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4260/4317 - Avg Loss: 0.4976 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4270/4317 - Avg Loss: 0.4975 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4280/4317 - Avg Loss: 0.4976 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4290/4317 - Avg Loss: 0.4976 - Accuracy: 78.91%\n",
      "Epoch: 1. Batch 4300/4317 - Avg Loss: 0.4977 - Accuracy: 78.89%\n",
      "Epoch: 1. Batch 4310/4317 - Avg Loss: 0.4976 - Accuracy: 78.90%\n",
      "Train loss: 0.4974 - Train accuracy: 78.91%\n",
      "Validation accuracy: 0.7369\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7383\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 70.64%        | 73.61%              |\n",
    "| **Epoch 2** | 78.91%        | 73.69%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** remains nearly constant (~73,6%), with a slight **increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_sentiment_model_overfitted\\\\tokenizer_config.json',\n",
       " './bert_sentiment_model_overfitted\\\\special_tokens_map.json',\n",
       " './bert_sentiment_model_overfitted\\\\vocab.txt',\n",
       " './bert_sentiment_model_overfitted\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./bert_sentiment_model')\n",
    "tokenizer.save_pretrained('./bert_sentiment_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
