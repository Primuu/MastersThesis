{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: BERT (multitasking)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Summary](#summary)\n",
    "7. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e63708c56d23ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c128e945955b499b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file_sentiment = os.path.join(base_dir, 'train_sentiment.csv')\n",
    "val_file_sentiment = os.path.join(base_dir, 'val_sentiment.csv')\n",
    "test_file_sentiment = os.path.join(base_dir, 'test_sentiment.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file_sentiment), os.path.exists(val_file_sentiment), os.path.exists(test_file_sentiment)]):\n",
    "    sentiment_df = pd.read_parquet('../../data/sentiment_without_outliers/sentiment_without_outliers.parquet')\n",
    "    sentiment_df = sentiment_df.drop(columns=['text_length'])\n",
    "    \n",
    "    train_data_sentiment, temp_data = train_test_split(sentiment_df, test_size=0.3, stratify=sentiment_df['label'], random_state=42)\n",
    "    val_data_sentiment, test_data_sentiment = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data_sentiment.to_csv(train_file_sentiment, index=False)\n",
    "    val_data_sentiment.to_csv(val_file_sentiment, index=False)\n",
    "    test_data_sentiment.to_csv(test_file_sentiment, index=False)\n",
    "else:\n",
    "    train_data_sentiment = pd.read_csv(train_file_sentiment)\n",
    "    val_data_sentiment = pd.read_csv(val_file_sentiment)\n",
    "    test_data_sentiment = pd.read_csv(test_file_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_file_emotion = os.path.join(base_dir, 'train_emotion.csv')\n",
    "val_file_emotion = os.path.join(base_dir, 'val_emotion.csv')\n",
    "test_file_emotion = os.path.join(base_dir, 'test_emotion.csv')\n",
    "\n",
    "if not all([os.path.exists(train_file_emotion), os.path.exists(val_file_emotion), os.path.exists(test_file_emotion)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data_emotion, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data_emotion, test_data_emotion = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data_emotion.to_csv(train_file_emotion, index=False)\n",
    "    val_data_emotion.to_csv(val_file_emotion, index=False)\n",
    "    test_data_emotion.to_csv(test_file_emotion, index=False)\n",
    "else:\n",
    "    train_data_emotion = pd.read_csv(train_file_emotion)\n",
    "    val_data_emotion = pd.read_csv(val_file_emotion)\n",
    "    test_data_emotion = pd.read_csv(test_file_emotion)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b266f96d35a30251",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_weights_sentiment = compute_class_weight('balanced', classes=pd.unique(train_data_sentiment['label']), y=train_data_sentiment['label'])\n",
    "class_weights_sentiment = torch.tensor(class_weights_sentiment, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35e5af23c9dd76d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b87c99bfacf903",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights_emotion = compute_class_weight('balanced', classes=pd.unique(train_data_emotion['label']), y=train_data_emotion['label'])\n",
    "class_weights_emotion = torch.tensor(class_weights_emotion, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8e07052fdcbf5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.texts = data['text']\n",
    "        self.labels = data['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label_sentiment': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.texts = data['text']\n",
    "        self.labels = data['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label_emotion': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9334e1f8594ea22b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_data_loader(dataset_class, data, tokenizer, max_len, batch_size):\n",
    "    dataset = dataset_class(data, tokenizer, max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d6096a0349b97ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader_sentiment = create_data_loader(SentimentDataset, train_data_sentiment, tokenizer, max_len=256, batch_size=16)\n",
    "val_loader_sentiment = create_data_loader(SentimentDataset, val_data_sentiment, tokenizer, max_len=256, batch_size=16)\n",
    "test_loader_sentiment = create_data_loader(SentimentDataset, test_data_sentiment, tokenizer, max_len=256, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "845331b20b16b5a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader_emotion = create_data_loader(EmotionDataset, train_data_emotion, tokenizer, max_len=256, batch_size=16)\n",
    "val_loader_emotion = create_data_loader(EmotionDataset, val_data_emotion, tokenizer, max_len=256, batch_size=16)\n",
    "test_loader_emotion = create_data_loader(EmotionDataset, test_data_emotion, tokenizer, max_len=256, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7364a02ed812d800",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, num_labels_sentiment, num_labels_emotion):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier_sentiment = nn.Linear(self.bert.config.hidden_size, num_labels_sentiment)\n",
    "        self.classifier_emotion = nn.Linear(self.bert.config.hidden_size, num_labels_emotion)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task='sentiment'):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        if task == 'sentiment':\n",
    "            return self.classifier_sentiment(pooled_output)\n",
    "        else:\n",
    "            return self.classifier_emotion(pooled_output)\n",
    "\n",
    "model = MultiTaskBERT(num_labels_sentiment=3, num_labels_emotion=6).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ebe066b618b8ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn_sentiment = nn.CrossEntropyLoss(weight=class_weights_sentiment.to(device))\n",
    "loss_fn_emotion = nn.CrossEntropyLoss(weight=class_weights_emotion.to(device))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd685857c73a1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ae38a64dd38620",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(model, sentiment_loader, emotion_loader, loss_fn_sentiment, loss_fn_emotion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_sentiment = 0\n",
    "    correct_emotion = 0\n",
    "    total_sentiment_samples = 0\n",
    "    total_emotion_samples = 0\n",
    "\n",
    "    for batch_idx, (batch_sentiment, batch_emotion) in enumerate(zip(sentiment_loader, emotion_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # === === === === SENTIMENT === === === ===\n",
    "        input_ids_sent = batch_sentiment['input_ids'].to(device)\n",
    "        attention_mask_sent = batch_sentiment['attention_mask'].to(device)\n",
    "        labels_sent = batch_sentiment['label_sentiment'].to(device)\n",
    "\n",
    "        logits_sentiment = model(input_ids_sent, attention_mask_sent, task='sentiment')\n",
    "        loss_sentiment = loss_fn_sentiment(logits_sentiment, labels_sent)\n",
    "        loss_sentiment.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds_sentiment = logits_sentiment.max(1)\n",
    "        correct_sentiment += (preds_sentiment == labels_sent).sum().item()\n",
    "        total_sentiment_samples += labels_sent.size(0)\n",
    "\n",
    "        # === === === === EMOTION === === === ===\n",
    "        input_ids_emot = batch_emotion['input_ids'].to(device)\n",
    "        attention_mask_emot = batch_emotion['attention_mask'].to(device)\n",
    "        labels_emot = batch_emotion['label_emotion'].to(device)\n",
    "\n",
    "        logits_emotion = model(input_ids_emot, attention_mask_emot, task='emotion')\n",
    "        loss_emotion = loss_fn_emotion(logits_emotion, labels_emot)\n",
    "        loss_emotion.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        _, preds_emotion = logits_emotion.max(1)\n",
    "        correct_emotion += (preds_emotion == labels_emot).sum().item()\n",
    "        total_emotion_samples += labels_emot.size(0)\n",
    "\n",
    "        total_loss += loss_sentiment.item() + loss_emotion.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            acc_sent = 100. * correct_sentiment / total_sentiment_samples\n",
    "            acc_emot = 100. * correct_emotion / total_emotion_samples\n",
    "            print(f\"Epoch {epoch}. Batch {batch_idx}/{min(len(sentiment_loader), len(emotion_loader))}: \"\n",
    "                  f\"AvgLoss: {avg_loss:.2f}, S.Acc: {acc_sent:.2f}%, E.Acc: {acc_emot:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / min(len(sentiment_loader), len(emotion_loader))\n",
    "    accuracy_sentiment = 100. * correct_sentiment / total_sentiment_samples\n",
    "    accuracy_emotion = 100. * correct_emotion / total_emotion_samples\n",
    "\n",
    "    return avg_loss, accuracy_sentiment, accuracy_emotion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97fed193712cfec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_model(model, sentiment_loader, emotion_loader, device):\n",
    "    model.eval()\n",
    "    correct_sentiment = 0\n",
    "    correct_emotion = 0\n",
    "    total_sentiment_samples = 0\n",
    "    total_emotion_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_sentiment, batch_emotion in zip(sentiment_loader, emotion_loader):\n",
    "            # === === === === SENTIMENT === === === ===\n",
    "            input_ids_sent = batch_sentiment['input_ids'].to(device)\n",
    "            attention_mask_sent = batch_sentiment['attention_mask'].to(device)\n",
    "            labels_sent = batch_sentiment['label_sentiment'].to(device)\n",
    "\n",
    "            logits_sentiment = model(input_ids_sent, attention_mask_sent, task='sentiment')\n",
    "            _, preds_sentiment = torch.max(logits_sentiment, dim=1)\n",
    "            correct_sentiment += torch.sum(preds_sentiment == labels_sent).item()\n",
    "            total_sentiment_samples += labels_sent.size(0)\n",
    "\n",
    "            # === === === === EMOTION === === === ===\n",
    "            input_ids_emot = batch_emotion['input_ids'].to(device)\n",
    "            attention_mask_emot = batch_emotion['attention_mask'].to(device)\n",
    "            labels_emot = batch_emotion['label_emotion'].to(device)\n",
    "\n",
    "            logits_emotion = model(input_ids_emot, attention_mask_emot, task='emotion')\n",
    "            _, preds_emotion = torch.max(logits_emotion, dim=1)\n",
    "            correct_emotion += torch.sum(preds_emotion == labels_emot).item()\n",
    "            total_emotion_samples += labels_emot.size(0)\n",
    "\n",
    "    accuracy_sentiment = 100. * correct_sentiment / total_sentiment_samples\n",
    "    accuracy_emotion = 100. * correct_emotion / total_emotion_samples\n",
    "\n",
    "    return accuracy_sentiment, accuracy_emotion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a35a37896b4250be",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train_loss, train_acc_sent, train_acc_emot = train_epoch(\n",
    "        model, train_loader_sentiment, train_loader_emotion,\n",
    "        loss_fn_sentiment, loss_fn_emotion, optimizer, device, epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Sentiment Accuracy: {train_acc_sent:.2f}%, \"\n",
    "          f\"Train Emotion Accuracy: {train_acc_emot:.2f}%\")\n",
    "    val_acc_sent, val_acc_emot = eval_model(model, val_loader_sentiment, val_loader_emotion, device)\n",
    "    print(f\"Validation Sentiment Accuracy: {val_acc_sent:.2f}%, Validation Emotion Accuracy: {val_acc_emot:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dab5c3c55d1ef7ea"
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc_sent, test_acc_emot = eval_model(model, test_loader_sentiment, test_loader_emotion, device)\n",
    "\n",
    "print(f\"Test Sentiment Accuracy: {test_acc_sent:.2f}%\")\n",
    "print(f\"Test Emotion Accuracy: {test_acc_emot:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d0180da5ba2b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "\n",
    "| Epoch        | Train Accuracy | Validation Accuracy |\n",
    "|-------------|---------------|---------------------|\n",
    "| **Epoch 1** | 70.64%        | 73.61%              |\n",
    "| **Epoch 2** | 78.91%        | 73.69%              |\n",
    "\n",
    "### Observation\n",
    "- The **train accuracy** increases.\n",
    "- The **validation accuracy** remains nearly constant (~73,6%), with a slight **increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./bert_multitasking_model')\n",
    "tokenizer.save_pretrained('./bert_multitasking_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
