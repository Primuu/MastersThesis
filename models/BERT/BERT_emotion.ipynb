{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8eb648a8b69d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training: BERT (emotion)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data loading and splitting](#data-loading-and-splitting)\n",
    "3. [Setting training parameters](#setting-training-parameters)\n",
    "4. [Model training](#model-training)\n",
    "5. [Model evaluation](#model-evaluation)\n",
    "6. [Model serialization](#model-serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed42d0f9ae35a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e63708c56d23ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:44:21.682256Z",
     "start_time": "2025-02-12T08:44:20.500869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4221171d43bfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2234a06ab639d6d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:42:15.311050Z",
     "start_time": "2025-02-12T08:42:12.887273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = '../train_emotion.csv'\n",
    "val_file = '../val_emotion.csv'\n",
    "test_file = '../test_emotion.csv'\n",
    "\n",
    "if not all([os.path.exists(train_file), os.path.exists(val_file), os.path.exists(test_file)]):\n",
    "    emotion_df = pd.read_parquet('../../data/emotion_without_outliers/emotion_without_outliers.parquet')\n",
    "    emotion_df = emotion_df.drop(columns=['text_length'])\n",
    "    \n",
    "    target_samples_per_class = 16_667  # 100k / 6 classes of emotions\n",
    "    \n",
    "    balanced_data = emotion_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), target_samples_per_class), random_state=42)\n",
    "    )\n",
    "    \n",
    "    train_data, temp_data = train_test_split(balanced_data, test_size=0.3, stratify=balanced_data['label'], random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0931b75680ca25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:44:33.806997Z",
     "start_time": "2025-02-12T08:44:32.226489Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = train_data['label'].value_counts()\n",
    "class_labels = {0: 'Sadness', 1: 'Joy', 2: 'Love', 3: 'Anger', 4: 'Fear', 5: 'Surprise'}\n",
    "class_percentages = (class_counts / class_counts.sum()) * 100\n",
    "\n",
    "class_counts_dict = {class_labels[key]: class_counts[key] for key in class_labels.keys()}\n",
    "class_percentages_dict = {class_labels[key]: class_percentages[key] for key in class_labels.keys()}\n",
    "\n",
    "plt.bar(class_counts_dict.keys(), class_counts_dict.values(), color=['grey', 'green', 'pink', 'red', 'yellow', 'blue'])\n",
    "\n",
    "for label, count in class_counts_dict.items():\n",
    "    percentage = f'{class_percentages_dict[label]:.2f}%'\n",
    "    plt.text(list(class_counts_dict.keys()).index(label), count + 1000, percentage, ha='center')\n",
    "\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.ylim(0, class_counts.max() + 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5908e1457d91a5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setting training parameters\n",
    "\n",
    "Due to the uneven distribution of classes in the dataset, the classes will be weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b87c99bfacf903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:47:14.236217Z",
     "start_time": "2025-02-12T08:47:14.012483Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=pd.unique(train_data['label']), y=train_data['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d8e07052fdcbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:47:17.105653Z",
     "start_time": "2025-02-12T08:47:15.284686Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6b01ac62f06847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:48:09.290774Z",
     "start_time": "2025-02-12T08:48:09.281420Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.texts = data['text']\n",
    "        self.labels = data['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed48f2cf48216d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:49:13.124099Z",
     "start_time": "2025-02-12T08:49:13.117202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_loader(data, tokenizer, max_len, batch_size):\n",
    "    dataset = LabeledDataset(data, tokenizer, max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45240e9d76e8773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T08:49:23.390262Z",
     "start_time": "2025-02-12T08:49:23.382239Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = create_data_loader(train_data, tokenizer, max_len=256, batch_size=16)\n",
    "val_loader = create_data_loader(val_data, tokenizer, max_len=256, batch_size=16)\n",
    "test_loader = create_data_loader(test_data, tokenizer, max_len=256, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b1533b72004d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5501044b9a2d82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54d574d38451a7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f74f6ffa591dd5a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olga\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9719f2c2fa628513",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = outputs.logits.max(1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            accuracy = 100. * correct_predictions / total_samples\n",
    "            print(f\"Epoch: {epoch}. Batch {batch_idx}/{len(data_loader)} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd71793d5dbf352b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    return correct_predictions.double() / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a18030568e003c8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batch 0/4290 - Avg Loss: 1.8444 - Accuracy: 6.25%\n",
      "Epoch: 0. Batch 10/4290 - Avg Loss: 1.8074 - Accuracy: 13.64%\n",
      "Epoch: 0. Batch 20/4290 - Avg Loss: 1.7865 - Accuracy: 18.75%\n",
      "Epoch: 0. Batch 30/4290 - Avg Loss: 1.7861 - Accuracy: 20.77%\n",
      "Epoch: 0. Batch 40/4290 - Avg Loss: 1.7834 - Accuracy: 21.34%\n",
      "Epoch: 0. Batch 50/4290 - Avg Loss: 1.7810 - Accuracy: 21.45%\n",
      "Epoch: 0. Batch 60/4290 - Avg Loss: 1.7689 - Accuracy: 22.95%\n",
      "Epoch: 0. Batch 70/4290 - Avg Loss: 1.7532 - Accuracy: 24.21%\n",
      "Epoch: 0. Batch 80/4290 - Avg Loss: 1.7300 - Accuracy: 26.70%\n",
      "Epoch: 0. Batch 90/4290 - Avg Loss: 1.6964 - Accuracy: 28.43%\n",
      "Epoch: 0. Batch 100/4290 - Avg Loss: 1.6654 - Accuracy: 30.63%\n",
      "Epoch: 0. Batch 110/4290 - Avg Loss: 1.6320 - Accuracy: 32.83%\n",
      "Epoch: 0. Batch 120/4290 - Avg Loss: 1.5891 - Accuracy: 35.49%\n",
      "Epoch: 0. Batch 130/4290 - Avg Loss: 1.5493 - Accuracy: 37.83%\n",
      "Epoch: 0. Batch 140/4290 - Avg Loss: 1.5134 - Accuracy: 39.85%\n",
      "Epoch: 0. Batch 150/4290 - Avg Loss: 1.4745 - Accuracy: 41.97%\n",
      "Epoch: 0. Batch 160/4290 - Avg Loss: 1.4287 - Accuracy: 44.14%\n",
      "Epoch: 0. Batch 170/4290 - Avg Loss: 1.3925 - Accuracy: 45.94%\n",
      "Epoch: 0. Batch 180/4290 - Avg Loss: 1.3489 - Accuracy: 47.93%\n",
      "Epoch: 0. Batch 190/4290 - Avg Loss: 1.3101 - Accuracy: 49.57%\n",
      "Epoch: 0. Batch 200/4290 - Avg Loss: 1.2735 - Accuracy: 51.12%\n",
      "Epoch: 0. Batch 210/4290 - Avg Loss: 1.2372 - Accuracy: 52.81%\n",
      "Epoch: 0. Batch 220/4290 - Avg Loss: 1.2020 - Accuracy: 54.33%\n",
      "Epoch: 0. Batch 230/4290 - Avg Loss: 1.1703 - Accuracy: 55.71%\n",
      "Epoch: 0. Batch 240/4290 - Avg Loss: 1.1441 - Accuracy: 56.74%\n",
      "Epoch: 0. Batch 250/4290 - Avg Loss: 1.1146 - Accuracy: 57.97%\n",
      "Epoch: 0. Batch 260/4290 - Avg Loss: 1.0881 - Accuracy: 59.17%\n",
      "Epoch: 0. Batch 270/4290 - Avg Loss: 1.0619 - Accuracy: 60.33%\n",
      "Epoch: 0. Batch 280/4290 - Avg Loss: 1.0366 - Accuracy: 61.32%\n",
      "Epoch: 0. Batch 290/4290 - Avg Loss: 1.0128 - Accuracy: 62.18%\n",
      "Epoch: 0. Batch 300/4290 - Avg Loss: 0.9887 - Accuracy: 63.21%\n",
      "Epoch: 0. Batch 310/4290 - Avg Loss: 0.9701 - Accuracy: 63.99%\n",
      "Epoch: 0. Batch 320/4290 - Avg Loss: 0.9498 - Accuracy: 64.82%\n",
      "Epoch: 0. Batch 330/4290 - Avg Loss: 0.9327 - Accuracy: 65.48%\n",
      "Epoch: 0. Batch 340/4290 - Avg Loss: 0.9167 - Accuracy: 66.13%\n",
      "Epoch: 0. Batch 350/4290 - Avg Loss: 0.9005 - Accuracy: 66.81%\n",
      "Epoch: 0. Batch 360/4290 - Avg Loss: 0.8837 - Accuracy: 67.52%\n",
      "Epoch: 0. Batch 370/4290 - Avg Loss: 0.8677 - Accuracy: 68.19%\n",
      "Epoch: 0. Batch 380/4290 - Avg Loss: 0.8535 - Accuracy: 68.75%\n",
      "Epoch: 0. Batch 390/4290 - Avg Loss: 0.8409 - Accuracy: 69.25%\n",
      "Epoch: 0. Batch 400/4290 - Avg Loss: 0.8289 - Accuracy: 69.70%\n",
      "Epoch: 0. Batch 410/4290 - Avg Loss: 0.8172 - Accuracy: 70.18%\n",
      "Epoch: 0. Batch 420/4290 - Avg Loss: 0.8016 - Accuracy: 70.80%\n",
      "Epoch: 0. Batch 430/4290 - Avg Loss: 0.7903 - Accuracy: 71.23%\n",
      "Epoch: 0. Batch 440/4290 - Avg Loss: 0.7799 - Accuracy: 71.61%\n",
      "Epoch: 0. Batch 450/4290 - Avg Loss: 0.7705 - Accuracy: 72.01%\n",
      "Epoch: 0. Batch 460/4290 - Avg Loss: 0.7597 - Accuracy: 72.45%\n",
      "Epoch: 0. Batch 470/4290 - Avg Loss: 0.7505 - Accuracy: 72.84%\n",
      "Epoch: 0. Batch 480/4290 - Avg Loss: 0.7420 - Accuracy: 73.19%\n",
      "Epoch: 0. Batch 490/4290 - Avg Loss: 0.7320 - Accuracy: 73.60%\n",
      "Epoch: 0. Batch 500/4290 - Avg Loss: 0.7217 - Accuracy: 74.00%\n",
      "Epoch: 0. Batch 510/4290 - Avg Loss: 0.7158 - Accuracy: 74.22%\n",
      "Epoch: 0. Batch 520/4290 - Avg Loss: 0.7065 - Accuracy: 74.57%\n",
      "Epoch: 0. Batch 530/4290 - Avg Loss: 0.6998 - Accuracy: 74.88%\n",
      "Epoch: 0. Batch 540/4290 - Avg Loss: 0.6911 - Accuracy: 75.22%\n",
      "Epoch: 0. Batch 550/4290 - Avg Loss: 0.6842 - Accuracy: 75.50%\n",
      "Epoch: 0. Batch 560/4290 - Avg Loss: 0.6764 - Accuracy: 75.78%\n",
      "Epoch: 0. Batch 570/4290 - Avg Loss: 0.6691 - Accuracy: 76.06%\n",
      "Epoch: 0. Batch 580/4290 - Avg Loss: 0.6611 - Accuracy: 76.38%\n",
      "Epoch: 0. Batch 590/4290 - Avg Loss: 0.6549 - Accuracy: 76.61%\n",
      "Epoch: 0. Batch 600/4290 - Avg Loss: 0.6476 - Accuracy: 76.90%\n",
      "Epoch: 0. Batch 610/4290 - Avg Loss: 0.6420 - Accuracy: 77.15%\n",
      "Epoch: 0. Batch 620/4290 - Avg Loss: 0.6357 - Accuracy: 77.35%\n",
      "Epoch: 0. Batch 630/4290 - Avg Loss: 0.6296 - Accuracy: 77.55%\n",
      "Epoch: 0. Batch 640/4290 - Avg Loss: 0.6229 - Accuracy: 77.83%\n",
      "Epoch: 0. Batch 650/4290 - Avg Loss: 0.6170 - Accuracy: 78.04%\n",
      "Epoch: 0. Batch 660/4290 - Avg Loss: 0.6106 - Accuracy: 78.28%\n",
      "Epoch: 0. Batch 670/4290 - Avg Loss: 0.6066 - Accuracy: 78.42%\n",
      "Epoch: 0. Batch 680/4290 - Avg Loss: 0.6016 - Accuracy: 78.60%\n",
      "Epoch: 0. Batch 690/4290 - Avg Loss: 0.5968 - Accuracy: 78.79%\n",
      "Epoch: 0. Batch 700/4290 - Avg Loss: 0.5920 - Accuracy: 78.98%\n",
      "Epoch: 0. Batch 710/4290 - Avg Loss: 0.5869 - Accuracy: 79.16%\n",
      "Epoch: 0. Batch 720/4290 - Avg Loss: 0.5818 - Accuracy: 79.36%\n",
      "Epoch: 0. Batch 730/4290 - Avg Loss: 0.5768 - Accuracy: 79.57%\n",
      "Epoch: 0. Batch 740/4290 - Avg Loss: 0.5720 - Accuracy: 79.75%\n",
      "Epoch: 0. Batch 750/4290 - Avg Loss: 0.5669 - Accuracy: 79.94%\n",
      "Epoch: 0. Batch 760/4290 - Avg Loss: 0.5623 - Accuracy: 80.09%\n",
      "Epoch: 0. Batch 770/4290 - Avg Loss: 0.5573 - Accuracy: 80.27%\n",
      "Epoch: 0. Batch 780/4290 - Avg Loss: 0.5519 - Accuracy: 80.48%\n",
      "Epoch: 0. Batch 790/4290 - Avg Loss: 0.5481 - Accuracy: 80.63%\n",
      "Epoch: 0. Batch 800/4290 - Avg Loss: 0.5443 - Accuracy: 80.75%\n",
      "Epoch: 0. Batch 810/4290 - Avg Loss: 0.5400 - Accuracy: 80.93%\n",
      "Epoch: 0. Batch 820/4290 - Avg Loss: 0.5362 - Accuracy: 81.07%\n",
      "Epoch: 0. Batch 830/4290 - Avg Loss: 0.5320 - Accuracy: 81.20%\n",
      "Epoch: 0. Batch 840/4290 - Avg Loss: 0.5279 - Accuracy: 81.35%\n",
      "Epoch: 0. Batch 850/4290 - Avg Loss: 0.5239 - Accuracy: 81.48%\n",
      "Epoch: 0. Batch 860/4290 - Avg Loss: 0.5192 - Accuracy: 81.65%\n",
      "Epoch: 0. Batch 870/4290 - Avg Loss: 0.5164 - Accuracy: 81.75%\n",
      "Epoch: 0. Batch 880/4290 - Avg Loss: 0.5126 - Accuracy: 81.90%\n",
      "Epoch: 0. Batch 890/4290 - Avg Loss: 0.5092 - Accuracy: 82.01%\n",
      "Epoch: 0. Batch 900/4290 - Avg Loss: 0.5052 - Accuracy: 82.15%\n",
      "Epoch: 0. Batch 910/4290 - Avg Loss: 0.5025 - Accuracy: 82.25%\n",
      "Epoch: 0. Batch 920/4290 - Avg Loss: 0.4992 - Accuracy: 82.38%\n",
      "Epoch: 0. Batch 930/4290 - Avg Loss: 0.4966 - Accuracy: 82.48%\n",
      "Epoch: 0. Batch 940/4290 - Avg Loss: 0.4944 - Accuracy: 82.57%\n",
      "Epoch: 0. Batch 950/4290 - Avg Loss: 0.4918 - Accuracy: 82.65%\n",
      "Epoch: 0. Batch 960/4290 - Avg Loss: 0.4891 - Accuracy: 82.75%\n",
      "Epoch: 0. Batch 970/4290 - Avg Loss: 0.4859 - Accuracy: 82.88%\n",
      "Epoch: 0. Batch 980/4290 - Avg Loss: 0.4834 - Accuracy: 82.98%\n",
      "Epoch: 0. Batch 990/4290 - Avg Loss: 0.4799 - Accuracy: 83.10%\n",
      "Epoch: 0. Batch 1000/4290 - Avg Loss: 0.4778 - Accuracy: 83.17%\n",
      "Epoch: 0. Batch 1010/4290 - Avg Loss: 0.4749 - Accuracy: 83.27%\n",
      "Epoch: 0. Batch 1020/4290 - Avg Loss: 0.4716 - Accuracy: 83.39%\n",
      "Epoch: 0. Batch 1030/4290 - Avg Loss: 0.4690 - Accuracy: 83.49%\n",
      "Epoch: 0. Batch 1040/4290 - Avg Loss: 0.4666 - Accuracy: 83.57%\n",
      "Epoch: 0. Batch 1050/4290 - Avg Loss: 0.4644 - Accuracy: 83.61%\n",
      "Epoch: 0. Batch 1060/4290 - Avg Loss: 0.4622 - Accuracy: 83.68%\n",
      "Epoch: 0. Batch 1070/4290 - Avg Loss: 0.4597 - Accuracy: 83.76%\n",
      "Epoch: 0. Batch 1080/4290 - Avg Loss: 0.4571 - Accuracy: 83.83%\n",
      "Epoch: 0. Batch 1090/4290 - Avg Loss: 0.4550 - Accuracy: 83.90%\n",
      "Epoch: 0. Batch 1100/4290 - Avg Loss: 0.4534 - Accuracy: 83.96%\n",
      "Epoch: 0. Batch 1110/4290 - Avg Loss: 0.4508 - Accuracy: 84.05%\n",
      "Epoch: 0. Batch 1120/4290 - Avg Loss: 0.4490 - Accuracy: 84.11%\n",
      "Epoch: 0. Batch 1130/4290 - Avg Loss: 0.4466 - Accuracy: 84.19%\n",
      "Epoch: 0. Batch 1140/4290 - Avg Loss: 0.4447 - Accuracy: 84.28%\n",
      "Epoch: 0. Batch 1150/4290 - Avg Loss: 0.4417 - Accuracy: 84.39%\n",
      "Epoch: 0. Batch 1160/4290 - Avg Loss: 0.4395 - Accuracy: 84.47%\n",
      "Epoch: 0. Batch 1170/4290 - Avg Loss: 0.4378 - Accuracy: 84.52%\n",
      "Epoch: 0. Batch 1180/4290 - Avg Loss: 0.4355 - Accuracy: 84.61%\n",
      "Epoch: 0. Batch 1190/4290 - Avg Loss: 0.4327 - Accuracy: 84.70%\n",
      "Epoch: 0. Batch 1200/4290 - Avg Loss: 0.4304 - Accuracy: 84.80%\n",
      "Epoch: 0. Batch 1210/4290 - Avg Loss: 0.4287 - Accuracy: 84.86%\n",
      "Epoch: 0. Batch 1220/4290 - Avg Loss: 0.4267 - Accuracy: 84.93%\n",
      "Epoch: 0. Batch 1230/4290 - Avg Loss: 0.4252 - Accuracy: 84.97%\n",
      "Epoch: 0. Batch 1240/4290 - Avg Loss: 0.4234 - Accuracy: 85.03%\n",
      "Epoch: 0. Batch 1250/4290 - Avg Loss: 0.4219 - Accuracy: 85.08%\n",
      "Epoch: 0. Batch 1260/4290 - Avg Loss: 0.4202 - Accuracy: 85.14%\n",
      "Epoch: 0. Batch 1270/4290 - Avg Loss: 0.4180 - Accuracy: 85.21%\n",
      "Epoch: 0. Batch 1280/4290 - Avg Loss: 0.4167 - Accuracy: 85.27%\n",
      "Epoch: 0. Batch 1290/4290 - Avg Loss: 0.4149 - Accuracy: 85.34%\n",
      "Epoch: 0. Batch 1300/4290 - Avg Loss: 0.4134 - Accuracy: 85.38%\n",
      "Epoch: 0. Batch 1310/4290 - Avg Loss: 0.4116 - Accuracy: 85.45%\n",
      "Epoch: 0. Batch 1320/4290 - Avg Loss: 0.4097 - Accuracy: 85.51%\n",
      "Epoch: 0. Batch 1330/4290 - Avg Loss: 0.4078 - Accuracy: 85.58%\n",
      "Epoch: 0. Batch 1340/4290 - Avg Loss: 0.4058 - Accuracy: 85.64%\n",
      "Epoch: 0. Batch 1350/4290 - Avg Loss: 0.4039 - Accuracy: 85.71%\n",
      "Epoch: 0. Batch 1360/4290 - Avg Loss: 0.4025 - Accuracy: 85.75%\n",
      "Epoch: 0. Batch 1370/4290 - Avg Loss: 0.4009 - Accuracy: 85.81%\n",
      "Epoch: 0. Batch 1380/4290 - Avg Loss: 0.3995 - Accuracy: 85.87%\n",
      "Epoch: 0. Batch 1390/4290 - Avg Loss: 0.3976 - Accuracy: 85.94%\n",
      "Epoch: 0. Batch 1400/4290 - Avg Loss: 0.3961 - Accuracy: 85.99%\n",
      "Epoch: 0. Batch 1410/4290 - Avg Loss: 0.3940 - Accuracy: 86.06%\n",
      "Epoch: 0. Batch 1420/4290 - Avg Loss: 0.3927 - Accuracy: 86.11%\n",
      "Epoch: 0. Batch 1430/4290 - Avg Loss: 0.3911 - Accuracy: 86.17%\n",
      "Epoch: 0. Batch 1440/4290 - Avg Loss: 0.3893 - Accuracy: 86.24%\n",
      "Epoch: 0. Batch 1450/4290 - Avg Loss: 0.3886 - Accuracy: 86.28%\n",
      "Epoch: 0. Batch 1460/4290 - Avg Loss: 0.3875 - Accuracy: 86.31%\n",
      "Epoch: 0. Batch 1470/4290 - Avg Loss: 0.3861 - Accuracy: 86.36%\n",
      "Epoch: 0. Batch 1480/4290 - Avg Loss: 0.3844 - Accuracy: 86.42%\n",
      "Epoch: 0. Batch 1490/4290 - Avg Loss: 0.3829 - Accuracy: 86.47%\n",
      "Epoch: 0. Batch 1500/4290 - Avg Loss: 0.3815 - Accuracy: 86.52%\n",
      "Epoch: 0. Batch 1510/4290 - Avg Loss: 0.3803 - Accuracy: 86.56%\n",
      "Epoch: 0. Batch 1520/4290 - Avg Loss: 0.3789 - Accuracy: 86.60%\n",
      "Epoch: 0. Batch 1530/4290 - Avg Loss: 0.3773 - Accuracy: 86.65%\n",
      "Epoch: 0. Batch 1540/4290 - Avg Loss: 0.3758 - Accuracy: 86.69%\n",
      "Epoch: 0. Batch 1550/4290 - Avg Loss: 0.3745 - Accuracy: 86.72%\n",
      "Epoch: 0. Batch 1560/4290 - Avg Loss: 0.3734 - Accuracy: 86.76%\n",
      "Epoch: 0. Batch 1570/4290 - Avg Loss: 0.3721 - Accuracy: 86.80%\n",
      "Epoch: 0. Batch 1580/4290 - Avg Loss: 0.3705 - Accuracy: 86.87%\n",
      "Epoch: 0. Batch 1590/4290 - Avg Loss: 0.3693 - Accuracy: 86.91%\n",
      "Epoch: 0. Batch 1600/4290 - Avg Loss: 0.3680 - Accuracy: 86.96%\n",
      "Epoch: 0. Batch 1610/4290 - Avg Loss: 0.3665 - Accuracy: 87.00%\n",
      "Epoch: 0. Batch 1620/4290 - Avg Loss: 0.3654 - Accuracy: 87.04%\n",
      "Epoch: 0. Batch 1630/4290 - Avg Loss: 0.3643 - Accuracy: 87.08%\n",
      "Epoch: 0. Batch 1640/4290 - Avg Loss: 0.3632 - Accuracy: 87.13%\n",
      "Epoch: 0. Batch 1650/4290 - Avg Loss: 0.3621 - Accuracy: 87.17%\n",
      "Epoch: 0. Batch 1660/4290 - Avg Loss: 0.3609 - Accuracy: 87.22%\n",
      "Epoch: 0. Batch 1670/4290 - Avg Loss: 0.3597 - Accuracy: 87.26%\n",
      "Epoch: 0. Batch 1680/4290 - Avg Loss: 0.3585 - Accuracy: 87.30%\n",
      "Epoch: 0. Batch 1690/4290 - Avg Loss: 0.3575 - Accuracy: 87.34%\n",
      "Epoch: 0. Batch 1700/4290 - Avg Loss: 0.3562 - Accuracy: 87.39%\n",
      "Epoch: 0. Batch 1710/4290 - Avg Loss: 0.3554 - Accuracy: 87.42%\n",
      "Epoch: 0. Batch 1720/4290 - Avg Loss: 0.3541 - Accuracy: 87.46%\n",
      "Epoch: 0. Batch 1730/4290 - Avg Loss: 0.3531 - Accuracy: 87.50%\n",
      "Epoch: 0. Batch 1740/4290 - Avg Loss: 0.3519 - Accuracy: 87.54%\n",
      "Epoch: 0. Batch 1750/4290 - Avg Loss: 0.3506 - Accuracy: 87.57%\n",
      "Epoch: 0. Batch 1760/4290 - Avg Loss: 0.3498 - Accuracy: 87.60%\n",
      "Epoch: 0. Batch 1770/4290 - Avg Loss: 0.3487 - Accuracy: 87.64%\n",
      "Epoch: 0. Batch 1780/4290 - Avg Loss: 0.3478 - Accuracy: 87.65%\n",
      "Epoch: 0. Batch 1790/4290 - Avg Loss: 0.3466 - Accuracy: 87.70%\n",
      "Epoch: 0. Batch 1800/4290 - Avg Loss: 0.3454 - Accuracy: 87.74%\n",
      "Epoch: 0. Batch 1810/4290 - Avg Loss: 0.3442 - Accuracy: 87.77%\n",
      "Epoch: 0. Batch 1820/4290 - Avg Loss: 0.3429 - Accuracy: 87.82%\n",
      "Epoch: 0. Batch 1830/4290 - Avg Loss: 0.3420 - Accuracy: 87.85%\n",
      "Epoch: 0. Batch 1840/4290 - Avg Loss: 0.3408 - Accuracy: 87.89%\n",
      "Epoch: 0. Batch 1850/4290 - Avg Loss: 0.3398 - Accuracy: 87.93%\n",
      "Epoch: 0. Batch 1860/4290 - Avg Loss: 0.3391 - Accuracy: 87.95%\n",
      "Epoch: 0. Batch 1870/4290 - Avg Loss: 0.3388 - Accuracy: 87.96%\n",
      "Epoch: 0. Batch 1880/4290 - Avg Loss: 0.3381 - Accuracy: 87.98%\n",
      "Epoch: 0. Batch 1890/4290 - Avg Loss: 0.3368 - Accuracy: 88.03%\n",
      "Epoch: 0. Batch 1900/4290 - Avg Loss: 0.3359 - Accuracy: 88.07%\n",
      "Epoch: 0. Batch 1910/4290 - Avg Loss: 0.3349 - Accuracy: 88.10%\n",
      "Epoch: 0. Batch 1920/4290 - Avg Loss: 0.3340 - Accuracy: 88.14%\n",
      "Epoch: 0. Batch 1930/4290 - Avg Loss: 0.3331 - Accuracy: 88.18%\n",
      "Epoch: 0. Batch 1940/4290 - Avg Loss: 0.3319 - Accuracy: 88.22%\n",
      "Epoch: 0. Batch 1950/4290 - Avg Loss: 0.3317 - Accuracy: 88.23%\n",
      "Epoch: 0. Batch 1960/4290 - Avg Loss: 0.3308 - Accuracy: 88.27%\n",
      "Epoch: 0. Batch 1970/4290 - Avg Loss: 0.3299 - Accuracy: 88.29%\n",
      "Epoch: 0. Batch 1980/4290 - Avg Loss: 0.3288 - Accuracy: 88.32%\n",
      "Epoch: 0. Batch 1990/4290 - Avg Loss: 0.3279 - Accuracy: 88.34%\n",
      "Epoch: 0. Batch 2000/4290 - Avg Loss: 0.3268 - Accuracy: 88.37%\n",
      "Epoch: 0. Batch 2010/4290 - Avg Loss: 0.3258 - Accuracy: 88.41%\n",
      "Epoch: 0. Batch 2020/4290 - Avg Loss: 0.3245 - Accuracy: 88.46%\n",
      "Epoch: 0. Batch 2030/4290 - Avg Loss: 0.3237 - Accuracy: 88.49%\n",
      "Epoch: 0. Batch 2040/4290 - Avg Loss: 0.3230 - Accuracy: 88.50%\n",
      "Epoch: 0. Batch 2050/4290 - Avg Loss: 0.3221 - Accuracy: 88.54%\n",
      "Epoch: 0. Batch 2060/4290 - Avg Loss: 0.3215 - Accuracy: 88.56%\n",
      "Epoch: 0. Batch 2070/4290 - Avg Loss: 0.3206 - Accuracy: 88.59%\n",
      "Epoch: 0. Batch 2080/4290 - Avg Loss: 0.3202 - Accuracy: 88.61%\n",
      "Epoch: 0. Batch 2090/4290 - Avg Loss: 0.3194 - Accuracy: 88.63%\n",
      "Epoch: 0. Batch 2100/4290 - Avg Loss: 0.3186 - Accuracy: 88.66%\n",
      "Epoch: 0. Batch 2110/4290 - Avg Loss: 0.3182 - Accuracy: 88.66%\n",
      "Epoch: 0. Batch 2120/4290 - Avg Loss: 0.3176 - Accuracy: 88.67%\n",
      "Epoch: 0. Batch 2130/4290 - Avg Loss: 0.3168 - Accuracy: 88.69%\n",
      "Epoch: 0. Batch 2140/4290 - Avg Loss: 0.3162 - Accuracy: 88.71%\n",
      "Epoch: 0. Batch 2150/4290 - Avg Loss: 0.3153 - Accuracy: 88.73%\n",
      "Epoch: 0. Batch 2160/4290 - Avg Loss: 0.3147 - Accuracy: 88.74%\n",
      "Epoch: 0. Batch 2170/4290 - Avg Loss: 0.3141 - Accuracy: 88.76%\n",
      "Epoch: 0. Batch 2180/4290 - Avg Loss: 0.3134 - Accuracy: 88.78%\n",
      "Epoch: 0. Batch 2190/4290 - Avg Loss: 0.3129 - Accuracy: 88.79%\n",
      "Epoch: 0. Batch 2200/4290 - Avg Loss: 0.3121 - Accuracy: 88.81%\n",
      "Epoch: 0. Batch 2210/4290 - Avg Loss: 0.3112 - Accuracy: 88.84%\n",
      "Epoch: 0. Batch 2220/4290 - Avg Loss: 0.3103 - Accuracy: 88.87%\n",
      "Epoch: 0. Batch 2230/4290 - Avg Loss: 0.3093 - Accuracy: 88.91%\n",
      "Epoch: 0. Batch 2240/4290 - Avg Loss: 0.3087 - Accuracy: 88.93%\n",
      "Epoch: 0. Batch 2250/4290 - Avg Loss: 0.3082 - Accuracy: 88.94%\n",
      "Epoch: 0. Batch 2260/4290 - Avg Loss: 0.3074 - Accuracy: 88.97%\n",
      "Epoch: 0. Batch 2270/4290 - Avg Loss: 0.3066 - Accuracy: 88.99%\n",
      "Epoch: 0. Batch 2280/4290 - Avg Loss: 0.3059 - Accuracy: 89.01%\n",
      "Epoch: 0. Batch 2290/4290 - Avg Loss: 0.3054 - Accuracy: 89.02%\n",
      "Epoch: 0. Batch 2300/4290 - Avg Loss: 0.3045 - Accuracy: 89.06%\n",
      "Epoch: 0. Batch 2310/4290 - Avg Loss: 0.3039 - Accuracy: 89.08%\n",
      "Epoch: 0. Batch 2320/4290 - Avg Loss: 0.3033 - Accuracy: 89.11%\n",
      "Epoch: 0. Batch 2330/4290 - Avg Loss: 0.3025 - Accuracy: 89.14%\n",
      "Epoch: 0. Batch 2340/4290 - Avg Loss: 0.3019 - Accuracy: 89.15%\n",
      "Epoch: 0. Batch 2350/4290 - Avg Loss: 0.3011 - Accuracy: 89.18%\n",
      "Epoch: 0. Batch 2360/4290 - Avg Loss: 0.3004 - Accuracy: 89.21%\n",
      "Epoch: 0. Batch 2370/4290 - Avg Loss: 0.2997 - Accuracy: 89.23%\n",
      "Epoch: 0. Batch 2380/4290 - Avg Loss: 0.2988 - Accuracy: 89.26%\n",
      "Epoch: 0. Batch 2390/4290 - Avg Loss: 0.2982 - Accuracy: 89.28%\n",
      "Epoch: 0. Batch 2400/4290 - Avg Loss: 0.2979 - Accuracy: 89.30%\n",
      "Epoch: 0. Batch 2410/4290 - Avg Loss: 0.2973 - Accuracy: 89.31%\n",
      "Epoch: 0. Batch 2420/4290 - Avg Loss: 0.2967 - Accuracy: 89.33%\n",
      "Epoch: 0. Batch 2430/4290 - Avg Loss: 0.2962 - Accuracy: 89.34%\n",
      "Epoch: 0. Batch 2440/4290 - Avg Loss: 0.2954 - Accuracy: 89.37%\n",
      "Epoch: 0. Batch 2450/4290 - Avg Loss: 0.2948 - Accuracy: 89.39%\n",
      "Epoch: 0. Batch 2460/4290 - Avg Loss: 0.2941 - Accuracy: 89.41%\n",
      "Epoch: 0. Batch 2470/4290 - Avg Loss: 0.2937 - Accuracy: 89.42%\n",
      "Epoch: 0. Batch 2480/4290 - Avg Loss: 0.2931 - Accuracy: 89.43%\n",
      "Epoch: 0. Batch 2490/4290 - Avg Loss: 0.2926 - Accuracy: 89.46%\n",
      "Epoch: 0. Batch 2500/4290 - Avg Loss: 0.2922 - Accuracy: 89.47%\n",
      "Epoch: 0. Batch 2510/4290 - Avg Loss: 0.2915 - Accuracy: 89.49%\n",
      "Epoch: 0. Batch 2520/4290 - Avg Loss: 0.2908 - Accuracy: 89.52%\n",
      "Epoch: 0. Batch 2530/4290 - Avg Loss: 0.2903 - Accuracy: 89.53%\n",
      "Epoch: 0. Batch 2540/4290 - Avg Loss: 0.2898 - Accuracy: 89.55%\n",
      "Epoch: 0. Batch 2550/4290 - Avg Loss: 0.2891 - Accuracy: 89.58%\n",
      "Epoch: 0. Batch 2560/4290 - Avg Loss: 0.2889 - Accuracy: 89.58%\n",
      "Epoch: 0. Batch 2570/4290 - Avg Loss: 0.2880 - Accuracy: 89.61%\n",
      "Epoch: 0. Batch 2580/4290 - Avg Loss: 0.2875 - Accuracy: 89.63%\n",
      "Epoch: 0. Batch 2590/4290 - Avg Loss: 0.2869 - Accuracy: 89.65%\n",
      "Epoch: 0. Batch 2600/4290 - Avg Loss: 0.2866 - Accuracy: 89.65%\n",
      "Epoch: 0. Batch 2610/4290 - Avg Loss: 0.2862 - Accuracy: 89.66%\n",
      "Epoch: 0. Batch 2620/4290 - Avg Loss: 0.2857 - Accuracy: 89.67%\n",
      "Epoch: 0. Batch 2630/4290 - Avg Loss: 0.2851 - Accuracy: 89.69%\n",
      "Epoch: 0. Batch 2640/4290 - Avg Loss: 0.2846 - Accuracy: 89.71%\n",
      "Epoch: 0. Batch 2650/4290 - Avg Loss: 0.2839 - Accuracy: 89.73%\n",
      "Epoch: 0. Batch 2660/4290 - Avg Loss: 0.2836 - Accuracy: 89.74%\n",
      "Epoch: 0. Batch 2670/4290 - Avg Loss: 0.2828 - Accuracy: 89.77%\n",
      "Epoch: 0. Batch 2680/4290 - Avg Loss: 0.2821 - Accuracy: 89.79%\n",
      "Epoch: 0. Batch 2690/4290 - Avg Loss: 0.2818 - Accuracy: 89.80%\n",
      "Epoch: 0. Batch 2700/4290 - Avg Loss: 0.2816 - Accuracy: 89.82%\n",
      "Epoch: 0. Batch 2710/4290 - Avg Loss: 0.2811 - Accuracy: 89.84%\n",
      "Epoch: 0. Batch 2720/4290 - Avg Loss: 0.2806 - Accuracy: 89.86%\n",
      "Epoch: 0. Batch 2730/4290 - Avg Loss: 0.2803 - Accuracy: 89.87%\n",
      "Epoch: 0. Batch 2740/4290 - Avg Loss: 0.2798 - Accuracy: 89.89%\n",
      "Epoch: 0. Batch 2750/4290 - Avg Loss: 0.2790 - Accuracy: 89.92%\n",
      "Epoch: 0. Batch 2760/4290 - Avg Loss: 0.2784 - Accuracy: 89.94%\n",
      "Epoch: 0. Batch 2770/4290 - Avg Loss: 0.2779 - Accuracy: 89.95%\n",
      "Epoch: 0. Batch 2780/4290 - Avg Loss: 0.2775 - Accuracy: 89.97%\n",
      "Epoch: 0. Batch 2790/4290 - Avg Loss: 0.2768 - Accuracy: 90.00%\n",
      "Epoch: 0. Batch 2800/4290 - Avg Loss: 0.2763 - Accuracy: 90.02%\n",
      "Epoch: 0. Batch 2810/4290 - Avg Loss: 0.2756 - Accuracy: 90.05%\n",
      "Epoch: 0. Batch 2820/4290 - Avg Loss: 0.2754 - Accuracy: 90.05%\n",
      "Epoch: 0. Batch 2830/4290 - Avg Loss: 0.2750 - Accuracy: 90.07%\n",
      "Epoch: 0. Batch 2840/4290 - Avg Loss: 0.2743 - Accuracy: 90.09%\n",
      "Epoch: 0. Batch 2850/4290 - Avg Loss: 0.2739 - Accuracy: 90.10%\n",
      "Epoch: 0. Batch 2860/4290 - Avg Loss: 0.2737 - Accuracy: 90.11%\n",
      "Epoch: 0. Batch 2870/4290 - Avg Loss: 0.2735 - Accuracy: 90.11%\n",
      "Epoch: 0. Batch 2880/4290 - Avg Loss: 0.2729 - Accuracy: 90.13%\n",
      "Epoch: 0. Batch 2890/4290 - Avg Loss: 0.2727 - Accuracy: 90.13%\n",
      "Epoch: 0. Batch 2900/4290 - Avg Loss: 0.2721 - Accuracy: 90.15%\n",
      "Epoch: 0. Batch 2910/4290 - Avg Loss: 0.2718 - Accuracy: 90.16%\n",
      "Epoch: 0. Batch 2920/4290 - Avg Loss: 0.2711 - Accuracy: 90.19%\n",
      "Epoch: 0. Batch 2930/4290 - Avg Loss: 0.2706 - Accuracy: 90.21%\n",
      "Epoch: 0. Batch 2940/4290 - Avg Loss: 0.2703 - Accuracy: 90.22%\n",
      "Epoch: 0. Batch 2950/4290 - Avg Loss: 0.2700 - Accuracy: 90.22%\n",
      "Epoch: 0. Batch 2960/4290 - Avg Loss: 0.2694 - Accuracy: 90.25%\n",
      "Epoch: 0. Batch 2970/4290 - Avg Loss: 0.2690 - Accuracy: 90.26%\n",
      "Epoch: 0. Batch 2980/4290 - Avg Loss: 0.2685 - Accuracy: 90.28%\n",
      "Epoch: 0. Batch 2990/4290 - Avg Loss: 0.2680 - Accuracy: 90.30%\n",
      "Epoch: 0. Batch 3000/4290 - Avg Loss: 0.2679 - Accuracy: 90.30%\n",
      "Epoch: 0. Batch 3010/4290 - Avg Loss: 0.2676 - Accuracy: 90.31%\n",
      "Epoch: 0. Batch 3020/4290 - Avg Loss: 0.2672 - Accuracy: 90.32%\n",
      "Epoch: 0. Batch 3030/4290 - Avg Loss: 0.2669 - Accuracy: 90.32%\n",
      "Epoch: 0. Batch 3040/4290 - Avg Loss: 0.2667 - Accuracy: 90.33%\n",
      "Epoch: 0. Batch 3050/4290 - Avg Loss: 0.2662 - Accuracy: 90.35%\n",
      "Epoch: 0. Batch 3060/4290 - Avg Loss: 0.2656 - Accuracy: 90.36%\n",
      "Epoch: 0. Batch 3070/4290 - Avg Loss: 0.2651 - Accuracy: 90.38%\n",
      "Epoch: 0. Batch 3080/4290 - Avg Loss: 0.2648 - Accuracy: 90.39%\n",
      "Epoch: 0. Batch 3090/4290 - Avg Loss: 0.2644 - Accuracy: 90.40%\n",
      "Epoch: 0. Batch 3100/4290 - Avg Loss: 0.2641 - Accuracy: 90.42%\n",
      "Epoch: 0. Batch 3110/4290 - Avg Loss: 0.2636 - Accuracy: 90.43%\n",
      "Epoch: 0. Batch 3120/4290 - Avg Loss: 0.2630 - Accuracy: 90.45%\n",
      "Epoch: 0. Batch 3130/4290 - Avg Loss: 0.2626 - Accuracy: 90.47%\n",
      "Epoch: 0. Batch 3140/4290 - Avg Loss: 0.2622 - Accuracy: 90.48%\n",
      "Epoch: 0. Batch 3150/4290 - Avg Loss: 0.2619 - Accuracy: 90.48%\n",
      "Epoch: 0. Batch 3160/4290 - Avg Loss: 0.2615 - Accuracy: 90.50%\n",
      "Epoch: 0. Batch 3170/4290 - Avg Loss: 0.2612 - Accuracy: 90.50%\n",
      "Epoch: 0. Batch 3180/4290 - Avg Loss: 0.2609 - Accuracy: 90.51%\n",
      "Epoch: 0. Batch 3190/4290 - Avg Loss: 0.2603 - Accuracy: 90.53%\n",
      "Epoch: 0. Batch 3200/4290 - Avg Loss: 0.2600 - Accuracy: 90.54%\n",
      "Epoch: 0. Batch 3210/4290 - Avg Loss: 0.2599 - Accuracy: 90.55%\n",
      "Epoch: 0. Batch 3220/4290 - Avg Loss: 0.2594 - Accuracy: 90.56%\n",
      "Epoch: 0. Batch 3230/4290 - Avg Loss: 0.2590 - Accuracy: 90.57%\n",
      "Epoch: 0. Batch 3240/4290 - Avg Loss: 0.2586 - Accuracy: 90.59%\n",
      "Epoch: 0. Batch 3250/4290 - Avg Loss: 0.2583 - Accuracy: 90.60%\n",
      "Epoch: 0. Batch 3260/4290 - Avg Loss: 0.2579 - Accuracy: 90.61%\n",
      "Epoch: 0. Batch 3270/4290 - Avg Loss: 0.2576 - Accuracy: 90.61%\n",
      "Epoch: 0. Batch 3280/4290 - Avg Loss: 0.2571 - Accuracy: 90.63%\n",
      "Epoch: 0. Batch 3290/4290 - Avg Loss: 0.2567 - Accuracy: 90.64%\n",
      "Epoch: 0. Batch 3300/4290 - Avg Loss: 0.2562 - Accuracy: 90.66%\n",
      "Epoch: 0. Batch 3310/4290 - Avg Loss: 0.2558 - Accuracy: 90.67%\n",
      "Epoch: 0. Batch 3320/4290 - Avg Loss: 0.2552 - Accuracy: 90.69%\n",
      "Epoch: 0. Batch 3330/4290 - Avg Loss: 0.2548 - Accuracy: 90.71%\n",
      "Epoch: 0. Batch 3340/4290 - Avg Loss: 0.2546 - Accuracy: 90.71%\n",
      "Epoch: 0. Batch 3350/4290 - Avg Loss: 0.2544 - Accuracy: 90.72%\n",
      "Epoch: 0. Batch 3360/4290 - Avg Loss: 0.2539 - Accuracy: 90.74%\n",
      "Epoch: 0. Batch 3370/4290 - Avg Loss: 0.2535 - Accuracy: 90.75%\n",
      "Epoch: 0. Batch 3380/4290 - Avg Loss: 0.2535 - Accuracy: 90.75%\n",
      "Epoch: 0. Batch 3390/4290 - Avg Loss: 0.2531 - Accuracy: 90.76%\n",
      "Epoch: 0. Batch 3400/4290 - Avg Loss: 0.2528 - Accuracy: 90.77%\n",
      "Epoch: 0. Batch 3410/4290 - Avg Loss: 0.2525 - Accuracy: 90.78%\n",
      "Epoch: 0. Batch 3420/4290 - Avg Loss: 0.2520 - Accuracy: 90.79%\n",
      "Epoch: 0. Batch 3430/4290 - Avg Loss: 0.2515 - Accuracy: 90.81%\n",
      "Epoch: 0. Batch 3440/4290 - Avg Loss: 0.2511 - Accuracy: 90.82%\n",
      "Epoch: 0. Batch 3450/4290 - Avg Loss: 0.2508 - Accuracy: 90.83%\n",
      "Epoch: 0. Batch 3460/4290 - Avg Loss: 0.2506 - Accuracy: 90.83%\n",
      "Epoch: 0. Batch 3470/4290 - Avg Loss: 0.2502 - Accuracy: 90.84%\n",
      "Epoch: 0. Batch 3480/4290 - Avg Loss: 0.2498 - Accuracy: 90.85%\n",
      "Epoch: 0. Batch 3490/4290 - Avg Loss: 0.2495 - Accuracy: 90.86%\n",
      "Epoch: 0. Batch 3500/4290 - Avg Loss: 0.2493 - Accuracy: 90.87%\n",
      "Epoch: 0. Batch 3510/4290 - Avg Loss: 0.2489 - Accuracy: 90.87%\n",
      "Epoch: 0. Batch 3520/4290 - Avg Loss: 0.2487 - Accuracy: 90.87%\n",
      "Epoch: 0. Batch 3530/4290 - Avg Loss: 0.2483 - Accuracy: 90.88%\n",
      "Epoch: 0. Batch 3540/4290 - Avg Loss: 0.2481 - Accuracy: 90.90%\n",
      "Epoch: 0. Batch 3550/4290 - Avg Loss: 0.2479 - Accuracy: 90.90%\n",
      "Epoch: 0. Batch 3560/4290 - Avg Loss: 0.2479 - Accuracy: 90.89%\n",
      "Epoch: 0. Batch 3570/4290 - Avg Loss: 0.2475 - Accuracy: 90.90%\n",
      "Epoch: 0. Batch 3580/4290 - Avg Loss: 0.2472 - Accuracy: 90.91%\n",
      "Epoch: 0. Batch 3590/4290 - Avg Loss: 0.2468 - Accuracy: 90.93%\n",
      "Epoch: 0. Batch 3600/4290 - Avg Loss: 0.2465 - Accuracy: 90.93%\n",
      "Epoch: 0. Batch 3610/4290 - Avg Loss: 0.2462 - Accuracy: 90.94%\n",
      "Epoch: 0. Batch 3620/4290 - Avg Loss: 0.2459 - Accuracy: 90.95%\n",
      "Epoch: 0. Batch 3630/4290 - Avg Loss: 0.2458 - Accuracy: 90.96%\n",
      "Epoch: 0. Batch 3640/4290 - Avg Loss: 0.2455 - Accuracy: 90.96%\n",
      "Epoch: 0. Batch 3650/4290 - Avg Loss: 0.2453 - Accuracy: 90.96%\n",
      "Epoch: 0. Batch 3660/4290 - Avg Loss: 0.2449 - Accuracy: 90.98%\n",
      "Epoch: 0. Batch 3670/4290 - Avg Loss: 0.2447 - Accuracy: 90.98%\n",
      "Epoch: 0. Batch 3680/4290 - Avg Loss: 0.2443 - Accuracy: 91.00%\n",
      "Epoch: 0. Batch 3690/4290 - Avg Loss: 0.2440 - Accuracy: 91.01%\n",
      "Epoch: 0. Batch 3700/4290 - Avg Loss: 0.2439 - Accuracy: 91.01%\n",
      "Epoch: 0. Batch 3710/4290 - Avg Loss: 0.2436 - Accuracy: 91.02%\n",
      "Epoch: 0. Batch 3720/4290 - Avg Loss: 0.2434 - Accuracy: 91.03%\n",
      "Epoch: 0. Batch 3730/4290 - Avg Loss: 0.2430 - Accuracy: 91.04%\n",
      "Epoch: 0. Batch 3740/4290 - Avg Loss: 0.2425 - Accuracy: 91.06%\n",
      "Epoch: 0. Batch 3750/4290 - Avg Loss: 0.2422 - Accuracy: 91.08%\n",
      "Epoch: 0. Batch 3760/4290 - Avg Loss: 0.2419 - Accuracy: 91.09%\n",
      "Epoch: 0. Batch 3770/4290 - Avg Loss: 0.2416 - Accuracy: 91.10%\n",
      "Epoch: 0. Batch 3780/4290 - Avg Loss: 0.2413 - Accuracy: 91.11%\n",
      "Epoch: 0. Batch 3790/4290 - Avg Loss: 0.2411 - Accuracy: 91.11%\n",
      "Epoch: 0. Batch 3800/4290 - Avg Loss: 0.2408 - Accuracy: 91.12%\n",
      "Epoch: 0. Batch 3810/4290 - Avg Loss: 0.2405 - Accuracy: 91.13%\n",
      "Epoch: 0. Batch 3820/4290 - Avg Loss: 0.2401 - Accuracy: 91.15%\n",
      "Epoch: 0. Batch 3830/4290 - Avg Loss: 0.2399 - Accuracy: 91.16%\n",
      "Epoch: 0. Batch 3840/4290 - Avg Loss: 0.2396 - Accuracy: 91.17%\n",
      "Epoch: 0. Batch 3850/4290 - Avg Loss: 0.2393 - Accuracy: 91.18%\n",
      "Epoch: 0. Batch 3860/4290 - Avg Loss: 0.2389 - Accuracy: 91.19%\n",
      "Epoch: 0. Batch 3870/4290 - Avg Loss: 0.2386 - Accuracy: 91.20%\n",
      "Epoch: 0. Batch 3880/4290 - Avg Loss: 0.2383 - Accuracy: 91.20%\n",
      "Epoch: 0. Batch 3890/4290 - Avg Loss: 0.2380 - Accuracy: 91.21%\n",
      "Epoch: 0. Batch 3900/4290 - Avg Loss: 0.2378 - Accuracy: 91.21%\n",
      "Epoch: 0. Batch 3910/4290 - Avg Loss: 0.2375 - Accuracy: 91.22%\n",
      "Epoch: 0. Batch 3920/4290 - Avg Loss: 0.2372 - Accuracy: 91.23%\n",
      "Epoch: 0. Batch 3930/4290 - Avg Loss: 0.2370 - Accuracy: 91.24%\n",
      "Epoch: 0. Batch 3940/4290 - Avg Loss: 0.2366 - Accuracy: 91.25%\n",
      "Epoch: 0. Batch 3950/4290 - Avg Loss: 0.2362 - Accuracy: 91.26%\n",
      "Epoch: 0. Batch 3960/4290 - Avg Loss: 0.2360 - Accuracy: 91.27%\n",
      "Epoch: 0. Batch 3970/4290 - Avg Loss: 0.2358 - Accuracy: 91.27%\n",
      "Epoch: 0. Batch 3980/4290 - Avg Loss: 0.2356 - Accuracy: 91.28%\n",
      "Epoch: 0. Batch 3990/4290 - Avg Loss: 0.2354 - Accuracy: 91.28%\n",
      "Epoch: 0. Batch 4000/4290 - Avg Loss: 0.2351 - Accuracy: 91.30%\n",
      "Epoch: 0. Batch 4010/4290 - Avg Loss: 0.2349 - Accuracy: 91.30%\n",
      "Epoch: 0. Batch 4020/4290 - Avg Loss: 0.2347 - Accuracy: 91.30%\n",
      "Epoch: 0. Batch 4030/4290 - Avg Loss: 0.2345 - Accuracy: 91.32%\n",
      "Epoch: 0. Batch 4040/4290 - Avg Loss: 0.2342 - Accuracy: 91.32%\n",
      "Epoch: 0. Batch 4050/4290 - Avg Loss: 0.2339 - Accuracy: 91.33%\n",
      "Epoch: 0. Batch 4060/4290 - Avg Loss: 0.2338 - Accuracy: 91.33%\n",
      "Epoch: 0. Batch 4070/4290 - Avg Loss: 0.2334 - Accuracy: 91.35%\n",
      "Epoch: 0. Batch 4080/4290 - Avg Loss: 0.2331 - Accuracy: 91.36%\n",
      "Epoch: 0. Batch 4090/4290 - Avg Loss: 0.2330 - Accuracy: 91.36%\n",
      "Epoch: 0. Batch 4100/4290 - Avg Loss: 0.2329 - Accuracy: 91.36%\n",
      "Epoch: 0. Batch 4110/4290 - Avg Loss: 0.2327 - Accuracy: 91.36%\n",
      "Epoch: 0. Batch 4120/4290 - Avg Loss: 0.2325 - Accuracy: 91.36%\n",
      "Epoch: 0. Batch 4130/4290 - Avg Loss: 0.2322 - Accuracy: 91.37%\n",
      "Epoch: 0. Batch 4140/4290 - Avg Loss: 0.2320 - Accuracy: 91.38%\n",
      "Epoch: 0. Batch 4150/4290 - Avg Loss: 0.2318 - Accuracy: 91.39%\n",
      "Epoch: 0. Batch 4160/4290 - Avg Loss: 0.2316 - Accuracy: 91.39%\n",
      "Epoch: 0. Batch 4170/4290 - Avg Loss: 0.2314 - Accuracy: 91.40%\n",
      "Epoch: 0. Batch 4180/4290 - Avg Loss: 0.2312 - Accuracy: 91.40%\n",
      "Epoch: 0. Batch 4190/4290 - Avg Loss: 0.2309 - Accuracy: 91.41%\n",
      "Epoch: 0. Batch 4200/4290 - Avg Loss: 0.2308 - Accuracy: 91.41%\n",
      "Epoch: 0. Batch 4210/4290 - Avg Loss: 0.2305 - Accuracy: 91.42%\n",
      "Epoch: 0. Batch 4220/4290 - Avg Loss: 0.2302 - Accuracy: 91.43%\n",
      "Epoch: 0. Batch 4230/4290 - Avg Loss: 0.2301 - Accuracy: 91.44%\n",
      "Epoch: 0. Batch 4240/4290 - Avg Loss: 0.2299 - Accuracy: 91.44%\n",
      "Epoch: 0. Batch 4250/4290 - Avg Loss: 0.2297 - Accuracy: 91.45%\n",
      "Epoch: 0. Batch 4260/4290 - Avg Loss: 0.2294 - Accuracy: 91.46%\n",
      "Epoch: 0. Batch 4270/4290 - Avg Loss: 0.2291 - Accuracy: 91.46%\n",
      "Epoch: 0. Batch 4280/4290 - Avg Loss: 0.2289 - Accuracy: 91.47%\n",
      "Train loss: 0.2287 - Train accuracy: 91.48%\n",
      "Validation accuracy: 0.9518\n",
      "Epoch: 1. Batch 0/4290 - Avg Loss: 0.0219 - Accuracy: 100.00%\n",
      "Epoch: 1. Batch 10/4290 - Avg Loss: 0.0828 - Accuracy: 96.59%\n",
      "Epoch: 1. Batch 20/4290 - Avg Loss: 0.1119 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 30/4290 - Avg Loss: 0.1051 - Accuracy: 95.77%\n",
      "Epoch: 1. Batch 40/4290 - Avg Loss: 0.1075 - Accuracy: 95.27%\n",
      "Epoch: 1. Batch 50/4290 - Avg Loss: 0.1023 - Accuracy: 95.59%\n",
      "Epoch: 1. Batch 60/4290 - Avg Loss: 0.1001 - Accuracy: 95.80%\n",
      "Epoch: 1. Batch 70/4290 - Avg Loss: 0.1033 - Accuracy: 95.69%\n",
      "Epoch: 1. Batch 80/4290 - Avg Loss: 0.1071 - Accuracy: 95.45%\n",
      "Epoch: 1. Batch 90/4290 - Avg Loss: 0.1104 - Accuracy: 95.19%\n",
      "Epoch: 1. Batch 100/4290 - Avg Loss: 0.1100 - Accuracy: 95.30%\n",
      "Epoch: 1. Batch 110/4290 - Avg Loss: 0.1078 - Accuracy: 95.44%\n",
      "Epoch: 1. Batch 120/4290 - Avg Loss: 0.1092 - Accuracy: 95.14%\n",
      "Epoch: 1. Batch 130/4290 - Avg Loss: 0.1107 - Accuracy: 95.13%\n",
      "Epoch: 1. Batch 140/4290 - Avg Loss: 0.1118 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 150/4290 - Avg Loss: 0.1143 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 160/4290 - Avg Loss: 0.1136 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 170/4290 - Avg Loss: 0.1144 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 180/4290 - Avg Loss: 0.1124 - Accuracy: 95.06%\n",
      "Epoch: 1. Batch 190/4290 - Avg Loss: 0.1179 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 200/4290 - Avg Loss: 0.1166 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 210/4290 - Avg Loss: 0.1162 - Accuracy: 95.05%\n",
      "Epoch: 1. Batch 220/4290 - Avg Loss: 0.1149 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 230/4290 - Avg Loss: 0.1136 - Accuracy: 95.21%\n",
      "Epoch: 1. Batch 240/4290 - Avg Loss: 0.1131 - Accuracy: 95.20%\n",
      "Epoch: 1. Batch 250/4290 - Avg Loss: 0.1122 - Accuracy: 95.22%\n",
      "Epoch: 1. Batch 260/4290 - Avg Loss: 0.1139 - Accuracy: 95.14%\n",
      "Epoch: 1. Batch 270/4290 - Avg Loss: 0.1136 - Accuracy: 95.13%\n",
      "Epoch: 1. Batch 280/4290 - Avg Loss: 0.1132 - Accuracy: 95.17%\n",
      "Epoch: 1. Batch 290/4290 - Avg Loss: 0.1126 - Accuracy: 95.19%\n",
      "Epoch: 1. Batch 300/4290 - Avg Loss: 0.1134 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 310/4290 - Avg Loss: 0.1122 - Accuracy: 95.16%\n",
      "Epoch: 1. Batch 320/4290 - Avg Loss: 0.1117 - Accuracy: 95.19%\n",
      "Epoch: 1. Batch 330/4290 - Avg Loss: 0.1133 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 340/4290 - Avg Loss: 0.1150 - Accuracy: 95.05%\n",
      "Epoch: 1. Batch 350/4290 - Avg Loss: 0.1155 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 360/4290 - Avg Loss: 0.1151 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 370/4290 - Avg Loss: 0.1145 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 380/4290 - Avg Loss: 0.1133 - Accuracy: 95.18%\n",
      "Epoch: 1. Batch 390/4290 - Avg Loss: 0.1125 - Accuracy: 95.20%\n",
      "Epoch: 1. Batch 400/4290 - Avg Loss: 0.1129 - Accuracy: 95.22%\n",
      "Epoch: 1. Batch 410/4290 - Avg Loss: 0.1121 - Accuracy: 95.27%\n",
      "Epoch: 1. Batch 420/4290 - Avg Loss: 0.1126 - Accuracy: 95.26%\n",
      "Epoch: 1. Batch 430/4290 - Avg Loss: 0.1114 - Accuracy: 95.33%\n",
      "Epoch: 1. Batch 440/4290 - Avg Loss: 0.1103 - Accuracy: 95.37%\n",
      "Epoch: 1. Batch 450/4290 - Avg Loss: 0.1102 - Accuracy: 95.36%\n",
      "Epoch: 1. Batch 460/4290 - Avg Loss: 0.1123 - Accuracy: 95.28%\n",
      "Epoch: 1. Batch 470/4290 - Avg Loss: 0.1126 - Accuracy: 95.28%\n",
      "Epoch: 1. Batch 480/4290 - Avg Loss: 0.1128 - Accuracy: 95.21%\n",
      "Epoch: 1. Batch 490/4290 - Avg Loss: 0.1125 - Accuracy: 95.16%\n",
      "Epoch: 1. Batch 500/4290 - Avg Loss: 0.1120 - Accuracy: 95.16%\n",
      "Epoch: 1. Batch 510/4290 - Avg Loss: 0.1114 - Accuracy: 95.16%\n",
      "Epoch: 1. Batch 520/4290 - Avg Loss: 0.1114 - Accuracy: 95.13%\n",
      "Epoch: 1. Batch 530/4290 - Avg Loss: 0.1108 - Accuracy: 95.16%\n",
      "Epoch: 1. Batch 540/4290 - Avg Loss: 0.1111 - Accuracy: 95.14%\n",
      "Epoch: 1. Batch 550/4290 - Avg Loss: 0.1113 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 560/4290 - Avg Loss: 0.1111 - Accuracy: 95.13%\n",
      "Epoch: 1. Batch 570/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 580/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 590/4290 - Avg Loss: 0.1120 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 600/4290 - Avg Loss: 0.1128 - Accuracy: 95.05%\n",
      "Epoch: 1. Batch 610/4290 - Avg Loss: 0.1139 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 620/4290 - Avg Loss: 0.1138 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 630/4290 - Avg Loss: 0.1134 - Accuracy: 95.06%\n",
      "Epoch: 1. Batch 640/4290 - Avg Loss: 0.1134 - Accuracy: 95.05%\n",
      "Epoch: 1. Batch 650/4290 - Avg Loss: 0.1138 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 660/4290 - Avg Loss: 0.1134 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 670/4290 - Avg Loss: 0.1131 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 680/4290 - Avg Loss: 0.1151 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 690/4290 - Avg Loss: 0.1146 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 700/4290 - Avg Loss: 0.1148 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 710/4290 - Avg Loss: 0.1139 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 720/4290 - Avg Loss: 0.1146 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 730/4290 - Avg Loss: 0.1142 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 740/4290 - Avg Loss: 0.1142 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 750/4290 - Avg Loss: 0.1139 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 760/4290 - Avg Loss: 0.1136 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 770/4290 - Avg Loss: 0.1136 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 780/4290 - Avg Loss: 0.1133 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 790/4290 - Avg Loss: 0.1134 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 800/4290 - Avg Loss: 0.1135 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 810/4290 - Avg Loss: 0.1138 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 820/4290 - Avg Loss: 0.1136 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 830/4290 - Avg Loss: 0.1135 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 840/4290 - Avg Loss: 0.1135 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 850/4290 - Avg Loss: 0.1139 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 860/4290 - Avg Loss: 0.1140 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 870/4290 - Avg Loss: 0.1137 - Accuracy: 95.05%\n",
      "Epoch: 1. Batch 880/4290 - Avg Loss: 0.1133 - Accuracy: 95.09%\n",
      "Epoch: 1. Batch 890/4290 - Avg Loss: 0.1135 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 900/4290 - Avg Loss: 0.1135 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 910/4290 - Avg Loss: 0.1135 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 920/4290 - Avg Loss: 0.1137 - Accuracy: 95.09%\n",
      "Epoch: 1. Batch 930/4290 - Avg Loss: 0.1133 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 940/4290 - Avg Loss: 0.1136 - Accuracy: 95.13%\n",
      "Epoch: 1. Batch 950/4290 - Avg Loss: 0.1142 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 960/4290 - Avg Loss: 0.1146 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 970/4290 - Avg Loss: 0.1141 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 980/4290 - Avg Loss: 0.1142 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 990/4290 - Avg Loss: 0.1138 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 1000/4290 - Avg Loss: 0.1141 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 1010/4290 - Avg Loss: 0.1142 - Accuracy: 95.10%\n",
      "Epoch: 1. Batch 1020/4290 - Avg Loss: 0.1142 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 1030/4290 - Avg Loss: 0.1148 - Accuracy: 95.09%\n",
      "Epoch: 1. Batch 1040/4290 - Avg Loss: 0.1147 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1050/4290 - Avg Loss: 0.1149 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1060/4290 - Avg Loss: 0.1145 - Accuracy: 95.12%\n",
      "Epoch: 1. Batch 1070/4290 - Avg Loss: 0.1147 - Accuracy: 95.11%\n",
      "Epoch: 1. Batch 1080/4290 - Avg Loss: 0.1150 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1090/4290 - Avg Loss: 0.1149 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1100/4290 - Avg Loss: 0.1151 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 1110/4290 - Avg Loss: 0.1150 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 1120/4290 - Avg Loss: 0.1152 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 1130/4290 - Avg Loss: 0.1153 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1140/4290 - Avg Loss: 0.1155 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 1150/4290 - Avg Loss: 0.1153 - Accuracy: 95.08%\n",
      "Epoch: 1. Batch 1160/4290 - Avg Loss: 0.1153 - Accuracy: 95.07%\n",
      "Epoch: 1. Batch 1170/4290 - Avg Loss: 0.1158 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 1180/4290 - Avg Loss: 0.1158 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 1190/4290 - Avg Loss: 0.1157 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 1200/4290 - Avg Loss: 0.1155 - Accuracy: 95.04%\n",
      "Epoch: 1. Batch 1210/4290 - Avg Loss: 0.1162 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1220/4290 - Avg Loss: 0.1166 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1230/4290 - Avg Loss: 0.1162 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 1240/4290 - Avg Loss: 0.1162 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 1250/4290 - Avg Loss: 0.1164 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1260/4290 - Avg Loss: 0.1166 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1270/4290 - Avg Loss: 0.1166 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1280/4290 - Avg Loss: 0.1170 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1290/4290 - Avg Loss: 0.1168 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1300/4290 - Avg Loss: 0.1168 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1310/4290 - Avg Loss: 0.1172 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1320/4290 - Avg Loss: 0.1169 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 1330/4290 - Avg Loss: 0.1166 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 1340/4290 - Avg Loss: 0.1167 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 1350/4290 - Avg Loss: 0.1169 - Accuracy: 95.03%\n",
      "Epoch: 1. Batch 1360/4290 - Avg Loss: 0.1173 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1370/4290 - Avg Loss: 0.1174 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1380/4290 - Avg Loss: 0.1178 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 1390/4290 - Avg Loss: 0.1178 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1400/4290 - Avg Loss: 0.1175 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1410/4290 - Avg Loss: 0.1172 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1420/4290 - Avg Loss: 0.1170 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1430/4290 - Avg Loss: 0.1174 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1440/4290 - Avg Loss: 0.1171 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1450/4290 - Avg Loss: 0.1171 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1460/4290 - Avg Loss: 0.1168 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 1470/4290 - Avg Loss: 0.1171 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1480/4290 - Avg Loss: 0.1169 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1490/4290 - Avg Loss: 0.1175 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 1500/4290 - Avg Loss: 0.1177 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 1510/4290 - Avg Loss: 0.1179 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 1520/4290 - Avg Loss: 0.1178 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 1530/4290 - Avg Loss: 0.1177 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 1540/4290 - Avg Loss: 0.1178 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 1550/4290 - Avg Loss: 0.1175 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1560/4290 - Avg Loss: 0.1172 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1570/4290 - Avg Loss: 0.1171 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1580/4290 - Avg Loss: 0.1175 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1590/4290 - Avg Loss: 0.1172 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1600/4290 - Avg Loss: 0.1173 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1610/4290 - Avg Loss: 0.1173 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1620/4290 - Avg Loss: 0.1172 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1630/4290 - Avg Loss: 0.1172 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1640/4290 - Avg Loss: 0.1170 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1650/4290 - Avg Loss: 0.1168 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1660/4290 - Avg Loss: 0.1170 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1670/4290 - Avg Loss: 0.1171 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1680/4290 - Avg Loss: 0.1169 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1690/4290 - Avg Loss: 0.1167 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1700/4290 - Avg Loss: 0.1166 - Accuracy: 95.01%\n",
      "Epoch: 1. Batch 1710/4290 - Avg Loss: 0.1165 - Accuracy: 95.02%\n",
      "Epoch: 1. Batch 1720/4290 - Avg Loss: 0.1167 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1730/4290 - Avg Loss: 0.1167 - Accuracy: 95.00%\n",
      "Epoch: 1. Batch 1740/4290 - Avg Loss: 0.1167 - Accuracy: 94.99%\n",
      "Epoch: 1. Batch 1750/4290 - Avg Loss: 0.1169 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1760/4290 - Avg Loss: 0.1172 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1770/4290 - Avg Loss: 0.1172 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 1780/4290 - Avg Loss: 0.1176 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 1790/4290 - Avg Loss: 0.1175 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 1800/4290 - Avg Loss: 0.1177 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1810/4290 - Avg Loss: 0.1177 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1820/4290 - Avg Loss: 0.1177 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1830/4290 - Avg Loss: 0.1178 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1840/4290 - Avg Loss: 0.1179 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1850/4290 - Avg Loss: 0.1178 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 1860/4290 - Avg Loss: 0.1180 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1870/4290 - Avg Loss: 0.1185 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1880/4290 - Avg Loss: 0.1185 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 1890/4290 - Avg Loss: 0.1183 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1900/4290 - Avg Loss: 0.1183 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1910/4290 - Avg Loss: 0.1182 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1920/4290 - Avg Loss: 0.1182 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1930/4290 - Avg Loss: 0.1183 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1940/4290 - Avg Loss: 0.1182 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1950/4290 - Avg Loss: 0.1182 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 1960/4290 - Avg Loss: 0.1186 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1970/4290 - Avg Loss: 0.1186 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1980/4290 - Avg Loss: 0.1184 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 1990/4290 - Avg Loss: 0.1184 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2000/4290 - Avg Loss: 0.1185 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2010/4290 - Avg Loss: 0.1185 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2020/4290 - Avg Loss: 0.1182 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2030/4290 - Avg Loss: 0.1179 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2040/4290 - Avg Loss: 0.1177 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2050/4290 - Avg Loss: 0.1179 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2060/4290 - Avg Loss: 0.1182 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2070/4290 - Avg Loss: 0.1180 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2080/4290 - Avg Loss: 0.1180 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2090/4290 - Avg Loss: 0.1179 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2100/4290 - Avg Loss: 0.1180 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2110/4290 - Avg Loss: 0.1181 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2120/4290 - Avg Loss: 0.1183 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2130/4290 - Avg Loss: 0.1183 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2140/4290 - Avg Loss: 0.1181 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2150/4290 - Avg Loss: 0.1181 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2160/4290 - Avg Loss: 0.1180 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2170/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2180/4290 - Avg Loss: 0.1181 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2190/4290 - Avg Loss: 0.1180 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2200/4290 - Avg Loss: 0.1181 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2210/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2220/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2230/4290 - Avg Loss: 0.1183 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2240/4290 - Avg Loss: 0.1184 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2250/4290 - Avg Loss: 0.1183 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2260/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2270/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2280/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2290/4290 - Avg Loss: 0.1182 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2300/4290 - Avg Loss: 0.1181 - Accuracy: 94.96%\n",
      "Epoch: 1. Batch 2310/4290 - Avg Loss: 0.1179 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2320/4290 - Avg Loss: 0.1178 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2330/4290 - Avg Loss: 0.1179 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2340/4290 - Avg Loss: 0.1180 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2350/4290 - Avg Loss: 0.1177 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2360/4290 - Avg Loss: 0.1177 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2370/4290 - Avg Loss: 0.1177 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2380/4290 - Avg Loss: 0.1176 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2390/4290 - Avg Loss: 0.1177 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2400/4290 - Avg Loss: 0.1177 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2410/4290 - Avg Loss: 0.1176 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2420/4290 - Avg Loss: 0.1176 - Accuracy: 94.98%\n",
      "Epoch: 1. Batch 2430/4290 - Avg Loss: 0.1177 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2440/4290 - Avg Loss: 0.1179 - Accuracy: 94.97%\n",
      "Epoch: 1. Batch 2450/4290 - Avg Loss: 0.1182 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2460/4290 - Avg Loss: 0.1182 - Accuracy: 94.95%\n",
      "Epoch: 1. Batch 2470/4290 - Avg Loss: 0.1184 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2480/4290 - Avg Loss: 0.1186 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2490/4290 - Avg Loss: 0.1187 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2500/4290 - Avg Loss: 0.1189 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 2510/4290 - Avg Loss: 0.1189 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 2520/4290 - Avg Loss: 0.1188 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2530/4290 - Avg Loss: 0.1187 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2540/4290 - Avg Loss: 0.1185 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2550/4290 - Avg Loss: 0.1186 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2560/4290 - Avg Loss: 0.1187 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 2570/4290 - Avg Loss: 0.1190 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2580/4290 - Avg Loss: 0.1190 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 2590/4290 - Avg Loss: 0.1193 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 2600/4290 - Avg Loss: 0.1195 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 2610/4290 - Avg Loss: 0.1195 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 2620/4290 - Avg Loss: 0.1194 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 2630/4290 - Avg Loss: 0.1195 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 2640/4290 - Avg Loss: 0.1199 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2650/4290 - Avg Loss: 0.1197 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2660/4290 - Avg Loss: 0.1198 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2670/4290 - Avg Loss: 0.1198 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2680/4290 - Avg Loss: 0.1197 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2690/4290 - Avg Loss: 0.1197 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2700/4290 - Avg Loss: 0.1197 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2710/4290 - Avg Loss: 0.1197 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 2720/4290 - Avg Loss: 0.1200 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2730/4290 - Avg Loss: 0.1201 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2740/4290 - Avg Loss: 0.1202 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2750/4290 - Avg Loss: 0.1201 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2760/4290 - Avg Loss: 0.1200 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2770/4290 - Avg Loss: 0.1200 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2780/4290 - Avg Loss: 0.1200 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2790/4290 - Avg Loss: 0.1200 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2800/4290 - Avg Loss: 0.1199 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2810/4290 - Avg Loss: 0.1199 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2820/4290 - Avg Loss: 0.1199 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2830/4290 - Avg Loss: 0.1199 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2840/4290 - Avg Loss: 0.1198 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 2850/4290 - Avg Loss: 0.1198 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2860/4290 - Avg Loss: 0.1199 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2870/4290 - Avg Loss: 0.1199 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2880/4290 - Avg Loss: 0.1199 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2890/4290 - Avg Loss: 0.1198 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2900/4290 - Avg Loss: 0.1198 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2910/4290 - Avg Loss: 0.1198 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2920/4290 - Avg Loss: 0.1197 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2930/4290 - Avg Loss: 0.1197 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2940/4290 - Avg Loss: 0.1196 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2950/4290 - Avg Loss: 0.1196 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 2960/4290 - Avg Loss: 0.1197 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2970/4290 - Avg Loss: 0.1196 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 2980/4290 - Avg Loss: 0.1197 - Accuracy: 94.85%\n",
      "Epoch: 1. Batch 2990/4290 - Avg Loss: 0.1197 - Accuracy: 94.85%\n",
      "Epoch: 1. Batch 3000/4290 - Avg Loss: 0.1197 - Accuracy: 94.85%\n",
      "Epoch: 1. Batch 3010/4290 - Avg Loss: 0.1197 - Accuracy: 94.85%\n",
      "Epoch: 1. Batch 3020/4290 - Avg Loss: 0.1197 - Accuracy: 94.85%\n",
      "Epoch: 1. Batch 3030/4290 - Avg Loss: 0.1195 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 3040/4290 - Avg Loss: 0.1196 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 3050/4290 - Avg Loss: 0.1195 - Accuracy: 94.86%\n",
      "Epoch: 1. Batch 3060/4290 - Avg Loss: 0.1194 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3070/4290 - Avg Loss: 0.1194 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3080/4290 - Avg Loss: 0.1193 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3090/4290 - Avg Loss: 0.1192 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3100/4290 - Avg Loss: 0.1192 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3110/4290 - Avg Loss: 0.1193 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3120/4290 - Avg Loss: 0.1192 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3130/4290 - Avg Loss: 0.1193 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3140/4290 - Avg Loss: 0.1192 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3150/4290 - Avg Loss: 0.1193 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3160/4290 - Avg Loss: 0.1191 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3170/4290 - Avg Loss: 0.1193 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3180/4290 - Avg Loss: 0.1195 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3190/4290 - Avg Loss: 0.1195 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3200/4290 - Avg Loss: 0.1196 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3210/4290 - Avg Loss: 0.1194 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3220/4290 - Avg Loss: 0.1193 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3230/4290 - Avg Loss: 0.1193 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3240/4290 - Avg Loss: 0.1192 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3250/4290 - Avg Loss: 0.1191 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3260/4290 - Avg Loss: 0.1192 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3270/4290 - Avg Loss: 0.1191 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3280/4290 - Avg Loss: 0.1190 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3290/4290 - Avg Loss: 0.1190 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3300/4290 - Avg Loss: 0.1189 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3310/4290 - Avg Loss: 0.1187 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 3320/4290 - Avg Loss: 0.1188 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3330/4290 - Avg Loss: 0.1188 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3340/4290 - Avg Loss: 0.1187 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3350/4290 - Avg Loss: 0.1188 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3360/4290 - Avg Loss: 0.1189 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3370/4290 - Avg Loss: 0.1188 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3380/4290 - Avg Loss: 0.1189 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3390/4290 - Avg Loss: 0.1189 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3400/4290 - Avg Loss: 0.1188 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3410/4290 - Avg Loss: 0.1188 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3420/4290 - Avg Loss: 0.1187 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3430/4290 - Avg Loss: 0.1188 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3440/4290 - Avg Loss: 0.1187 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3450/4290 - Avg Loss: 0.1187 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3460/4290 - Avg Loss: 0.1187 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3470/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3480/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3490/4290 - Avg Loss: 0.1184 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 3500/4290 - Avg Loss: 0.1184 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 3510/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3520/4290 - Avg Loss: 0.1184 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 3530/4290 - Avg Loss: 0.1184 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 3540/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3550/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3560/4290 - Avg Loss: 0.1185 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 3570/4290 - Avg Loss: 0.1185 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3580/4290 - Avg Loss: 0.1188 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3590/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3600/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3610/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3620/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3630/4290 - Avg Loss: 0.1187 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3640/4290 - Avg Loss: 0.1187 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3650/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3660/4290 - Avg Loss: 0.1187 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3670/4290 - Avg Loss: 0.1188 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3680/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3690/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3700/4290 - Avg Loss: 0.1187 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3710/4290 - Avg Loss: 0.1187 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3720/4290 - Avg Loss: 0.1188 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3730/4290 - Avg Loss: 0.1191 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3740/4290 - Avg Loss: 0.1191 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3750/4290 - Avg Loss: 0.1190 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3760/4290 - Avg Loss: 0.1190 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3770/4290 - Avg Loss: 0.1190 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3780/4290 - Avg Loss: 0.1190 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3790/4290 - Avg Loss: 0.1189 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3800/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3810/4290 - Avg Loss: 0.1188 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3820/4290 - Avg Loss: 0.1187 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3830/4290 - Avg Loss: 0.1189 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3840/4290 - Avg Loss: 0.1189 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3850/4290 - Avg Loss: 0.1190 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3860/4290 - Avg Loss: 0.1189 - Accuracy: 94.87%\n",
      "Epoch: 1. Batch 3870/4290 - Avg Loss: 0.1189 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3880/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3890/4290 - Avg Loss: 0.1188 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3900/4290 - Avg Loss: 0.1186 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3910/4290 - Avg Loss: 0.1185 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3920/4290 - Avg Loss: 0.1185 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3930/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3940/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3950/4290 - Avg Loss: 0.1183 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 3960/4290 - Avg Loss: 0.1183 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3970/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 3980/4290 - Avg Loss: 0.1185 - Accuracy: 94.88%\n",
      "Epoch: 1. Batch 3990/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 4000/4290 - Avg Loss: 0.1183 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 4010/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 4020/4290 - Avg Loss: 0.1184 - Accuracy: 94.89%\n",
      "Epoch: 1. Batch 4030/4290 - Avg Loss: 0.1183 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 4040/4290 - Avg Loss: 0.1182 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 4050/4290 - Avg Loss: 0.1182 - Accuracy: 94.90%\n",
      "Epoch: 1. Batch 4060/4290 - Avg Loss: 0.1182 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 4070/4290 - Avg Loss: 0.1181 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 4080/4290 - Avg Loss: 0.1181 - Accuracy: 94.91%\n",
      "Epoch: 1. Batch 4090/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4100/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4110/4290 - Avg Loss: 0.1181 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4120/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4130/4290 - Avg Loss: 0.1179 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4140/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4150/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4160/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4170/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4180/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4190/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4200/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4210/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4220/4290 - Avg Loss: 0.1180 - Accuracy: 94.92%\n",
      "Epoch: 1. Batch 4230/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4240/4290 - Avg Loss: 0.1179 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4250/4290 - Avg Loss: 0.1178 - Accuracy: 94.93%\n",
      "Epoch: 1. Batch 4260/4290 - Avg Loss: 0.1177 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 4270/4290 - Avg Loss: 0.1178 - Accuracy: 94.94%\n",
      "Epoch: 1. Batch 4280/4290 - Avg Loss: 0.1178 - Accuracy: 94.94%\n",
      "Train loss: 0.1178 - Train accuracy: 94.94%\n",
      "Validation accuracy: 0.9524\n",
      "Epoch: 2. Batch 0/4290 - Avg Loss: 0.1201 - Accuracy: 93.75%\n",
      "Epoch: 2. Batch 10/4290 - Avg Loss: 0.0923 - Accuracy: 94.32%\n",
      "Epoch: 2. Batch 20/4290 - Avg Loss: 0.0898 - Accuracy: 95.54%\n",
      "Epoch: 2. Batch 30/4290 - Avg Loss: 0.0914 - Accuracy: 95.56%\n",
      "Epoch: 2. Batch 40/4290 - Avg Loss: 0.0943 - Accuracy: 95.73%\n",
      "Epoch: 2. Batch 50/4290 - Avg Loss: 0.0925 - Accuracy: 95.83%\n",
      "Epoch: 2. Batch 60/4290 - Avg Loss: 0.0922 - Accuracy: 95.80%\n",
      "Epoch: 2. Batch 70/4290 - Avg Loss: 0.0913 - Accuracy: 95.77%\n",
      "Epoch: 2. Batch 80/4290 - Avg Loss: 0.0917 - Accuracy: 95.91%\n",
      "Epoch: 2. Batch 90/4290 - Avg Loss: 0.0967 - Accuracy: 95.67%\n",
      "Epoch: 2. Batch 100/4290 - Avg Loss: 0.0960 - Accuracy: 95.67%\n",
      "Epoch: 2. Batch 110/4290 - Avg Loss: 0.1003 - Accuracy: 95.38%\n",
      "Epoch: 2. Batch 120/4290 - Avg Loss: 0.0998 - Accuracy: 95.35%\n",
      "Epoch: 2. Batch 130/4290 - Avg Loss: 0.1004 - Accuracy: 95.47%\n",
      "Epoch: 2. Batch 140/4290 - Avg Loss: 0.1048 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 150/4290 - Avg Loss: 0.1058 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 160/4290 - Avg Loss: 0.1082 - Accuracy: 94.95%\n",
      "Epoch: 2. Batch 170/4290 - Avg Loss: 0.1086 - Accuracy: 94.96%\n",
      "Epoch: 2. Batch 180/4290 - Avg Loss: 0.1074 - Accuracy: 94.99%\n",
      "Epoch: 2. Batch 190/4290 - Avg Loss: 0.1057 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 200/4290 - Avg Loss: 0.1061 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 210/4290 - Avg Loss: 0.1046 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 220/4290 - Avg Loss: 0.1037 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 230/4290 - Avg Loss: 0.1031 - Accuracy: 95.27%\n",
      "Epoch: 2. Batch 240/4290 - Avg Loss: 0.1038 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 250/4290 - Avg Loss: 0.1031 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 260/4290 - Avg Loss: 0.1042 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 270/4290 - Avg Loss: 0.1042 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 280/4290 - Avg Loss: 0.1047 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 290/4290 - Avg Loss: 0.1045 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 300/4290 - Avg Loss: 0.1039 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 310/4290 - Avg Loss: 0.1037 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 320/4290 - Avg Loss: 0.1034 - Accuracy: 95.29%\n",
      "Epoch: 2. Batch 330/4290 - Avg Loss: 0.1016 - Accuracy: 95.39%\n",
      "Epoch: 2. Batch 340/4290 - Avg Loss: 0.1027 - Accuracy: 95.33%\n",
      "Epoch: 2. Batch 350/4290 - Avg Loss: 0.1037 - Accuracy: 95.28%\n",
      "Epoch: 2. Batch 360/4290 - Avg Loss: 0.1051 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 370/4290 - Avg Loss: 0.1046 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 380/4290 - Avg Loss: 0.1050 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 390/4290 - Avg Loss: 0.1046 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 400/4290 - Avg Loss: 0.1051 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 410/4290 - Avg Loss: 0.1052 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 420/4290 - Avg Loss: 0.1053 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 430/4290 - Avg Loss: 0.1044 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 440/4290 - Avg Loss: 0.1037 - Accuracy: 95.27%\n",
      "Epoch: 2. Batch 450/4290 - Avg Loss: 0.1029 - Accuracy: 95.32%\n",
      "Epoch: 2. Batch 460/4290 - Avg Loss: 0.1023 - Accuracy: 95.35%\n",
      "Epoch: 2. Batch 470/4290 - Avg Loss: 0.1031 - Accuracy: 95.29%\n",
      "Epoch: 2. Batch 480/4290 - Avg Loss: 0.1034 - Accuracy: 95.30%\n",
      "Epoch: 2. Batch 490/4290 - Avg Loss: 0.1033 - Accuracy: 95.30%\n",
      "Epoch: 2. Batch 500/4290 - Avg Loss: 0.1041 - Accuracy: 95.27%\n",
      "Epoch: 2. Batch 510/4290 - Avg Loss: 0.1042 - Accuracy: 95.28%\n",
      "Epoch: 2. Batch 520/4290 - Avg Loss: 0.1043 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 530/4290 - Avg Loss: 0.1045 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 540/4290 - Avg Loss: 0.1045 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 550/4290 - Avg Loss: 0.1047 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 560/4290 - Avg Loss: 0.1045 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 570/4290 - Avg Loss: 0.1047 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 580/4290 - Avg Loss: 0.1046 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 590/4290 - Avg Loss: 0.1048 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 600/4290 - Avg Loss: 0.1045 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 610/4290 - Avg Loss: 0.1043 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 620/4290 - Avg Loss: 0.1040 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 630/4290 - Avg Loss: 0.1038 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 640/4290 - Avg Loss: 0.1036 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 650/4290 - Avg Loss: 0.1041 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 660/4290 - Avg Loss: 0.1043 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 670/4290 - Avg Loss: 0.1038 - Accuracy: 95.27%\n",
      "Epoch: 2. Batch 680/4290 - Avg Loss: 0.1039 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 690/4290 - Avg Loss: 0.1037 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 700/4290 - Avg Loss: 0.1040 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 710/4290 - Avg Loss: 0.1044 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 720/4290 - Avg Loss: 0.1048 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 730/4290 - Avg Loss: 0.1050 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 740/4290 - Avg Loss: 0.1049 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 750/4290 - Avg Loss: 0.1048 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 760/4290 - Avg Loss: 0.1048 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 770/4290 - Avg Loss: 0.1052 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 780/4290 - Avg Loss: 0.1054 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 790/4290 - Avg Loss: 0.1052 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 800/4290 - Avg Loss: 0.1060 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 810/4290 - Avg Loss: 0.1057 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 820/4290 - Avg Loss: 0.1055 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 830/4290 - Avg Loss: 0.1053 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 840/4290 - Avg Loss: 0.1057 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 850/4290 - Avg Loss: 0.1060 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 860/4290 - Avg Loss: 0.1063 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 870/4290 - Avg Loss: 0.1064 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 880/4290 - Avg Loss: 0.1067 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 890/4290 - Avg Loss: 0.1066 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 900/4290 - Avg Loss: 0.1072 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 910/4290 - Avg Loss: 0.1069 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 920/4290 - Avg Loss: 0.1067 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 930/4290 - Avg Loss: 0.1063 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 940/4290 - Avg Loss: 0.1061 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 950/4290 - Avg Loss: 0.1061 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 960/4290 - Avg Loss: 0.1060 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 970/4290 - Avg Loss: 0.1057 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 980/4290 - Avg Loss: 0.1063 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 990/4290 - Avg Loss: 0.1061 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 1000/4290 - Avg Loss: 0.1059 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 1010/4290 - Avg Loss: 0.1060 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1020/4290 - Avg Loss: 0.1056 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1030/4290 - Avg Loss: 0.1059 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1040/4290 - Avg Loss: 0.1064 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1050/4290 - Avg Loss: 0.1065 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1060/4290 - Avg Loss: 0.1068 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1070/4290 - Avg Loss: 0.1069 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1080/4290 - Avg Loss: 0.1066 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1090/4290 - Avg Loss: 0.1070 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1100/4290 - Avg Loss: 0.1069 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1110/4290 - Avg Loss: 0.1075 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1120/4290 - Avg Loss: 0.1078 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1130/4290 - Avg Loss: 0.1077 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1140/4290 - Avg Loss: 0.1076 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1150/4290 - Avg Loss: 0.1075 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1160/4290 - Avg Loss: 0.1077 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 1170/4290 - Avg Loss: 0.1081 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 1180/4290 - Avg Loss: 0.1080 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 1190/4290 - Avg Loss: 0.1078 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 1200/4290 - Avg Loss: 0.1075 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1210/4290 - Avg Loss: 0.1075 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1220/4290 - Avg Loss: 0.1075 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1230/4290 - Avg Loss: 0.1080 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 1240/4290 - Avg Loss: 0.1078 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 1250/4290 - Avg Loss: 0.1074 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 1260/4290 - Avg Loss: 0.1072 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 1270/4290 - Avg Loss: 0.1073 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 1280/4290 - Avg Loss: 0.1078 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 1290/4290 - Avg Loss: 0.1079 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 1300/4290 - Avg Loss: 0.1080 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1310/4290 - Avg Loss: 0.1080 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1320/4290 - Avg Loss: 0.1077 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1330/4290 - Avg Loss: 0.1079 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1340/4290 - Avg Loss: 0.1076 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1350/4290 - Avg Loss: 0.1074 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1360/4290 - Avg Loss: 0.1071 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1370/4290 - Avg Loss: 0.1070 - Accuracy: 95.25%\n",
      "Epoch: 2. Batch 1380/4290 - Avg Loss: 0.1067 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 1390/4290 - Avg Loss: 0.1069 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 1400/4290 - Avg Loss: 0.1070 - Accuracy: 95.25%\n",
      "Epoch: 2. Batch 1410/4290 - Avg Loss: 0.1069 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 1420/4290 - Avg Loss: 0.1070 - Accuracy: 95.26%\n",
      "Epoch: 2. Batch 1430/4290 - Avg Loss: 0.1069 - Accuracy: 95.27%\n",
      "Epoch: 2. Batch 1440/4290 - Avg Loss: 0.1074 - Accuracy: 95.25%\n",
      "Epoch: 2. Batch 1450/4290 - Avg Loss: 0.1075 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1460/4290 - Avg Loss: 0.1076 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1470/4290 - Avg Loss: 0.1078 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1480/4290 - Avg Loss: 0.1079 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1490/4290 - Avg Loss: 0.1080 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1500/4290 - Avg Loss: 0.1078 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1510/4290 - Avg Loss: 0.1079 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1520/4290 - Avg Loss: 0.1081 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1530/4290 - Avg Loss: 0.1082 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1540/4290 - Avg Loss: 0.1083 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 1550/4290 - Avg Loss: 0.1085 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 1560/4290 - Avg Loss: 0.1082 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1570/4290 - Avg Loss: 0.1081 - Accuracy: 95.25%\n",
      "Epoch: 2. Batch 1580/4290 - Avg Loss: 0.1086 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1590/4290 - Avg Loss: 0.1085 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1600/4290 - Avg Loss: 0.1088 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 1610/4290 - Avg Loss: 0.1088 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 1620/4290 - Avg Loss: 0.1086 - Accuracy: 95.24%\n",
      "Epoch: 2. Batch 1630/4290 - Avg Loss: 0.1088 - Accuracy: 95.23%\n",
      "Epoch: 2. Batch 1640/4290 - Avg Loss: 0.1090 - Accuracy: 95.22%\n",
      "Epoch: 2. Batch 1650/4290 - Avg Loss: 0.1092 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1660/4290 - Avg Loss: 0.1094 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1670/4290 - Avg Loss: 0.1093 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1680/4290 - Avg Loss: 0.1092 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1690/4290 - Avg Loss: 0.1095 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1700/4290 - Avg Loss: 0.1097 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1710/4290 - Avg Loss: 0.1097 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1720/4290 - Avg Loss: 0.1097 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1730/4290 - Avg Loss: 0.1096 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1740/4290 - Avg Loss: 0.1095 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1750/4290 - Avg Loss: 0.1094 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1760/4290 - Avg Loss: 0.1096 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1770/4290 - Avg Loss: 0.1096 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1780/4290 - Avg Loss: 0.1096 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1790/4290 - Avg Loss: 0.1099 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1800/4290 - Avg Loss: 0.1100 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1810/4290 - Avg Loss: 0.1100 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1820/4290 - Avg Loss: 0.1103 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 1830/4290 - Avg Loss: 0.1101 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1840/4290 - Avg Loss: 0.1100 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1850/4290 - Avg Loss: 0.1100 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1860/4290 - Avg Loss: 0.1100 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1870/4290 - Avg Loss: 0.1102 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 1880/4290 - Avg Loss: 0.1099 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1890/4290 - Avg Loss: 0.1098 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1900/4290 - Avg Loss: 0.1100 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1910/4290 - Avg Loss: 0.1101 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1920/4290 - Avg Loss: 0.1102 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1930/4290 - Avg Loss: 0.1101 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 1940/4290 - Avg Loss: 0.1099 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1950/4290 - Avg Loss: 0.1100 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1960/4290 - Avg Loss: 0.1102 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 1970/4290 - Avg Loss: 0.1101 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1980/4290 - Avg Loss: 0.1100 - Accuracy: 95.21%\n",
      "Epoch: 2. Batch 1990/4290 - Avg Loss: 0.1105 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 2000/4290 - Avg Loss: 0.1106 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 2010/4290 - Avg Loss: 0.1106 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 2020/4290 - Avg Loss: 0.1106 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 2030/4290 - Avg Loss: 0.1104 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 2040/4290 - Avg Loss: 0.1105 - Accuracy: 95.19%\n",
      "Epoch: 2. Batch 2050/4290 - Avg Loss: 0.1105 - Accuracy: 95.20%\n",
      "Epoch: 2. Batch 2060/4290 - Avg Loss: 0.1107 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 2070/4290 - Avg Loss: 0.1109 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 2080/4290 - Avg Loss: 0.1111 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2090/4290 - Avg Loss: 0.1109 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 2100/4290 - Avg Loss: 0.1109 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 2110/4290 - Avg Loss: 0.1111 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2120/4290 - Avg Loss: 0.1112 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2130/4290 - Avg Loss: 0.1110 - Accuracy: 95.18%\n",
      "Epoch: 2. Batch 2140/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2150/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2160/4290 - Avg Loss: 0.1116 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2170/4290 - Avg Loss: 0.1118 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2180/4290 - Avg Loss: 0.1118 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2190/4290 - Avg Loss: 0.1119 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2200/4290 - Avg Loss: 0.1121 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2210/4290 - Avg Loss: 0.1120 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2220/4290 - Avg Loss: 0.1120 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 2230/4290 - Avg Loss: 0.1120 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2240/4290 - Avg Loss: 0.1120 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2250/4290 - Avg Loss: 0.1120 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2260/4290 - Avg Loss: 0.1119 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2270/4290 - Avg Loss: 0.1119 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2280/4290 - Avg Loss: 0.1117 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2290/4290 - Avg Loss: 0.1119 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2300/4290 - Avg Loss: 0.1116 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2310/4290 - Avg Loss: 0.1115 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2320/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2330/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2340/4290 - Avg Loss: 0.1114 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2350/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2360/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2370/4290 - Avg Loss: 0.1109 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2380/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2390/4290 - Avg Loss: 0.1111 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2400/4290 - Avg Loss: 0.1109 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2410/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2420/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2430/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2440/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2450/4290 - Avg Loss: 0.1108 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2460/4290 - Avg Loss: 0.1109 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2470/4290 - Avg Loss: 0.1109 - Accuracy: 95.17%\n",
      "Epoch: 2. Batch 2480/4290 - Avg Loss: 0.1109 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2490/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2500/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2510/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2520/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2530/4290 - Avg Loss: 0.1111 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2540/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2550/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2560/4290 - Avg Loss: 0.1110 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2570/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2580/4290 - Avg Loss: 0.1113 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2590/4290 - Avg Loss: 0.1113 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2600/4290 - Avg Loss: 0.1114 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2610/4290 - Avg Loss: 0.1114 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2620/4290 - Avg Loss: 0.1113 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2630/4290 - Avg Loss: 0.1113 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2640/4290 - Avg Loss: 0.1111 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2650/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2660/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2670/4290 - Avg Loss: 0.1111 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 2680/4290 - Avg Loss: 0.1114 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2690/4290 - Avg Loss: 0.1115 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2700/4290 - Avg Loss: 0.1116 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2710/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2720/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2730/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2740/4290 - Avg Loss: 0.1117 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2750/4290 - Avg Loss: 0.1118 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2760/4290 - Avg Loss: 0.1120 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2770/4290 - Avg Loss: 0.1119 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2780/4290 - Avg Loss: 0.1119 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2790/4290 - Avg Loss: 0.1117 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2800/4290 - Avg Loss: 0.1116 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2810/4290 - Avg Loss: 0.1117 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2820/4290 - Avg Loss: 0.1118 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2830/4290 - Avg Loss: 0.1116 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2840/4290 - Avg Loss: 0.1117 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2850/4290 - Avg Loss: 0.1117 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2860/4290 - Avg Loss: 0.1115 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2870/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2880/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2890/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2900/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2910/4290 - Avg Loss: 0.1118 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 2920/4290 - Avg Loss: 0.1116 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 2930/4290 - Avg Loss: 0.1114 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 2940/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2950/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2960/4290 - Avg Loss: 0.1113 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2970/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2980/4290 - Avg Loss: 0.1112 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 2990/4290 - Avg Loss: 0.1111 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 3000/4290 - Avg Loss: 0.1110 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 3010/4290 - Avg Loss: 0.1110 - Accuracy: 95.16%\n",
      "Epoch: 2. Batch 3020/4290 - Avg Loss: 0.1110 - Accuracy: 95.15%\n",
      "Epoch: 2. Batch 3030/4290 - Avg Loss: 0.1110 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 3040/4290 - Avg Loss: 0.1111 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 3050/4290 - Avg Loss: 0.1111 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 3060/4290 - Avg Loss: 0.1113 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 3070/4290 - Avg Loss: 0.1114 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 3080/4290 - Avg Loss: 0.1114 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3090/4290 - Avg Loss: 0.1114 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3100/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3110/4290 - Avg Loss: 0.1114 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3120/4290 - Avg Loss: 0.1113 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3130/4290 - Avg Loss: 0.1113 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3140/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3150/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3160/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3170/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3180/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3190/4290 - Avg Loss: 0.1113 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3200/4290 - Avg Loss: 0.1112 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3210/4290 - Avg Loss: 0.1110 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 3220/4290 - Avg Loss: 0.1111 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 3230/4290 - Avg Loss: 0.1113 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3240/4290 - Avg Loss: 0.1113 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 3250/4290 - Avg Loss: 0.1114 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3260/4290 - Avg Loss: 0.1116 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3270/4290 - Avg Loss: 0.1116 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3280/4290 - Avg Loss: 0.1116 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3290/4290 - Avg Loss: 0.1116 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3300/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3310/4290 - Avg Loss: 0.1118 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3320/4290 - Avg Loss: 0.1118 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3330/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3340/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3350/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3360/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3370/4290 - Avg Loss: 0.1116 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3380/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3390/4290 - Avg Loss: 0.1117 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3400/4290 - Avg Loss: 0.1116 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3410/4290 - Avg Loss: 0.1116 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3420/4290 - Avg Loss: 0.1115 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3430/4290 - Avg Loss: 0.1115 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3440/4290 - Avg Loss: 0.1114 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3450/4290 - Avg Loss: 0.1113 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3460/4290 - Avg Loss: 0.1114 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3470/4290 - Avg Loss: 0.1116 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3480/4290 - Avg Loss: 0.1118 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3490/4290 - Avg Loss: 0.1118 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3500/4290 - Avg Loss: 0.1117 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3510/4290 - Avg Loss: 0.1117 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3520/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3530/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3540/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3550/4290 - Avg Loss: 0.1116 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3560/4290 - Avg Loss: 0.1117 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3570/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3580/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3590/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3600/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3610/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3620/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3630/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3640/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3650/4290 - Avg Loss: 0.1117 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3660/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3670/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3680/4290 - Avg Loss: 0.1117 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3690/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3700/4290 - Avg Loss: 0.1117 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3710/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3720/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3730/4290 - Avg Loss: 0.1116 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3740/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3750/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3760/4290 - Avg Loss: 0.1116 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3770/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3780/4290 - Avg Loss: 0.1115 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3790/4290 - Avg Loss: 0.1117 - Accuracy: 95.06%\n",
      "Epoch: 2. Batch 3800/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3810/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3820/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3830/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3840/4290 - Avg Loss: 0.1116 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3850/4290 - Avg Loss: 0.1115 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3860/4290 - Avg Loss: 0.1115 - Accuracy: 95.07%\n",
      "Epoch: 2. Batch 3870/4290 - Avg Loss: 0.1114 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3880/4290 - Avg Loss: 0.1114 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3890/4290 - Avg Loss: 0.1114 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3900/4290 - Avg Loss: 0.1114 - Accuracy: 95.08%\n",
      "Epoch: 2. Batch 3910/4290 - Avg Loss: 0.1113 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3920/4290 - Avg Loss: 0.1112 - Accuracy: 95.09%\n",
      "Epoch: 2. Batch 3930/4290 - Avg Loss: 0.1111 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3940/4290 - Avg Loss: 0.1110 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3950/4290 - Avg Loss: 0.1108 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3960/4290 - Avg Loss: 0.1108 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3970/4290 - Avg Loss: 0.1108 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 3980/4290 - Avg Loss: 0.1110 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 3990/4290 - Avg Loss: 0.1110 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4000/4290 - Avg Loss: 0.1111 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4010/4290 - Avg Loss: 0.1111 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4020/4290 - Avg Loss: 0.1111 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 4030/4290 - Avg Loss: 0.1110 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 4040/4290 - Avg Loss: 0.1109 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 4050/4290 - Avg Loss: 0.1110 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4060/4290 - Avg Loss: 0.1108 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4070/4290 - Avg Loss: 0.1109 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4080/4290 - Avg Loss: 0.1108 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 4090/4290 - Avg Loss: 0.1109 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 4100/4290 - Avg Loss: 0.1108 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 4110/4290 - Avg Loss: 0.1107 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4120/4290 - Avg Loss: 0.1106 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4130/4290 - Avg Loss: 0.1106 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4140/4290 - Avg Loss: 0.1107 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4150/4290 - Avg Loss: 0.1107 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4160/4290 - Avg Loss: 0.1109 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 4170/4290 - Avg Loss: 0.1111 - Accuracy: 95.13%\n",
      "Epoch: 2. Batch 4180/4290 - Avg Loss: 0.1110 - Accuracy: 95.14%\n",
      "Epoch: 2. Batch 4190/4290 - Avg Loss: 0.1112 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4200/4290 - Avg Loss: 0.1112 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4210/4290 - Avg Loss: 0.1112 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4220/4290 - Avg Loss: 0.1112 - Accuracy: 95.12%\n",
      "Epoch: 2. Batch 4230/4290 - Avg Loss: 0.1113 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 4240/4290 - Avg Loss: 0.1113 - Accuracy: 95.11%\n",
      "Epoch: 2. Batch 4250/4290 - Avg Loss: 0.1113 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4260/4290 - Avg Loss: 0.1113 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4270/4290 - Avg Loss: 0.1113 - Accuracy: 95.10%\n",
      "Epoch: 2. Batch 4280/4290 - Avg Loss: 0.1113 - Accuracy: 95.10%\n",
      "Train loss: 0.1114 - Train accuracy: 95.10%\n",
      "Validation accuracy: 0.9510\n",
      "Epoch: 3. Batch 0/4290 - Avg Loss: 0.0779 - Accuracy: 93.75%\n",
      "Epoch: 3. Batch 10/4290 - Avg Loss: 0.0922 - Accuracy: 96.02%\n",
      "Epoch: 3. Batch 20/4290 - Avg Loss: 0.1035 - Accuracy: 95.83%\n",
      "Epoch: 3. Batch 30/4290 - Avg Loss: 0.0964 - Accuracy: 96.77%\n",
      "Epoch: 3. Batch 40/4290 - Avg Loss: 0.0932 - Accuracy: 96.65%\n",
      "Epoch: 3. Batch 50/4290 - Avg Loss: 0.0958 - Accuracy: 96.32%\n",
      "Epoch: 3. Batch 60/4290 - Avg Loss: 0.1005 - Accuracy: 96.21%\n",
      "Epoch: 3. Batch 70/4290 - Avg Loss: 0.0989 - Accuracy: 96.21%\n",
      "Epoch: 3. Batch 80/4290 - Avg Loss: 0.0983 - Accuracy: 96.06%\n",
      "Epoch: 3. Batch 90/4290 - Avg Loss: 0.0980 - Accuracy: 95.95%\n",
      "Epoch: 3. Batch 100/4290 - Avg Loss: 0.0980 - Accuracy: 95.92%\n",
      "Epoch: 3. Batch 110/4290 - Avg Loss: 0.1020 - Accuracy: 95.66%\n",
      "Epoch: 3. Batch 120/4290 - Avg Loss: 0.1019 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 130/4290 - Avg Loss: 0.1018 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 140/4290 - Avg Loss: 0.1001 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 150/4290 - Avg Loss: 0.1002 - Accuracy: 95.57%\n",
      "Epoch: 3. Batch 160/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 170/4290 - Avg Loss: 0.1048 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 180/4290 - Avg Loss: 0.1066 - Accuracy: 95.23%\n",
      "Epoch: 3. Batch 190/4290 - Avg Loss: 0.1079 - Accuracy: 95.16%\n",
      "Epoch: 3. Batch 200/4290 - Avg Loss: 0.1086 - Accuracy: 95.15%\n",
      "Epoch: 3. Batch 210/4290 - Avg Loss: 0.1096 - Accuracy: 95.14%\n",
      "Epoch: 3. Batch 220/4290 - Avg Loss: 0.1095 - Accuracy: 95.16%\n",
      "Epoch: 3. Batch 230/4290 - Avg Loss: 0.1093 - Accuracy: 95.16%\n",
      "Epoch: 3. Batch 240/4290 - Avg Loss: 0.1090 - Accuracy: 95.20%\n",
      "Epoch: 3. Batch 250/4290 - Avg Loss: 0.1080 - Accuracy: 95.22%\n",
      "Epoch: 3. Batch 260/4290 - Avg Loss: 0.1078 - Accuracy: 95.23%\n",
      "Epoch: 3. Batch 270/4290 - Avg Loss: 0.1069 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 280/4290 - Avg Loss: 0.1074 - Accuracy: 95.28%\n",
      "Epoch: 3. Batch 290/4290 - Avg Loss: 0.1067 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 300/4290 - Avg Loss: 0.1077 - Accuracy: 95.25%\n",
      "Epoch: 3. Batch 310/4290 - Avg Loss: 0.1077 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 320/4290 - Avg Loss: 0.1078 - Accuracy: 95.27%\n",
      "Epoch: 3. Batch 330/4290 - Avg Loss: 0.1071 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 340/4290 - Avg Loss: 0.1078 - Accuracy: 95.27%\n",
      "Epoch: 3. Batch 350/4290 - Avg Loss: 0.1078 - Accuracy: 95.23%\n",
      "Epoch: 3. Batch 360/4290 - Avg Loss: 0.1077 - Accuracy: 95.24%\n",
      "Epoch: 3. Batch 370/4290 - Avg Loss: 0.1068 - Accuracy: 95.28%\n",
      "Epoch: 3. Batch 380/4290 - Avg Loss: 0.1061 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 390/4290 - Avg Loss: 0.1059 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 400/4290 - Avg Loss: 0.1058 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 410/4290 - Avg Loss: 0.1058 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 420/4290 - Avg Loss: 0.1060 - Accuracy: 95.29%\n",
      "Epoch: 3. Batch 430/4290 - Avg Loss: 0.1056 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 440/4290 - Avg Loss: 0.1043 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 450/4290 - Avg Loss: 0.1031 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 460/4290 - Avg Loss: 0.1030 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 470/4290 - Avg Loss: 0.1033 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 480/4290 - Avg Loss: 0.1032 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 490/4290 - Avg Loss: 0.1036 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 500/4290 - Avg Loss: 0.1034 - Accuracy: 95.50%\n",
      "Epoch: 3. Batch 510/4290 - Avg Loss: 0.1035 - Accuracy: 95.50%\n",
      "Epoch: 3. Batch 520/4290 - Avg Loss: 0.1024 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 530/4290 - Avg Loss: 0.1021 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 540/4290 - Avg Loss: 0.1023 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 550/4290 - Avg Loss: 0.1027 - Accuracy: 95.51%\n",
      "Epoch: 3. Batch 560/4290 - Avg Loss: 0.1025 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 570/4290 - Avg Loss: 0.1025 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 580/4290 - Avg Loss: 0.1019 - Accuracy: 95.54%\n",
      "Epoch: 3. Batch 590/4290 - Avg Loss: 0.1013 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 600/4290 - Avg Loss: 0.1017 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 610/4290 - Avg Loss: 0.1013 - Accuracy: 95.60%\n",
      "Epoch: 3. Batch 620/4290 - Avg Loss: 0.1012 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 630/4290 - Avg Loss: 0.1008 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 640/4290 - Avg Loss: 0.1003 - Accuracy: 95.62%\n",
      "Epoch: 3. Batch 650/4290 - Avg Loss: 0.1003 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 660/4290 - Avg Loss: 0.0999 - Accuracy: 95.65%\n",
      "Epoch: 3. Batch 670/4290 - Avg Loss: 0.1002 - Accuracy: 95.64%\n",
      "Epoch: 3. Batch 680/4290 - Avg Loss: 0.1000 - Accuracy: 95.65%\n",
      "Epoch: 3. Batch 690/4290 - Avg Loss: 0.1003 - Accuracy: 95.66%\n",
      "Epoch: 3. Batch 700/4290 - Avg Loss: 0.1000 - Accuracy: 95.68%\n",
      "Epoch: 3. Batch 710/4290 - Avg Loss: 0.1006 - Accuracy: 95.63%\n",
      "Epoch: 3. Batch 720/4290 - Avg Loss: 0.1005 - Accuracy: 95.63%\n",
      "Epoch: 3. Batch 730/4290 - Avg Loss: 0.1005 - Accuracy: 95.64%\n",
      "Epoch: 3. Batch 740/4290 - Avg Loss: 0.1005 - Accuracy: 95.63%\n",
      "Epoch: 3. Batch 750/4290 - Avg Loss: 0.1010 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 760/4290 - Avg Loss: 0.1015 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 770/4290 - Avg Loss: 0.1012 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 780/4290 - Avg Loss: 0.1010 - Accuracy: 95.64%\n",
      "Epoch: 3. Batch 790/4290 - Avg Loss: 0.1009 - Accuracy: 95.64%\n",
      "Epoch: 3. Batch 800/4290 - Avg Loss: 0.1009 - Accuracy: 95.63%\n",
      "Epoch: 3. Batch 810/4290 - Avg Loss: 0.1013 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 820/4290 - Avg Loss: 0.1016 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 830/4290 - Avg Loss: 0.1023 - Accuracy: 95.54%\n",
      "Epoch: 3. Batch 840/4290 - Avg Loss: 0.1026 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 850/4290 - Avg Loss: 0.1027 - Accuracy: 95.50%\n",
      "Epoch: 3. Batch 860/4290 - Avg Loss: 0.1028 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 870/4290 - Avg Loss: 0.1029 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 880/4290 - Avg Loss: 0.1026 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 890/4290 - Avg Loss: 0.1027 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 900/4290 - Avg Loss: 0.1027 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 910/4290 - Avg Loss: 0.1025 - Accuracy: 95.50%\n",
      "Epoch: 3. Batch 920/4290 - Avg Loss: 0.1022 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 930/4290 - Avg Loss: 0.1020 - Accuracy: 95.53%\n",
      "Epoch: 3. Batch 940/4290 - Avg Loss: 0.1018 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 950/4290 - Avg Loss: 0.1023 - Accuracy: 95.51%\n",
      "Epoch: 3. Batch 960/4290 - Avg Loss: 0.1032 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 970/4290 - Avg Loss: 0.1032 - Accuracy: 95.51%\n",
      "Epoch: 3. Batch 980/4290 - Avg Loss: 0.1032 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 990/4290 - Avg Loss: 0.1030 - Accuracy: 95.54%\n",
      "Epoch: 3. Batch 1000/4290 - Avg Loss: 0.1029 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1010/4290 - Avg Loss: 0.1027 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1020/4290 - Avg Loss: 0.1025 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1030/4290 - Avg Loss: 0.1025 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1040/4290 - Avg Loss: 0.1027 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1050/4290 - Avg Loss: 0.1026 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1060/4290 - Avg Loss: 0.1025 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1070/4290 - Avg Loss: 0.1024 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1080/4290 - Avg Loss: 0.1021 - Accuracy: 95.57%\n",
      "Epoch: 3. Batch 1090/4290 - Avg Loss: 0.1024 - Accuracy: 95.57%\n",
      "Epoch: 3. Batch 1100/4290 - Avg Loss: 0.1023 - Accuracy: 95.57%\n",
      "Epoch: 3. Batch 1110/4290 - Avg Loss: 0.1022 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 1120/4290 - Avg Loss: 0.1022 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 1130/4290 - Avg Loss: 0.1022 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 1140/4290 - Avg Loss: 0.1020 - Accuracy: 95.60%\n",
      "Epoch: 3. Batch 1150/4290 - Avg Loss: 0.1020 - Accuracy: 95.60%\n",
      "Epoch: 3. Batch 1160/4290 - Avg Loss: 0.1022 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 1170/4290 - Avg Loss: 0.1019 - Accuracy: 95.60%\n",
      "Epoch: 3. Batch 1180/4290 - Avg Loss: 0.1019 - Accuracy: 95.61%\n",
      "Epoch: 3. Batch 1190/4290 - Avg Loss: 0.1018 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 1200/4290 - Avg Loss: 0.1019 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 1210/4290 - Avg Loss: 0.1018 - Accuracy: 95.60%\n",
      "Epoch: 3. Batch 1220/4290 - Avg Loss: 0.1020 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 1230/4290 - Avg Loss: 0.1023 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1240/4290 - Avg Loss: 0.1024 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1250/4290 - Avg Loss: 0.1023 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1260/4290 - Avg Loss: 0.1022 - Accuracy: 95.56%\n",
      "Epoch: 3. Batch 1270/4290 - Avg Loss: 0.1018 - Accuracy: 95.57%\n",
      "Epoch: 3. Batch 1280/4290 - Avg Loss: 0.1016 - Accuracy: 95.59%\n",
      "Epoch: 3. Batch 1290/4290 - Avg Loss: 0.1018 - Accuracy: 95.58%\n",
      "Epoch: 3. Batch 1300/4290 - Avg Loss: 0.1021 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1310/4290 - Avg Loss: 0.1021 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1320/4290 - Avg Loss: 0.1020 - Accuracy: 95.55%\n",
      "Epoch: 3. Batch 1330/4290 - Avg Loss: 0.1024 - Accuracy: 95.52%\n",
      "Epoch: 3. Batch 1340/4290 - Avg Loss: 0.1028 - Accuracy: 95.50%\n",
      "Epoch: 3. Batch 1350/4290 - Avg Loss: 0.1030 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 1360/4290 - Avg Loss: 0.1032 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 1370/4290 - Avg Loss: 0.1033 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1380/4290 - Avg Loss: 0.1032 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1390/4290 - Avg Loss: 0.1030 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1400/4290 - Avg Loss: 0.1030 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1410/4290 - Avg Loss: 0.1028 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1420/4290 - Avg Loss: 0.1028 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1430/4290 - Avg Loss: 0.1029 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 1440/4290 - Avg Loss: 0.1028 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 1450/4290 - Avg Loss: 0.1027 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1460/4290 - Avg Loss: 0.1027 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1470/4290 - Avg Loss: 0.1029 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1480/4290 - Avg Loss: 0.1026 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 1490/4290 - Avg Loss: 0.1025 - Accuracy: 95.49%\n",
      "Epoch: 3. Batch 1500/4290 - Avg Loss: 0.1028 - Accuracy: 95.48%\n",
      "Epoch: 3. Batch 1510/4290 - Avg Loss: 0.1028 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1520/4290 - Avg Loss: 0.1028 - Accuracy: 95.47%\n",
      "Epoch: 3. Batch 1530/4290 - Avg Loss: 0.1031 - Accuracy: 95.44%\n",
      "Epoch: 3. Batch 1540/4290 - Avg Loss: 0.1031 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1550/4290 - Avg Loss: 0.1030 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 1560/4290 - Avg Loss: 0.1031 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1570/4290 - Avg Loss: 0.1030 - Accuracy: 95.46%\n",
      "Epoch: 3. Batch 1580/4290 - Avg Loss: 0.1031 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1590/4290 - Avg Loss: 0.1032 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1600/4290 - Avg Loss: 0.1033 - Accuracy: 95.44%\n",
      "Epoch: 3. Batch 1610/4290 - Avg Loss: 0.1033 - Accuracy: 95.44%\n",
      "Epoch: 3. Batch 1620/4290 - Avg Loss: 0.1034 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1630/4290 - Avg Loss: 0.1035 - Accuracy: 95.44%\n",
      "Epoch: 3. Batch 1640/4290 - Avg Loss: 0.1037 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1650/4290 - Avg Loss: 0.1039 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1660/4290 - Avg Loss: 0.1038 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1670/4290 - Avg Loss: 0.1038 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1680/4290 - Avg Loss: 0.1038 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1690/4290 - Avg Loss: 0.1037 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1700/4290 - Avg Loss: 0.1036 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1710/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1720/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1730/4290 - Avg Loss: 0.1033 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1740/4290 - Avg Loss: 0.1031 - Accuracy: 95.44%\n",
      "Epoch: 3. Batch 1750/4290 - Avg Loss: 0.1031 - Accuracy: 95.45%\n",
      "Epoch: 3. Batch 1760/4290 - Avg Loss: 0.1033 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1770/4290 - Avg Loss: 0.1033 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1780/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1790/4290 - Avg Loss: 0.1037 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1800/4290 - Avg Loss: 0.1036 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1810/4290 - Avg Loss: 0.1037 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1820/4290 - Avg Loss: 0.1036 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1830/4290 - Avg Loss: 0.1037 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1840/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1850/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1860/4290 - Avg Loss: 0.1036 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1870/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1880/4290 - Avg Loss: 0.1036 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1890/4290 - Avg Loss: 0.1034 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1900/4290 - Avg Loss: 0.1036 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1910/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1920/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1930/4290 - Avg Loss: 0.1038 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1940/4290 - Avg Loss: 0.1037 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1950/4290 - Avg Loss: 0.1036 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 1960/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 1970/4290 - Avg Loss: 0.1033 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1980/4290 - Avg Loss: 0.1034 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 1990/4290 - Avg Loss: 0.1033 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2000/4290 - Avg Loss: 0.1032 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2010/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2020/4290 - Avg Loss: 0.1035 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2030/4290 - Avg Loss: 0.1035 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2040/4290 - Avg Loss: 0.1035 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2050/4290 - Avg Loss: 0.1036 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2060/4290 - Avg Loss: 0.1037 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 2070/4290 - Avg Loss: 0.1036 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2080/4290 - Avg Loss: 0.1034 - Accuracy: 95.43%\n",
      "Epoch: 3. Batch 2090/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2100/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2110/4290 - Avg Loss: 0.1035 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2120/4290 - Avg Loss: 0.1033 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2130/4290 - Avg Loss: 0.1035 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2140/4290 - Avg Loss: 0.1034 - Accuracy: 95.42%\n",
      "Epoch: 3. Batch 2150/4290 - Avg Loss: 0.1034 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2160/4290 - Avg Loss: 0.1034 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 2170/4290 - Avg Loss: 0.1033 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2180/4290 - Avg Loss: 0.1034 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2190/4290 - Avg Loss: 0.1035 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2200/4290 - Avg Loss: 0.1036 - Accuracy: 95.41%\n",
      "Epoch: 3. Batch 2210/4290 - Avg Loss: 0.1038 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2220/4290 - Avg Loss: 0.1038 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2230/4290 - Avg Loss: 0.1040 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2240/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2250/4290 - Avg Loss: 0.1042 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2260/4290 - Avg Loss: 0.1042 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2270/4290 - Avg Loss: 0.1042 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2280/4290 - Avg Loss: 0.1042 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2290/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2300/4290 - Avg Loss: 0.1041 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2310/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2320/4290 - Avg Loss: 0.1041 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2330/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2340/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2350/4290 - Avg Loss: 0.1038 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2360/4290 - Avg Loss: 0.1038 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2370/4290 - Avg Loss: 0.1038 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2380/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2390/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2400/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2410/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2420/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2430/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2440/4290 - Avg Loss: 0.1036 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2450/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2460/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2470/4290 - Avg Loss: 0.1035 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 2480/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2490/4290 - Avg Loss: 0.1038 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2500/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2510/4290 - Avg Loss: 0.1037 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2520/4290 - Avg Loss: 0.1037 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2530/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2540/4290 - Avg Loss: 0.1037 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2550/4290 - Avg Loss: 0.1039 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2560/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2570/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2580/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2590/4290 - Avg Loss: 0.1037 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 2600/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2610/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2620/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2630/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2640/4290 - Avg Loss: 0.1039 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2650/4290 - Avg Loss: 0.1039 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 2660/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2670/4290 - Avg Loss: 0.1041 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2680/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2690/4290 - Avg Loss: 0.1042 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2700/4290 - Avg Loss: 0.1041 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2710/4290 - Avg Loss: 0.1041 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2720/4290 - Avg Loss: 0.1043 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2730/4290 - Avg Loss: 0.1045 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2740/4290 - Avg Loss: 0.1044 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2750/4290 - Avg Loss: 0.1043 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2760/4290 - Avg Loss: 0.1042 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2770/4290 - Avg Loss: 0.1044 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2780/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2790/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2800/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2810/4290 - Avg Loss: 0.1043 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 2820/4290 - Avg Loss: 0.1043 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 2830/4290 - Avg Loss: 0.1043 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 2840/4290 - Avg Loss: 0.1043 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 2850/4290 - Avg Loss: 0.1042 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 2860/4290 - Avg Loss: 0.1041 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2870/4290 - Avg Loss: 0.1041 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2880/4290 - Avg Loss: 0.1041 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2890/4290 - Avg Loss: 0.1040 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2900/4290 - Avg Loss: 0.1040 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 2910/4290 - Avg Loss: 0.1039 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2920/4290 - Avg Loss: 0.1038 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2930/4290 - Avg Loss: 0.1039 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2940/4290 - Avg Loss: 0.1039 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 2950/4290 - Avg Loss: 0.1038 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2960/4290 - Avg Loss: 0.1038 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 2970/4290 - Avg Loss: 0.1037 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2980/4290 - Avg Loss: 0.1037 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 2990/4290 - Avg Loss: 0.1035 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3000/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3010/4290 - Avg Loss: 0.1033 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3020/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3030/4290 - Avg Loss: 0.1034 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3040/4290 - Avg Loss: 0.1033 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3050/4290 - Avg Loss: 0.1034 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3060/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3070/4290 - Avg Loss: 0.1036 - Accuracy: 95.36%\n",
      "Epoch: 3. Batch 3080/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3090/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3100/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3110/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3120/4290 - Avg Loss: 0.1034 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3130/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3140/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3150/4290 - Avg Loss: 0.1034 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3160/4290 - Avg Loss: 0.1036 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3170/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3180/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3190/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3200/4290 - Avg Loss: 0.1035 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3210/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3220/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3230/4290 - Avg Loss: 0.1034 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3240/4290 - Avg Loss: 0.1033 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 3250/4290 - Avg Loss: 0.1034 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 3260/4290 - Avg Loss: 0.1034 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 3270/4290 - Avg Loss: 0.1034 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 3280/4290 - Avg Loss: 0.1034 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 3290/4290 - Avg Loss: 0.1035 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 3300/4290 - Avg Loss: 0.1036 - Accuracy: 95.39%\n",
      "Epoch: 3. Batch 3310/4290 - Avg Loss: 0.1034 - Accuracy: 95.40%\n",
      "Epoch: 3. Batch 3320/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3330/4290 - Avg Loss: 0.1039 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3340/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3350/4290 - Avg Loss: 0.1037 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3360/4290 - Avg Loss: 0.1037 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3370/4290 - Avg Loss: 0.1037 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3380/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3390/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3400/4290 - Avg Loss: 0.1038 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3410/4290 - Avg Loss: 0.1037 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3420/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3430/4290 - Avg Loss: 0.1038 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3440/4290 - Avg Loss: 0.1039 - Accuracy: 95.38%\n",
      "Epoch: 3. Batch 3450/4290 - Avg Loss: 0.1040 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3460/4290 - Avg Loss: 0.1041 - Accuracy: 95.37%\n",
      "Epoch: 3. Batch 3470/4290 - Avg Loss: 0.1043 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 3480/4290 - Avg Loss: 0.1042 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 3490/4290 - Avg Loss: 0.1044 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3500/4290 - Avg Loss: 0.1043 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3510/4290 - Avg Loss: 0.1044 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3520/4290 - Avg Loss: 0.1044 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 3530/4290 - Avg Loss: 0.1044 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3540/4290 - Avg Loss: 0.1044 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3550/4290 - Avg Loss: 0.1043 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 3560/4290 - Avg Loss: 0.1043 - Accuracy: 95.35%\n",
      "Epoch: 3. Batch 3570/4290 - Avg Loss: 0.1045 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3580/4290 - Avg Loss: 0.1046 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3590/4290 - Avg Loss: 0.1045 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3600/4290 - Avg Loss: 0.1047 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3610/4290 - Avg Loss: 0.1047 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3620/4290 - Avg Loss: 0.1048 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3630/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3640/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3650/4290 - Avg Loss: 0.1050 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 3660/4290 - Avg Loss: 0.1050 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3670/4290 - Avg Loss: 0.1050 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3680/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3690/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3700/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3710/4290 - Avg Loss: 0.1048 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3720/4290 - Avg Loss: 0.1050 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3730/4290 - Avg Loss: 0.1051 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 3740/4290 - Avg Loss: 0.1052 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3750/4290 - Avg Loss: 0.1052 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3760/4290 - Avg Loss: 0.1051 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3770/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 3780/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 3790/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 3800/4290 - Avg Loss: 0.1053 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3810/4290 - Avg Loss: 0.1052 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3820/4290 - Avg Loss: 0.1052 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3830/4290 - Avg Loss: 0.1051 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 3840/4290 - Avg Loss: 0.1051 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3850/4290 - Avg Loss: 0.1051 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3860/4290 - Avg Loss: 0.1050 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3870/4290 - Avg Loss: 0.1050 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3880/4290 - Avg Loss: 0.1050 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3890/4290 - Avg Loss: 0.1049 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3900/4290 - Avg Loss: 0.1050 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3910/4290 - Avg Loss: 0.1049 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3920/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3930/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3940/4290 - Avg Loss: 0.1048 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3950/4290 - Avg Loss: 0.1047 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3960/4290 - Avg Loss: 0.1048 - Accuracy: 95.34%\n",
      "Epoch: 3. Batch 3970/4290 - Avg Loss: 0.1048 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3980/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 3990/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4000/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4010/4290 - Avg Loss: 0.1048 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4020/4290 - Avg Loss: 0.1048 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4030/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4040/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4050/4290 - Avg Loss: 0.1049 - Accuracy: 95.33%\n",
      "Epoch: 3. Batch 4060/4290 - Avg Loss: 0.1049 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 4070/4290 - Avg Loss: 0.1052 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4080/4290 - Avg Loss: 0.1053 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 4090/4290 - Avg Loss: 0.1053 - Accuracy: 95.32%\n",
      "Epoch: 3. Batch 4100/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4110/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4120/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4130/4290 - Avg Loss: 0.1053 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4140/4290 - Avg Loss: 0.1052 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4150/4290 - Avg Loss: 0.1054 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4160/4290 - Avg Loss: 0.1055 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4170/4290 - Avg Loss: 0.1056 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4180/4290 - Avg Loss: 0.1056 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4190/4290 - Avg Loss: 0.1057 - Accuracy: 95.29%\n",
      "Epoch: 3. Batch 4200/4290 - Avg Loss: 0.1056 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4210/4290 - Avg Loss: 0.1055 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4220/4290 - Avg Loss: 0.1055 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4230/4290 - Avg Loss: 0.1055 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4240/4290 - Avg Loss: 0.1055 - Accuracy: 95.30%\n",
      "Epoch: 3. Batch 4250/4290 - Avg Loss: 0.1055 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4260/4290 - Avg Loss: 0.1054 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4270/4290 - Avg Loss: 0.1054 - Accuracy: 95.31%\n",
      "Epoch: 3. Batch 4280/4290 - Avg Loss: 0.1053 - Accuracy: 95.32%\n",
      "Train loss: 0.1053 - Train accuracy: 95.32%\n",
      "Validation accuracy: 0.9524\n",
      "Epoch: 4. Batch 0/4290 - Avg Loss: 0.0004 - Accuracy: 100.00%\n",
      "Epoch: 4. Batch 10/4290 - Avg Loss: 0.0764 - Accuracy: 97.16%\n",
      "Epoch: 4. Batch 20/4290 - Avg Loss: 0.1044 - Accuracy: 95.54%\n",
      "Epoch: 4. Batch 30/4290 - Avg Loss: 0.0934 - Accuracy: 95.97%\n",
      "Epoch: 4. Batch 40/4290 - Avg Loss: 0.0826 - Accuracy: 96.65%\n",
      "Epoch: 4. Batch 50/4290 - Avg Loss: 0.0881 - Accuracy: 96.32%\n",
      "Epoch: 4. Batch 60/4290 - Avg Loss: 0.0932 - Accuracy: 96.00%\n",
      "Epoch: 4. Batch 70/4290 - Avg Loss: 0.0964 - Accuracy: 95.86%\n",
      "Epoch: 4. Batch 80/4290 - Avg Loss: 0.1009 - Accuracy: 95.68%\n",
      "Epoch: 4. Batch 90/4290 - Avg Loss: 0.1018 - Accuracy: 95.54%\n",
      "Epoch: 4. Batch 100/4290 - Avg Loss: 0.1017 - Accuracy: 95.54%\n",
      "Epoch: 4. Batch 110/4290 - Avg Loss: 0.1028 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 120/4290 - Avg Loss: 0.1033 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 130/4290 - Avg Loss: 0.1053 - Accuracy: 95.23%\n",
      "Epoch: 4. Batch 140/4290 - Avg Loss: 0.1052 - Accuracy: 95.17%\n",
      "Epoch: 4. Batch 150/4290 - Avg Loss: 0.1035 - Accuracy: 95.16%\n",
      "Epoch: 4. Batch 160/4290 - Avg Loss: 0.1055 - Accuracy: 95.07%\n",
      "Epoch: 4. Batch 170/4290 - Avg Loss: 0.1023 - Accuracy: 95.25%\n",
      "Epoch: 4. Batch 180/4290 - Avg Loss: 0.1018 - Accuracy: 95.20%\n",
      "Epoch: 4. Batch 190/4290 - Avg Loss: 0.1029 - Accuracy: 95.19%\n",
      "Epoch: 4. Batch 200/4290 - Avg Loss: 0.1041 - Accuracy: 95.09%\n",
      "Epoch: 4. Batch 210/4290 - Avg Loss: 0.1028 - Accuracy: 95.11%\n",
      "Epoch: 4. Batch 220/4290 - Avg Loss: 0.1041 - Accuracy: 95.02%\n",
      "Epoch: 4. Batch 230/4290 - Avg Loss: 0.1039 - Accuracy: 95.08%\n",
      "Epoch: 4. Batch 240/4290 - Avg Loss: 0.1036 - Accuracy: 95.10%\n",
      "Epoch: 4. Batch 250/4290 - Avg Loss: 0.1035 - Accuracy: 95.12%\n",
      "Epoch: 4. Batch 260/4290 - Avg Loss: 0.1027 - Accuracy: 95.19%\n",
      "Epoch: 4. Batch 270/4290 - Avg Loss: 0.1027 - Accuracy: 95.20%\n",
      "Epoch: 4. Batch 280/4290 - Avg Loss: 0.1016 - Accuracy: 95.28%\n",
      "Epoch: 4. Batch 290/4290 - Avg Loss: 0.1007 - Accuracy: 95.34%\n",
      "Epoch: 4. Batch 300/4290 - Avg Loss: 0.0990 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 310/4290 - Avg Loss: 0.0989 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 320/4290 - Avg Loss: 0.0997 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 330/4290 - Avg Loss: 0.0986 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 340/4290 - Avg Loss: 0.0987 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 350/4290 - Avg Loss: 0.0989 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 360/4290 - Avg Loss: 0.0986 - Accuracy: 95.57%\n",
      "Epoch: 4. Batch 370/4290 - Avg Loss: 0.0993 - Accuracy: 95.54%\n",
      "Epoch: 4. Batch 380/4290 - Avg Loss: 0.0989 - Accuracy: 95.57%\n",
      "Epoch: 4. Batch 390/4290 - Avg Loss: 0.0982 - Accuracy: 95.60%\n",
      "Epoch: 4. Batch 400/4290 - Avg Loss: 0.0994 - Accuracy: 95.54%\n",
      "Epoch: 4. Batch 410/4290 - Avg Loss: 0.0991 - Accuracy: 95.57%\n",
      "Epoch: 4. Batch 420/4290 - Avg Loss: 0.0995 - Accuracy: 95.53%\n",
      "Epoch: 4. Batch 430/4290 - Avg Loss: 0.1007 - Accuracy: 95.52%\n",
      "Epoch: 4. Batch 440/4290 - Avg Loss: 0.1019 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 450/4290 - Avg Loss: 0.1022 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 460/4290 - Avg Loss: 0.1021 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 470/4290 - Avg Loss: 0.1023 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 480/4290 - Avg Loss: 0.1023 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 490/4290 - Avg Loss: 0.1020 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 500/4290 - Avg Loss: 0.1021 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 510/4290 - Avg Loss: 0.1020 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 520/4290 - Avg Loss: 0.1015 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 530/4290 - Avg Loss: 0.1021 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 540/4290 - Avg Loss: 0.1017 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 550/4290 - Avg Loss: 0.1018 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 560/4290 - Avg Loss: 0.1018 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 570/4290 - Avg Loss: 0.1016 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 580/4290 - Avg Loss: 0.1022 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 590/4290 - Avg Loss: 0.1015 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 600/4290 - Avg Loss: 0.1007 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 610/4290 - Avg Loss: 0.1009 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 620/4290 - Avg Loss: 0.1005 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 630/4290 - Avg Loss: 0.1003 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 640/4290 - Avg Loss: 0.1005 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 650/4290 - Avg Loss: 0.1007 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 660/4290 - Avg Loss: 0.1002 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 670/4290 - Avg Loss: 0.1003 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 680/4290 - Avg Loss: 0.1003 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 690/4290 - Avg Loss: 0.1004 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 700/4290 - Avg Loss: 0.1005 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 710/4290 - Avg Loss: 0.1004 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 720/4290 - Avg Loss: 0.1004 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 730/4290 - Avg Loss: 0.1004 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 740/4290 - Avg Loss: 0.1004 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 750/4290 - Avg Loss: 0.1006 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 760/4290 - Avg Loss: 0.1009 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 770/4290 - Avg Loss: 0.1006 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 780/4290 - Avg Loss: 0.1004 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 790/4290 - Avg Loss: 0.1003 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 800/4290 - Avg Loss: 0.1008 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 810/4290 - Avg Loss: 0.1004 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 820/4290 - Avg Loss: 0.1004 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 830/4290 - Avg Loss: 0.1008 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 840/4290 - Avg Loss: 0.1010 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 850/4290 - Avg Loss: 0.1013 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 860/4290 - Avg Loss: 0.1008 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 870/4290 - Avg Loss: 0.1010 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 880/4290 - Avg Loss: 0.1009 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 890/4290 - Avg Loss: 0.1007 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 900/4290 - Avg Loss: 0.1005 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 910/4290 - Avg Loss: 0.1005 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 920/4290 - Avg Loss: 0.1006 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 930/4290 - Avg Loss: 0.1006 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 940/4290 - Avg Loss: 0.1004 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 950/4290 - Avg Loss: 0.1008 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 960/4290 - Avg Loss: 0.1007 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 970/4290 - Avg Loss: 0.1006 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 980/4290 - Avg Loss: 0.1006 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 990/4290 - Avg Loss: 0.1003 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1000/4290 - Avg Loss: 0.0999 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1010/4290 - Avg Loss: 0.1000 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1020/4290 - Avg Loss: 0.0999 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1030/4290 - Avg Loss: 0.0997 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1040/4290 - Avg Loss: 0.0997 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1050/4290 - Avg Loss: 0.0998 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1060/4290 - Avg Loss: 0.1001 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1070/4290 - Avg Loss: 0.1004 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1080/4290 - Avg Loss: 0.1007 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1090/4290 - Avg Loss: 0.1013 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 1100/4290 - Avg Loss: 0.1012 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 1110/4290 - Avg Loss: 0.1013 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 1120/4290 - Avg Loss: 0.1013 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 1130/4290 - Avg Loss: 0.1012 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1140/4290 - Avg Loss: 0.1011 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1150/4290 - Avg Loss: 0.1012 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1160/4290 - Avg Loss: 0.1012 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1170/4290 - Avg Loss: 0.1012 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1180/4290 - Avg Loss: 0.1012 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1190/4290 - Avg Loss: 0.1013 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1200/4290 - Avg Loss: 0.1012 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1210/4290 - Avg Loss: 0.1016 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 1220/4290 - Avg Loss: 0.1018 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 1230/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1240/4290 - Avg Loss: 0.1018 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 1250/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1260/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 1270/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1280/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 1290/4290 - Avg Loss: 0.1022 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1300/4290 - Avg Loss: 0.1022 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 1310/4290 - Avg Loss: 0.1021 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 1320/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1330/4290 - Avg Loss: 0.1020 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 1340/4290 - Avg Loss: 0.1015 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1350/4290 - Avg Loss: 0.1013 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1360/4290 - Avg Loss: 0.1010 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1370/4290 - Avg Loss: 0.1010 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1380/4290 - Avg Loss: 0.1009 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1390/4290 - Avg Loss: 0.1010 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1400/4290 - Avg Loss: 0.1009 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1410/4290 - Avg Loss: 0.1008 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1420/4290 - Avg Loss: 0.1008 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 1430/4290 - Avg Loss: 0.1008 - Accuracy: 95.51%\n",
      "Epoch: 4. Batch 1440/4290 - Avg Loss: 0.1013 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1450/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1460/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1470/4290 - Avg Loss: 0.1012 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1480/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1490/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 1500/4290 - Avg Loss: 0.1014 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 1510/4290 - Avg Loss: 0.1012 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 1520/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 1530/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1540/4290 - Avg Loss: 0.1018 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 1550/4290 - Avg Loss: 0.1018 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 1560/4290 - Avg Loss: 0.1019 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 1570/4290 - Avg Loss: 0.1024 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1580/4290 - Avg Loss: 0.1025 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 1590/4290 - Avg Loss: 0.1029 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 1600/4290 - Avg Loss: 0.1028 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 1610/4290 - Avg Loss: 0.1028 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1620/4290 - Avg Loss: 0.1028 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1630/4290 - Avg Loss: 0.1032 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1640/4290 - Avg Loss: 0.1031 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 1650/4290 - Avg Loss: 0.1029 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1660/4290 - Avg Loss: 0.1029 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1670/4290 - Avg Loss: 0.1029 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1680/4290 - Avg Loss: 0.1028 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1690/4290 - Avg Loss: 0.1029 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1700/4290 - Avg Loss: 0.1028 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1710/4290 - Avg Loss: 0.1028 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 1720/4290 - Avg Loss: 0.1028 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 1730/4290 - Avg Loss: 0.1031 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1740/4290 - Avg Loss: 0.1030 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1750/4290 - Avg Loss: 0.1031 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1760/4290 - Avg Loss: 0.1030 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 1770/4290 - Avg Loss: 0.1030 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1780/4290 - Avg Loss: 0.1028 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1790/4290 - Avg Loss: 0.1029 - Accuracy: 95.38%\n",
      "Epoch: 4. Batch 1800/4290 - Avg Loss: 0.1031 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1810/4290 - Avg Loss: 0.1031 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1820/4290 - Avg Loss: 0.1032 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1830/4290 - Avg Loss: 0.1029 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1840/4290 - Avg Loss: 0.1030 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1850/4290 - Avg Loss: 0.1030 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1860/4290 - Avg Loss: 0.1032 - Accuracy: 95.36%\n",
      "Epoch: 4. Batch 1870/4290 - Avg Loss: 0.1031 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1880/4290 - Avg Loss: 0.1032 - Accuracy: 95.36%\n",
      "Epoch: 4. Batch 1890/4290 - Avg Loss: 0.1031 - Accuracy: 95.36%\n",
      "Epoch: 4. Batch 1900/4290 - Avg Loss: 0.1030 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1910/4290 - Avg Loss: 0.1029 - Accuracy: 95.37%\n",
      "Epoch: 4. Batch 1920/4290 - Avg Loss: 0.1025 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1930/4290 - Avg Loss: 0.1022 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 1940/4290 - Avg Loss: 0.1023 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1950/4290 - Avg Loss: 0.1022 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1960/4290 - Avg Loss: 0.1023 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1970/4290 - Avg Loss: 0.1022 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 1980/4290 - Avg Loss: 0.1021 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 1990/4290 - Avg Loss: 0.1023 - Accuracy: 95.39%\n",
      "Epoch: 4. Batch 2000/4290 - Avg Loss: 0.1021 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 2010/4290 - Avg Loss: 0.1020 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2020/4290 - Avg Loss: 0.1021 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2030/4290 - Avg Loss: 0.1020 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2040/4290 - Avg Loss: 0.1018 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2050/4290 - Avg Loss: 0.1020 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2060/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2070/4290 - Avg Loss: 0.1020 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2080/4290 - Avg Loss: 0.1018 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2090/4290 - Avg Loss: 0.1019 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2100/4290 - Avg Loss: 0.1019 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2110/4290 - Avg Loss: 0.1018 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2120/4290 - Avg Loss: 0.1018 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2130/4290 - Avg Loss: 0.1018 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2140/4290 - Avg Loss: 0.1019 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 2150/4290 - Avg Loss: 0.1018 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2160/4290 - Avg Loss: 0.1019 - Accuracy: 95.40%\n",
      "Epoch: 4. Batch 2170/4290 - Avg Loss: 0.1019 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 2180/4290 - Avg Loss: 0.1017 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2190/4290 - Avg Loss: 0.1019 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 2200/4290 - Avg Loss: 0.1018 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2210/4290 - Avg Loss: 0.1017 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2220/4290 - Avg Loss: 0.1019 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2230/4290 - Avg Loss: 0.1019 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2240/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2250/4290 - Avg Loss: 0.1019 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2260/4290 - Avg Loss: 0.1018 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2270/4290 - Avg Loss: 0.1018 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 2280/4290 - Avg Loss: 0.1020 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2290/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2300/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2310/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2320/4290 - Avg Loss: 0.1022 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2330/4290 - Avg Loss: 0.1024 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2340/4290 - Avg Loss: 0.1024 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2350/4290 - Avg Loss: 0.1025 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2360/4290 - Avg Loss: 0.1023 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2370/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2380/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2390/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2400/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2410/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2420/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2430/4290 - Avg Loss: 0.1024 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 2440/4290 - Avg Loss: 0.1025 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2450/4290 - Avg Loss: 0.1026 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2460/4290 - Avg Loss: 0.1025 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2470/4290 - Avg Loss: 0.1026 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 2480/4290 - Avg Loss: 0.1023 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 2490/4290 - Avg Loss: 0.1023 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 2500/4290 - Avg Loss: 0.1022 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 2510/4290 - Avg Loss: 0.1021 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 2520/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2530/4290 - Avg Loss: 0.1020 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2540/4290 - Avg Loss: 0.1020 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2550/4290 - Avg Loss: 0.1018 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2560/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2570/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2580/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2590/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2600/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2610/4290 - Avg Loss: 0.1015 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 2620/4290 - Avg Loss: 0.1014 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 2630/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 2640/4290 - Avg Loss: 0.1011 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 2650/4290 - Avg Loss: 0.1012 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 2660/4290 - Avg Loss: 0.1011 - Accuracy: 95.50%\n",
      "Epoch: 4. Batch 2670/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 2680/4290 - Avg Loss: 0.1014 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2690/4290 - Avg Loss: 0.1013 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2700/4290 - Avg Loss: 0.1013 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2710/4290 - Avg Loss: 0.1013 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2720/4290 - Avg Loss: 0.1014 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2730/4290 - Avg Loss: 0.1014 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2740/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2750/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2760/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2770/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2780/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2790/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2800/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2810/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2820/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2830/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2840/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2850/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2860/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2870/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2880/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2890/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2900/4290 - Avg Loss: 0.1018 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 2910/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2920/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2930/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2940/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2950/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 2960/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2970/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2980/4290 - Avg Loss: 0.1018 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 2990/4290 - Avg Loss: 0.1019 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3000/4290 - Avg Loss: 0.1018 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3010/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3020/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3030/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3040/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3050/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3060/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3070/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3080/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3090/4290 - Avg Loss: 0.1022 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 3100/4290 - Avg Loss: 0.1023 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 3110/4290 - Avg Loss: 0.1023 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 3120/4290 - Avg Loss: 0.1023 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 3130/4290 - Avg Loss: 0.1022 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 3140/4290 - Avg Loss: 0.1022 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 3150/4290 - Avg Loss: 0.1021 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3160/4290 - Avg Loss: 0.1021 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3170/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3180/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3190/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3200/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3210/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3220/4290 - Avg Loss: 0.1019 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3230/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3240/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3250/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3260/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3270/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3280/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3290/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3300/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3310/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3320/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3330/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3340/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3350/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3360/4290 - Avg Loss: 0.1014 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 3370/4290 - Avg Loss: 0.1013 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 3380/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3390/4290 - Avg Loss: 0.1014 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 3400/4290 - Avg Loss: 0.1014 - Accuracy: 95.49%\n",
      "Epoch: 4. Batch 3410/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3420/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3430/4290 - Avg Loss: 0.1016 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3440/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3450/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3460/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3470/4290 - Avg Loss: 0.1017 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3480/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3490/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3500/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3510/4290 - Avg Loss: 0.1018 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3520/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3530/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3540/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3550/4290 - Avg Loss: 0.1017 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3560/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3570/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3580/4290 - Avg Loss: 0.1017 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3590/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3600/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3610/4290 - Avg Loss: 0.1015 - Accuracy: 95.48%\n",
      "Epoch: 4. Batch 3620/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3630/4290 - Avg Loss: 0.1015 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3640/4290 - Avg Loss: 0.1016 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3650/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3660/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3670/4290 - Avg Loss: 0.1016 - Accuracy: 95.47%\n",
      "Epoch: 4. Batch 3680/4290 - Avg Loss: 0.1019 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3690/4290 - Avg Loss: 0.1020 - Accuracy: 95.46%\n",
      "Epoch: 4. Batch 3700/4290 - Avg Loss: 0.1021 - Accuracy: 95.45%\n",
      "Epoch: 4. Batch 3710/4290 - Avg Loss: 0.1022 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 3720/4290 - Avg Loss: 0.1022 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 3730/4290 - Avg Loss: 0.1023 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 3740/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3750/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3760/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3770/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3780/4290 - Avg Loss: 0.1024 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 3790/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3800/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3810/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3820/4290 - Avg Loss: 0.1022 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 3830/4290 - Avg Loss: 0.1022 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 3840/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 3850/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3860/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3870/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3880/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3890/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3900/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3910/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3920/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3930/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3940/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3950/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3960/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3970/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3980/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 3990/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4000/4290 - Avg Loss: 0.1022 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4010/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4020/4290 - Avg Loss: 0.1022 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4030/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 4040/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 4050/4290 - Avg Loss: 0.1021 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 4060/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4070/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4080/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4090/4290 - Avg Loss: 0.1020 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4100/4290 - Avg Loss: 0.1019 - Accuracy: 95.44%\n",
      "Epoch: 4. Batch 4110/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4120/4290 - Avg Loss: 0.1021 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4130/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4140/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4150/4290 - Avg Loss: 0.1025 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4160/4290 - Avg Loss: 0.1026 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4170/4290 - Avg Loss: 0.1026 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4180/4290 - Avg Loss: 0.1026 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 4190/4290 - Avg Loss: 0.1026 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4200/4290 - Avg Loss: 0.1026 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 4210/4290 - Avg Loss: 0.1025 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4220/4290 - Avg Loss: 0.1025 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 4230/4290 - Avg Loss: 0.1025 - Accuracy: 95.41%\n",
      "Epoch: 4. Batch 4240/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4250/4290 - Avg Loss: 0.1023 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4260/4290 - Avg Loss: 0.1023 - Accuracy: 95.43%\n",
      "Epoch: 4. Batch 4270/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Epoch: 4. Batch 4280/4290 - Avg Loss: 0.1024 - Accuracy: 95.42%\n",
      "Train loss: 0.1023 - Train accuracy: 95.43%\n",
      "Validation accuracy: 0.9504\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch)\n",
    "    print(f\"Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "    val_accuracy = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb356eebc65dda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9104323f8866022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9498\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = eval_model(model, test_loader, device)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5968b387c2a016",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91008cc011888e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_emotion_model\\\\tokenizer_config.json',\n",
       " './bert_emotion_model\\\\special_tokens_map.json',\n",
       " './bert_emotion_model\\\\vocab.txt',\n",
       " './bert_emotion_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./bert_emotion_model')\n",
    "tokenizer.save_pretrained('./bert_emotion_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
